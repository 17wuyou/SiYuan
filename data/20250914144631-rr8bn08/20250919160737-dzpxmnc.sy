{"ID":"20250919160737-dzpxmnc","Spec":"1","Type":"NodeDocument","Properties":{"id":"20250919160737-dzpxmnc","title":"04 重新思考演示的角色：什么让上下文学习起作用","type":"doc","updated":"20250920134433"},"Children":[{"ID":"20250919160945-u8wsua9","Type":"NodeHeading","HeadingLevel":3,"Properties":{"id":"20250919160945-u8wsua9","updated":"20250920134433"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"摘要"}]},{"ID":"20250919160945-bs6mmc8","Type":"NodeParagraph","Properties":{"id":"20250919160945-bs6mmc8","updated":"20250919160946"},"Children":[{"Type":"NodeText","Data":"大型语言模型（LMs）能够进行"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"情境学习（in-context learn）"},{"Type":"NodeText","Data":"——仅通过对少数输入-标签对（即"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"演示"},{"Type":"NodeText","Data":"）进行条件化，并对新输入进行预测，就能通过"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"推理"},{"Type":"NodeText","Data":"来执行一项新任务。然而，对于模型是"},{"Type":"NodeTextMark","TextMarkType":"em strong","TextMarkTextContent":"如何"},{"Type":"NodeText","Data":"学习的，以及演示的"},{"Type":"NodeTextMark","TextMarkType":"em strong","TextMarkTextContent":"哪些"},{"Type":"NodeText","Data":"方面对最终任务性能有贡献，人们的理解甚少。在本文中，我们表明，"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"真实的演示实际上并非必需"},{"Type":"NodeText","Data":"——在演示中随机替换标签几乎不会损害性能，这一发现在包括GPT-3在内的12种不同模型上都表现一致。相反，我们发现演示的其他方面才是最终任务性能的"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"关键驱动因素"},{"Type":"NodeText","Data":"，包括它们提供了关于以下几个方面的少量示例：(1) "},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"标签空间"},{"Type":"NodeText","Data":"，(2) "},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"输入文本的分布"},{"Type":"NodeText","Data":"，以及 (3) "},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"序列的整体格式"},{"Type":"NodeText","Data":"。总而言之，我们的分析提供了一种理解情境学习如何以及为何起作用的新方式，同时也引出了关于大型语言模型仅通过推理能学到多少的新问题。"}]},{"ID":"20250919160945-6tpjxld","Type":"NodeBlockquote","Properties":{"fold":"1","id":"20250919160945-6tpjxld","updated":"20250919160946"},"Children":[{"Type":"NodeBlockquoteMarker","Data":"\u003e"},{"ID":"20250919160945-s8mto8i","Type":"NodeHeading","HeadingLevel":3,"Properties":{"id":"20250919160945-s8mto8i","updated":"20250919160945"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"内容解析"}]},{"ID":"20250919160945-nfhbw92","Type":"NodeParagraph","Properties":{"id":"20250919160945-nfhbw92","updated":"20250919160945"},"Children":[{"Type":"NodeText","Data":"这篇摘要颠覆了对大型语言模型（LLM）“情境学习”能力的传统认知，提出了一个非常深刻且反直觉的观点。"}]},{"ID":"20250919160945-1gw2yc2","Type":"NodeHeading","HeadingLevel":5,"Properties":{"id":"20250919160945-1gw2yc2","updated":"20250919160945"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"核心发现：标签的正确性无关紧要"}]},{"ID":"20250919160945-9murzm6","Type":"NodeList","ListData":{},"Properties":{"id":"20250919160945-9murzm6","updated":"20250919160945"},"Children":[{"ID":"20250919160945-153tzdq","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919160945-153tzdq","updated":"20250919160945"},"Children":[{"ID":"20250919160945-qzobl77","Type":"NodeParagraph","Properties":{"id":"20250919160945-qzobl77","updated":"20250919160945"},"Children":[{"Type":"NodeText","Data":"研究的最大爆点在于，提供给模型的示例（demonstrations）中，"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"输入和标签之间的对应关系是否正确，对模型的最终表现影响甚微"},{"Type":"NodeText","Data":"。"}]}]},{"ID":"20250919160945-nee8p9z","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919160945-nee8p9z","updated":"20250919160945"},"Children":[{"ID":"20250919160945-xxj6m6x","Type":"NodeParagraph","Properties":{"id":"20250919160945-xxj6m6x","updated":"20250919160945"},"Children":[{"Type":"NodeText","Data":"这意味着，即使用随机、错误的标签去构建示例，模型仍然能很好地完成新任务。这直接挑战了“模型是从示例中学习任务逻辑”这一普遍假设。"}]}]}]},{"ID":"20250919160945-9nw3zfm","Type":"NodeHeading","HeadingLevel":5,"Properties":{"id":"20250919160945-9nw3zfm","updated":"20250919160945"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"什么才是真正重要的？"}]},{"ID":"20250919160945-x66eber","Type":"NodeList","ListData":{},"Properties":{"id":"20250919160945-x66eber","updated":"20250919160945"},"Children":[{"ID":"20250919160945-j556kpq","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919160945-j556kpq","updated":"20250919160945"},"Children":[{"ID":"20250919160945-ue39qf8","Type":"NodeParagraph","Properties":{"id":"20250919160945-ue39qf8","updated":"20250919160945"},"Children":[{"Type":"NodeText","Data":"既然正确的标签不重要，那么什么才重要？作者指出了三个关键因素："}]},{"ID":"20250919160945-ya7a3kg","Type":"NodeList","ListData":{"Typ":1},"Properties":{"id":"20250919160945-ya7a3kg","updated":"20250919160945"},"Children":[{"ID":"20250919160945-7sam4hi","Type":"NodeListItem","ListData":{"Typ":1,"Delimiter":46,"Marker":"MS4=","Num":1},"Properties":{"id":"20250919160945-7sam4hi","updated":"20250919160945"},"Children":[{"ID":"20250919160945-lhh9gtn","Type":"NodeParagraph","Properties":{"id":"20250919160945-lhh9gtn","updated":"20250919160945"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"标签空间（Label Space）"},{"Type":"NodeText","Data":": 示例告诉了模型"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"答案可能是什么样子的"},{"Type":"NodeText","Data":"。例如，如果任务是情感分类，示例中出现的“积极”和“消极”这两个词，就为模型划定了输出范围。"}]}]},{"ID":"20250919160945-ayw9elw","Type":"NodeListItem","ListData":{"Typ":1,"Delimiter":46,"Marker":"Mi4=","Num":2},"Properties":{"id":"20250919160945-ayw9elw","updated":"20250919160945"},"Children":[{"ID":"20250919160945-8h7f1az","Type":"NodeParagraph","Properties":{"id":"20250919160945-8h7f1az","updated":"20250919160945"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"输入文本的分布（Distribution of the input text）"},{"Type":"NodeText","Data":": 示例展示了"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"输入文本的风格、格式和类型"},{"Type":"NodeText","Data":"。这帮助模型理解它将要处理的是什么样的数据。"}]}]},{"ID":"20250919160945-tnkf2xm","Type":"NodeListItem","ListData":{"Typ":1,"Delimiter":46,"Marker":"My4=","Num":3},"Properties":{"id":"20250919160945-tnkf2xm","updated":"20250919160945"},"Children":[{"ID":"20250919160945-a15tdjt","Type":"NodeParagraph","Properties":{"id":"20250919160945-a15tdjt","updated":"20250919160945"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"整体格式（Overall format）"},{"Type":"NodeText","Data":": 示例定义了"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"任务的交互模式"},{"Type":"NodeText","Data":"。例如，“问题：[输入] \\n 答案：[输出]”这样的格式，教会了模型如何组织输入和输出，而不是教会它具体的解题逻辑。"}]}]}]}]}]},{"ID":"20250919160945-dbrqeju","Type":"NodeHeading","HeadingLevel":5,"Properties":{"id":"20250919160945-dbrqeju","updated":"20250919160945"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"对“情境学习”的重新理解"}]},{"ID":"20250919160945-mbk1gme","Type":"NodeList","ListData":{},"Properties":{"id":"20250919160945-mbk1gme","updated":"20250919160945"},"Children":[{"ID":"20250919160945-f0yyqtp","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919160945-f0yyqtp","updated":"20250919160945"},"Children":[{"ID":"20250919160945-ju3ejzg","Type":"NodeParagraph","Properties":{"id":"20250919160945-ju3ejzg","updated":"20250919160945"},"Children":[{"Type":"NodeText","Data":"这项研究表明，LLM的情境学习可能并非真正的“学习”一个任务，而更像是一种"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"“模式匹配”或“格式遵循”"},{"Type":"NodeText","Data":" 的过程。"}]}]},{"ID":"20250919160945-vvz1sjg","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919160945-vvz1sjg","updated":"20250919160945"},"Children":[{"ID":"20250919160945-8ji5h1a","Type":"NodeParagraph","Properties":{"id":"20250919160945-8ji5h1a","updated":"20250919160945"},"Children":[{"Type":"NodeText","Data":"模型利用示例来理解任务的"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"元信息"},{"Type":"NodeText","Data":"（meta-information）——即任务的格式、输入输出的类型和范围——而不是从示例中归纳出解决任务的算法或知识。"}]}]},{"ID":"20250919160945-pbijrks","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919160945-pbijrks","updated":"20250919160945"},"Children":[{"ID":"20250919160945-w7be417","Type":"NodeParagraph","Properties":{"id":"20250919160945-w7be417","updated":"20250919160945"},"Children":[{"Type":"NodeText","Data":"这为我们理解LLM的能力和局限性提供了全新的视角，并引发了更深层次的思考：如果模型不需要正确的逻辑关联就能“解决”任务，那么它们到底在多大程度上理解了我们给出的指令？"}]}]}]}]},{"ID":"20250919160945-dbia859","Type":"NodeBlockquote","Properties":{"fold":"1","id":"20250919160945-dbia859","updated":"20250919160945"},"Children":[{"Type":"NodeBlockquoteMarker","Data":"\u003e"},{"ID":"20250919160945-juiisc7","Type":"NodeHeading","HeadingLevel":3,"Properties":{"id":"20250919160945-juiisc7","updated":"20250919160945"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"综合性解析"}]},{"ID":"20250919160945-gjx3igj","Type":"NodeParagraph","Properties":{"id":"20250919160945-gjx3igj","updated":"20250919160945"},"Children":[{"Type":"NodeText","Data":"这篇摘要的核心贡献在于，它通过实验揭示了大型语言模型（LLM）“情境学习”能力的内在机制，并得出了一个颠覆性的结论："},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"在情境学习中，模型学习的不是示例中蕴含的“知识”或“逻辑”，而是任务的“格式”和“约束”"},{"Type":"NodeText","Data":"。"}]},{"ID":"20250919160945-0a47fzx","Type":"NodeParagraph","Properties":{"id":"20250919160945-0a47fzx","updated":"20250919160945"},"Children":[{"Type":"NodeText","Data":"传统的观点认为，当我们给模型提供几个“问题-正确答案”的示例时，模型会从中学习如何解决这类问题。但该研究发现，即使把示例中的答案换成随机的、错误的标签，模型的性能也几乎不受影响。这说明模型并没有真正依赖示例中的输入-输出映射关系。"}]},{"ID":"20250919160945-3fyepz9","Type":"NodeParagraph","Properties":{"id":"20250919160945-3fyepz9","updated":"20250919160945"},"Children":[{"Type":"NodeText","Data":"相反，真正起作用的是示例所提供的"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"元信息"},{"Type":"NodeText","Data":"："}]},{"ID":"20250919160945-uzv814s","Type":"NodeList","ListData":{"Typ":1},"Properties":{"id":"20250919160945-uzv814s","updated":"20250919160945"},"Children":[{"ID":"20250919160945-upgw34f","Type":"NodeListItem","ListData":{"Typ":1,"Delimiter":46,"Marker":"MS4=","Num":1},"Properties":{"id":"20250919160945-upgw34f","updated":"20250919160945"},"Children":[{"ID":"20250919160945-il2rwlb","Type":"NodeParagraph","Properties":{"id":"20250919160945-il2rwlb","updated":"20250919160945"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"它告诉模型最终的答案应该是什么样子"},{"Type":"NodeText","Data":"（例如，是“积极”/“消极”这两个词，还是一个数字）。"}]}]},{"ID":"20250919160945-2vwd5ls","Type":"NodeListItem","ListData":{"Typ":1,"Delimiter":46,"Marker":"Mi4=","Num":2},"Properties":{"id":"20250919160945-2vwd5ls","updated":"20250919160945"},"Children":[{"ID":"20250919160945-xrlwgjp","Type":"NodeParagraph","Properties":{"id":"20250919160945-xrlwgjp","updated":"20250919160945"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"它告诉模型输入的文本大概是什么风格"},{"Type":"NodeText","Data":"。"}]}]},{"ID":"20250919160945-ei3a7r5","Type":"NodeListItem","ListData":{"Typ":1,"Delimiter":46,"Marker":"My4=","Num":3},"Properties":{"id":"20250919160945-ei3a7r5","updated":"20250919160945"},"Children":[{"ID":"20250919160945-p0zvmz1","Type":"NodeParagraph","Properties":{"id":"20250919160945-p0zvmz1","updated":"20250919160945"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"它告诉模型应该遵循什么样的问答格式"},{"Type":"NodeText","Data":"。"}]}]}]},{"ID":"20250919160945-b0xtm7q","Type":"NodeParagraph","Properties":{"id":"20250919160945-b0xtm7q","updated":"20250919160945"},"Children":[{"Type":"NodeText","Data":"简单来说，模型通过示例学会了“"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"如何作答"},{"Type":"NodeText","Data":"”，而不是“"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"为何这样作答"},{"Type":"NodeText","Data":"”。这一发现深刻地改变了我们对“情境学习”的理解，从一个看似智能的“快速学习”过程，转变为一个更偏向于“高级格式塔匹配”的过程。它促使我们重新审视LLM的能力边界，并对如何更有效地引导和利用这些模型提出了新的问题。"}]}]},{"ID":"20250919161110-2d18as7","Type":"NodeHeading","HeadingLevel":3,"Properties":{"id":"20250919161110-2d18as7","updated":"20250920134433"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"1 引入"}]},{"ID":"20250919161110-y6mns6f","Type":"NodeParagraph","Properties":{"id":"20250919161110-y6mns6f","updated":"20250920131638"},"Children":[{"Type":"NodeText","Data":"大型语言模型（LMs）已展现出令人印象深刻的能力，它们仅通过对少数输入-标签对（即"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"演示"},{"Type":"NodeText","Data":"）进行条件化，就能在下游任务上表现出色；这种类型的推理被称为"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"情境学习（in-context learning）"},{"Type":"NodeText","Data":"（Brown et al., 2020）。尽管情境学习在广泛的任务中持续优于零样本推理（Zhao et al., 2021; Liu et al., 2021），但关于它"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"如何"},{"Type":"NodeText","Data":"运作以及演示的"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"哪些"},{"Type":"NodeText","Data":"方面对最终任务性能有贡献，我们知之甚少。"}]},{"ID":"20250919161110-zngjpuo","Type":"NodeParagraph","Properties":{"id":"20250919161110-zngjpuo","updated":"20250919161110"},"Children":[{"Type":"NodeText","Data":"在本文中，我们表明，对于有效的情境学习而言，"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"真实的演示实际上并非必需"},{"Type":"NodeText","Data":"（第4节）。具体来说，在演示中用"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"随机标签"},{"Type":"NodeText","Data":"替换原始标签几乎不会损害性能（图1）。这一结果在包括GPT-3家族在内的12种不同模型上表现一致（Radford et al., 2019; Min et al., 2021b; Wang and Komatsuzaki, 2021; Artetxe et al., 2021; Brown et al., 2020）。这有力地、且有悖常理地表明，模型在执行任务时，"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"并"},{"Type":"NodeTextMark","TextMarkType":"strong em","TextMarkTextContent":"不"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"依赖于演示中的输入-标签映射关系"},{"Type":"NodeText","Data":"。"}]},{"ID":"20250919161141-2yetj5w","Type":"NodeParagraph","Properties":{"id":"20250919161141-2yetj5w","updated":"20250919161141"},"Children":[{"Type":"NodeText","Data":"​"},{"Type":"NodeImage","Data":"span","Children":[{"Type":"NodeBang"},{"Type":"NodeOpenBracket"},{"Type":"NodeLinkText","Data":"image"},{"Type":"NodeCloseBracket"},{"Type":"NodeOpenParen"},{"Type":"NodeLinkDest","Data":"assets/image-20250919161141-pinex4i.png"},{"Type":"NodeCloseParen"}]},{"Type":"NodeText","Data":"​"}]},{"ID":"20250919161110-kry37wn","Type":"NodeBlockquote","Properties":{"fold":"1","id":"20250919161110-kry37wn","updated":"20250919161110"},"Children":[{"Type":"NodeBlockquoteMarker","Data":"\u003e"},{"ID":"20250919161110-pdajek0","Type":"NodeHeading","HeadingLevel":5,"Properties":{"id":"20250919161110-pdajek0","updated":"20250919161110"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"图1：在分类（顶部）和多项选择（底部）任务中的结果"}]},{"ID":"20250919161110-e4glm0y","Type":"NodeParagraph","Properties":{"id":"20250919161110-e4glm0y","updated":"20250919161110"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"图片内容翻译:"}]},{"ID":"20250919161110-lavk6xl","Type":"NodeList","ListData":{},"Properties":{"id":"20250919161110-lavk6xl","updated":"20250919161110"},"Children":[{"ID":"20250919161110-ccn3u4y","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919161110-ccn3u4y","updated":"20250919161110"},"Children":[{"ID":"20250919161110-fmsyyx1","Type":"NodeParagraph","Properties":{"id":"20250919161110-fmsyyx1","updated":"20250919161110"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"图例:"}]},{"ID":"20250919161110-9vexiv0","Type":"NodeList","ListData":{},"Properties":{"id":"20250919161110-9vexiv0","updated":"20250919161110"},"Children":[{"ID":"20250919161110-y4tx4nm","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919161110-y4tx4nm","updated":"20250919161110"},"Children":[{"ID":"20250919161110-j2odakk","Type":"NodeParagraph","Properties":{"id":"20250919161110-j2odakk","updated":"20250919161110"},"Children":[{"Type":"NodeText","Data":"蓝色 (No Demos): 无演示"}]}]},{"ID":"20250919161110-rsbfk9k","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919161110-rsbfk9k","updated":"20250919161110"},"Children":[{"ID":"20250919161110-r56nsa9","Type":"NodeParagraph","Properties":{"id":"20250919161110-r56nsa9","updated":"20250919161110"},"Children":[{"Type":"NodeText","Data":"橙色 (Demos w/ gold labels): 使用黄金标签的演示（即正确的标签）"}]}]},{"ID":"20250919161110-wccrag4","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919161110-wccrag4","updated":"20250919161110"},"Children":[{"ID":"20250919161110-jch3wgy","Type":"NodeParagraph","Properties":{"id":"20250919161110-jch3wgy","updated":"20250919161110"},"Children":[{"Type":"NodeText","Data":"红色 (Demos w/ random labels): 使用随机标签的演示"}]}]}]}]},{"ID":"20250919161110-rxkceba","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919161110-rxkceba","updated":"20250919161110"},"Children":[{"ID":"20250919161110-19ve24s","Type":"NodeParagraph","Properties":{"id":"20250919161110-19ve24s","updated":"20250919161110"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"顶部图表: 分类 (Classification)"}]},{"ID":"20250919161110-26bkt8i","Type":"NodeList","ListData":{},"Properties":{"id":"20250919161110-26bkt8i","updated":"20250919161110"},"Children":[{"ID":"20250919161110-r43ravw","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919161110-r43ravw","updated":"20250919161110"},"Children":[{"ID":"20250919161110-fpsnchc","Type":"NodeParagraph","Properties":{"id":"20250919161110-fpsnchc","updated":"20250919161110"},"Children":[{"Type":"NodeText","Data":"纵坐标: Macro-F1 (%)，一种衡量分类性能的指标，越高越好。"}]}]}]}]},{"ID":"20250919161110-9gpf85j","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919161110-9gpf85j","updated":"20250919161110"},"Children":[{"ID":"20250919161110-msvzjkt","Type":"NodeParagraph","Properties":{"id":"20250919161110-msvzjkt","updated":"20250919161110"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"底部图表: 多项选择 (Multi-choice)"}]},{"ID":"20250919161110-ylmf9iw","Type":"NodeList","ListData":{},"Properties":{"id":"20250919161110-ylmf9iw","updated":"20250919161110"},"Children":[{"ID":"20250919161110-7tlpd2c","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919161110-7tlpd2c","updated":"20250919161110"},"Children":[{"ID":"20250919161110-yyge0dz","Type":"NodeParagraph","Properties":{"id":"20250919161110-yyge0dz","updated":"20250919161110"},"Children":[{"Type":"NodeText","Data":"纵坐标: 准确率 (%) (Accuracy (%))，越高越好。"}]}]}]}]},{"ID":"20250919161110-su83lqh","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919161110-su83lqh","updated":"20250919161110"},"Children":[{"ID":"20250919161110-dgv4wgj","Type":"NodeParagraph","Properties":{"id":"20250919161110-dgv4wgj","updated":"20250919161110"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"横坐标:"}]},{"ID":"20250919161110-iwncjaf","Type":"NodeList","ListData":{},"Properties":{"id":"20250919161110-iwncjaf","updated":"20250919161110"},"Children":[{"ID":"20250919161110-a1voggq","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919161110-a1voggq","updated":"20250919161110"},"Children":[{"ID":"20250919161110-azbqr06","Type":"NodeParagraph","Properties":{"id":"20250919161110-azbqr06","updated":"20250919161110"},"Children":[{"Type":"NodeText","Data":"MetaICL (774M): 7.74亿参数模型"}]}]},{"ID":"20250919161110-gkwpcoc","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919161110-gkwpcoc","updated":"20250919161110"},"Children":[{"ID":"20250919161110-r4bkvsh","Type":"NodeParagraph","Properties":{"id":"20250919161110-r4bkvsh","updated":"20250919161110"},"Children":[{"Type":"NodeText","Data":"GPT-J (6B): 60亿参数模型"}]}]},{"ID":"20250919161110-pwp4rxh","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919161110-pwp4rxh","updated":"20250919161110"},"Children":[{"ID":"20250919161110-0d830tv","Type":"NodeParagraph","Properties":{"id":"20250919161110-0d830tv","updated":"20250919161110"},"Children":[{"Type":"NodeText","Data":"GPT-3 (175B): 1750亿参数模型"}]}]}]}]},{"ID":"20250919161110-hvjv7sj","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919161110-hvjv7sj","updated":"20250919161110"},"Children":[{"ID":"20250919161110-bmcm1lb","Type":"NodeParagraph","Properties":{"id":"20250919161110-bmcm1lb","updated":"20250919161110"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"图解说明:"},{"Type":"NodeText","Data":" 在分类（顶部）和多项选择（底部）任务中的结果。使用了三种不同规模的LM。报告了GPT-3评估所用的六个数据集上的结果；使用的是通道方法。详见第4节。"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"当演示中的标签被替换为随机标签时，情境学习的性能仅有轻微下降"},{"Type":"NodeText","Data":"。"}]}]}]},{"ID":"20250919161110-mct2gls","Type":"NodeHeading","HeadingLevel":3,"Properties":{"id":"20250919161110-mct2gls","updated":"20250919161110"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"内容解析"}]},{"ID":"20250919161110-k6mx3qi","Type":"NodeList","ListData":{},"Properties":{"id":"20250919161110-k6mx3qi","updated":"20250919161110"},"Children":[{"ID":"20250919161110-yf8m1bw","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919161110-yf8m1bw","updated":"20250919161110"},"Children":[{"ID":"20250919161110-9uo9864","Type":"NodeParagraph","Properties":{"id":"20250919161110-9uo9864","updated":"20250919161110"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"核心发现的可视化"},{"Type":"NodeText","Data":": 这张图是本文核心论点的最有力证据。通过对比橙色条（正确标签）和红色条（随机标签），我们可以清晰地看到，在所有模型和任务上，两者的性能都"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"惊人地接近"},{"Type":"NodeText","Data":"。这直观地证明了，"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"标签是否正确，对模型的最终表现影响甚微"},{"Type":"NodeText","Data":"。"}]}]},{"ID":"20250919161110-z9k3nos","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919161110-z9k3nos","updated":"20250919161110"},"Children":[{"ID":"20250919161110-2as554d","Type":"NodeParagraph","Properties":{"id":"20250919161110-2as554d","updated":"20250919161110"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"“演示”本身的重要性"},{"Type":"NodeText","Data":": 对比蓝色条（无演示）和另外两个（有演示），可以看出，"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"提供任何形式的演示都远胜于不提供"},{"Type":"NodeText","Data":"。这说明“演示”这个行为本身是至关重要的，但重要的不是其内在的逻辑关系。"}]}]},{"ID":"20250919161110-q25bw3d","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919161110-q25bw3d","updated":"20250919161110"},"Children":[{"ID":"20250919161110-3m9cabp","Type":"NodeParagraph","Properties":{"id":"20250919161110-3m9cabp","updated":"20250919161110"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"规模效应 (Scaling Effect)"},{"Type":"NodeText","Data":": 随着模型从7.74亿参数（MetaICL）增长到1750亿参数（GPT-3），所有条的高度都在上升，说明"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"模型越大，任务性能越好"},{"Type":"NodeText","Data":"。更重要的是，有演示（橙/红）相比无演示（蓝）的性能提升幅度也随模型规模的增大而变大。这表明，"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"越大的模型，越能有效利用演示所提供的“元信息”"},{"Type":"NodeText","Data":"。"}]}]},{"ID":"20250919161110-sm7jewc","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919161110-sm7jewc","updated":"20250919161110"},"Children":[{"ID":"20250919161110-45ncbia","Type":"NodeParagraph","Properties":{"id":"20250919161110-45ncbia","updated":"20250919161110"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"总结"},{"Type":"NodeText","Data":": 该图表有力地反驳了“模型从示例中学习如何解决问题”的传统观点，并为作者的新理论——模型学习的是格式、标签空间和输入分布——奠定了坚实的实验基础。"}]}]}]}]},{"ID":"20250919161110-aq8ybuh","Type":"NodeParagraph","Properties":{"id":"20250919161110-aq8ybuh","updated":"20250919161110"},"Children":[{"Type":"NodeText","Data":"进一步的分析调查了演示的哪些部分确实对性能有贡献。我们确定了演示的可能方面（例如，标签空间和输入文本的分布），并评估了一系列变体来量化每个方面的影响（第5节）。我们发现：(1) "},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"由演示指定的标签空间和输入文本的分布是情境学习的关键"},{"Type":"NodeText","Data":"（无论标签对于单个输入是否正确）；(2) "},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"指定整体格式也至关重要"},{"Type":"NodeText","Data":"，例如，在标签空间未知时，使用随机的英文单词作为标签也显著优于不使用任何标签；以及 (3) 使用情境学习目标进行"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"元训练（meta-training）"},{"Type":"NodeText","Data":"（Min et al., 2021b）会放大这些效应——模型几乎完全利用演示中更简单的方面，如"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"格式"},{"Type":"NodeText","Data":"，而不是输入-标签的映射关系。"}]},{"ID":"20250919161110-u5umlyh","Type":"NodeParagraph","Properties":{"id":"20250919161110-u5umlyh","updated":"20250919161110"},"Children":[{"Type":"NodeText","Data":"总而言之，我们的分析提供了一种理解演示在情境学习中作用的新方式。我们通过经验证明，模型 (1) 与我们想象的不同，"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"它反直觉地并不依赖演示中提供的真实输入-标签映射"},{"Type":"NodeText","Data":"（第4节），并且 (2) "},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"尽管如此，它仍然能从演示所指定的标签空间和输入分布中获益"},{"Type":"NodeText","Data":"（第5节）。我们还讨论了更广泛的影响，例如，关于模型在测试时学习了什么，我们能说些什么，以及未来的工作方向（第6节）。"}]},{"ID":"20250919161110-4tgpfdm","Type":"NodeBlockquote","Properties":{"fold":"1","id":"20250919161110-4tgpfdm","updated":"20250919161110"},"Children":[{"Type":"NodeBlockquoteMarker","Data":"\u003e"},{"ID":"20250919161110-qbyaznx","Type":"NodeHeading","HeadingLevel":3,"Properties":{"id":"20250919161110-qbyaznx","updated":"20250919161110"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"内容解析"}]},{"ID":"20250919161110-u7ks44v","Type":"NodeHeading","HeadingLevel":5,"Properties":{"id":"20250919161110-u7ks44v","updated":"20250919161110"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"颠覆性观点的提出"}]},{"ID":"20250919161110-8ljxml4","Type":"NodeList","ListData":{},"Properties":{"id":"20250919161110-8ljxml4","updated":"20250919161110"},"Children":[{"ID":"20250919161110-2dmt0ey","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919161110-2dmt0ey","updated":"20250919161110"},"Children":[{"ID":"20250919161110-dtq84hk","Type":"NodeParagraph","Properties":{"id":"20250919161110-dtq84hk","updated":"20250919161110"},"Children":[{"Type":"NodeText","Data":"引言部分开门见山，直接挑战了“情境学习”的核心假设。作者明确指出，大家普遍认为模型是通过学习示例中的“输入-正确标签”关系来完成任务的，但他们的研究发现，"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"这种关系并非必要"},{"Type":"NodeText","Data":"。"}]}]},{"ID":"20250919161110-f2o3w5l","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919161110-f2o3w5l","updated":"20250919161110"},"Children":[{"ID":"20250919161110-1c1nebl","Type":"NodeParagraph","Properties":{"id":"20250919161110-1c1nebl","updated":"20250919161110"},"Children":[{"Type":"NodeText","Data":"通过引用图1，作者给出了一个极具冲击力的初步证据："},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"用随机、错误的标签替换掉示例中的正确标签，模型的性能几乎不受影响"},{"Type":"NodeText","Data":"。"}]}]}]},{"ID":"20250919161110-nfzk5k8","Type":"NodeHeading","HeadingLevel":5,"Properties":{"id":"20250919161110-nfzk5k8","updated":"20250919161110"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"从“是什么”到“为什么”"}]},{"ID":"20250919161110-t4nku1r","Type":"NodeList","ListData":{},"Properties":{"id":"20250919161110-t4nku1r","updated":"20250919161110"},"Children":[{"ID":"20250919161110-rw5hsuk","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919161110-rw5hsuk","updated":"20250919161110"},"Children":[{"ID":"20250919161110-srbsek4","Type":"NodeParagraph","Properties":{"id":"20250919161110-srbsek4","updated":"20250919161110"},"Children":[{"Type":"NodeText","Data":"在抛出颠覆性观点后，作者立即转向探索性问题：如果输入-标签映射不重要，那什么才是真正重要的？"}]}]},{"ID":"20250919161110-s9ekydh","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919161110-s9ekydh","updated":"20250919161110"},"Children":[{"ID":"20250919161110-8i3rpwl","Type":"NodeParagraph","Properties":{"id":"20250919161110-8i3rpwl","updated":"20250919161110"},"Children":[{"Type":"NodeText","Data":"他们系统性地拆解了“演示”可能包含的信息，并总结出三个关键因素："}]},{"ID":"20250919161110-lw9u4g4","Type":"NodeList","ListData":{"Typ":1},"Properties":{"id":"20250919161110-lw9u4g4","updated":"20250919161110"},"Children":[{"ID":"20250919161110-2o40eek","Type":"NodeListItem","ListData":{"Typ":1,"Delimiter":46,"Marker":"MS4=","Num":1},"Properties":{"id":"20250919161110-2o40eek","updated":"20250919161110"},"Children":[{"ID":"20250919161110-ydpuut1","Type":"NodeParagraph","Properties":{"id":"20250919161110-ydpuut1","updated":"20250919161110"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"内容分布"},{"Type":"NodeText","Data":": 演示提供了"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"标签空间"},{"Type":"NodeText","Data":"（答案长什么样）和"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"输入文本的分布"},{"Type":"NodeText","Data":"（问题长什么样）。"}]}]},{"ID":"20250919161110-9nbzdu6","Type":"NodeListItem","ListData":{"Typ":1,"Delimiter":46,"Marker":"Mi4=","Num":2},"Properties":{"id":"20250919161110-9nbzdu6","updated":"20250919161110"},"Children":[{"ID":"20250919161110-3tyokm7","Type":"NodeParagraph","Properties":{"id":"20250919161110-3tyokm7","updated":"20250919161110"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"结构格式"},{"Type":"NodeText","Data":": 演示定义了"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"任务的交互范式"},{"Type":"NodeText","Data":"（例如，一问一答的格式）。"}]}]},{"ID":"20250919161110-luf1ne3","Type":"NodeListItem","ListData":{"Typ":1,"Delimiter":46,"Marker":"My4=","Num":3},"Properties":{"id":"20250919161110-luf1ne3","updated":"20250919161110"},"Children":[{"ID":"20250919161110-93k7f32","Type":"NodeParagraph","Properties":{"id":"20250919161110-93k7f32","updated":"20250919161110"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"训练效应"},{"Type":"NodeText","Data":": 如果模型在预训练阶段就接触过类似的情境学习任务（即"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"元训练"},{"Type":"NodeText","Data":"），它会更加倾向于依赖这种格式信息，而非逻辑关系。"}]}]}]}]}]},{"ID":"20250919161110-q0y3l76","Type":"NodeHeading","HeadingLevel":5,"Properties":{"id":"20250919161110-q0y3l76","updated":"20250919161110"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"清晰的论文路线图"}]},{"ID":"20250919161110-yzu2iv6","Type":"NodeList","ListData":{},"Properties":{"id":"20250919161110-yzu2iv6","updated":"20250919161110"},"Children":[{"ID":"20250919161110-wb1oljg","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919161110-wb1oljg","updated":"20250919161110"},"Children":[{"ID":"20250919161110-41g7t9w","Type":"NodeParagraph","Properties":{"id":"20250919161110-41g7t9w","updated":"20250919161110"},"Children":[{"Type":"NodeText","Data":"引言的最后一部分清晰地勾勒出了全文的结构，引导读者跟随作者的思路：首先通过实验证明核心观点（第4节），然后深入分析背后的原因（第5节），最后讨论其深远影响和未来方向（第6节）。"}]}]}]}]},{"ID":"20250919161110-029yuy7","Type":"NodeBlockquote","Properties":{"fold":"1","id":"20250919161110-029yuy7","updated":"20250919161110"},"Children":[{"Type":"NodeBlockquoteMarker","Data":"\u003e"},{"ID":"20250919161110-bzntbvo","Type":"NodeHeading","HeadingLevel":3,"Properties":{"id":"20250919161110-bzntbvo","updated":"20250919161110"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"综合性解析"}]},{"ID":"20250919161110-7bnjt5z","Type":"NodeParagraph","Properties":{"id":"20250919161110-7bnjt5z","updated":"20250919161110"},"Children":[{"Type":"NodeText","Data":"本文的引言部分完成了一项出色的任务：它以一个"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"极具颠覆性和反直觉的观点"},{"Type":"NodeText","Data":"开篇，并立即用一张"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"强有力的、一目了然的图表（图1）"},{"Type":"NodeText","Data":" 来支撑这一观点，从而迅速抓住了读者的注意力。"}]},{"ID":"20250919161110-5poiq94","Type":"NodeParagraph","Properties":{"id":"20250919161110-5poiq94","updated":"20250919161110"},"Children":[{"Type":"NodeText","Data":"核心论点是，大型语言模型的“情境学习”能力可能被误解了。它并非一个真正的“学习”过程，即从示例中归纳出解决问题的逻辑规则。相反，它更像是一个“"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"格式匹配"},{"Type":"NodeText","Data":"”和“"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"模式识别"},{"Type":"NodeText","Data":"”的过程。模型利用演示来搞清楚三件事："}]},{"ID":"20250919161110-ab9jyvx","Type":"NodeList","ListData":{"Typ":1},"Properties":{"id":"20250919161110-ab9jyvx","updated":"20250919161110"},"Children":[{"ID":"20250919161110-xrqihd8","Type":"NodeListItem","ListData":{"Typ":1,"Delimiter":46,"Marker":"MS4=","Num":1},"Properties":{"id":"20250919161110-xrqihd8","updated":"20250919161110"},"Children":[{"ID":"20250919161110-3fozkay","Type":"NodeParagraph","Properties":{"id":"20250919161110-3fozkay","updated":"20250919161110"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"我应该回答什么类型的东西？"},{"Type":"NodeText","Data":"（标签空间）"}]}]},{"ID":"20250919161110-0lcyt6h","Type":"NodeListItem","ListData":{"Typ":1,"Delimiter":46,"Marker":"Mi4=","Num":2},"Properties":{"id":"20250919161110-0lcyt6h","updated":"20250919161110"},"Children":[{"ID":"20250919161110-4w8b5oh","Type":"NodeParagraph","Properties":{"id":"20250919161110-4w8b5oh","updated":"20250919161110"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"我正在处理什么类型的输入？"},{"Type":"NodeText","Data":"（输入分布）"}]}]},{"ID":"20250919161110-gnyrf1c","Type":"NodeListItem","ListData":{"Typ":1,"Delimiter":46,"Marker":"My4=","Num":3},"Properties":{"id":"20250919161110-gnyrf1c","updated":"20250919161110"},"Children":[{"ID":"20250919161110-dh81m1u","Type":"NodeParagraph","Properties":{"id":"20250919161110-dh81m1u","updated":"20250919161110"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"我应该以什么样的格式来组织我的回答？"},{"Type":"NodeText","Data":"（整体格式）"}]}]}]},{"ID":"20250919161110-dk6hpd2","Type":"NodeParagraph","Properties":{"id":"20250919161110-dk6hpd2","updated":"20250919161110"},"Children":[{"Type":"NodeText","Data":"图1中的实验结果是这一论点的基石。当发现用“随机标签”和“黄金标签”得到的结果几乎没差别时，这就强烈暗示了模型并没有关心标签的正确性，即它没有在学习“输入X -\u003e 输出Y”这个映射。然而，这两种情况都远比“无演示”要好，这又说明演示的"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"存在"},{"Type":"NodeText","Data":"本身至关重要。"}]},{"ID":"20250919161110-k2ycawx","Type":"NodeParagraph","Properties":{"id":"20250919161110-k2ycawx","updated":"20250919161110"},"Children":[{"Type":"NodeText","Data":"综上所述，这篇引言成功地将一个复杂的概念问题，转化为了一个可以通过实验验证的清晰假设，并为全文的探索——即重新定义和理解情境学习的真正机制——铺平了道路。它告诉我们，我们与大型语言模型的交互，可能更多的是在"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"引导"},{"Type":"NodeText","Data":"其遵循一个预设的结构，而不是在"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"教会"},{"Type":"NodeText","Data":"它新的知识。"}]}]},{"ID":"20250919161156-0ryippg","Type":"NodeHeading","HeadingLevel":3,"Properties":{"id":"20250919161156-0ryippg","updated":"20250920134433"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"2 相关工作"}]},{"ID":"20250919161156-26kzgyc","Type":"NodeParagraph","Properties":{"id":"20250919161156-26kzgyc","updated":"20250919161157"},"Children":[{"Type":"NodeText","Data":"大型语言模型已在广泛的下游任务中展现出强大的性能 (Devlin et al., 2019; Radford et al., 2019; Liu et al., 2019; Raffel et al., 2020; Lewis et al., 2020)。虽然"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"微调（finetuning）"},{"Type":"NodeText","Data":" 一直是向新任务迁移的流行方法 (Devlin et al., 2019)，但对一个非常大的模型（例如，参数量超过100亿）进行微调通常是"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"不切实际的"},{"Type":"NodeText","Data":"。Brown等人 (2020) 提出了"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"情境学习（in-context learning）"},{"Type":"NodeText","Data":" 作为学习新任务的替代方法。如图2所示，语言模型（LM）仅通过对训练数据的拼接（作为演示）进行条件化，"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"无需任何梯度更新"},{"Type":"NodeText","Data":"，就能"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"仅通过推理"},{"Type":"NodeText","Data":"来学习新任务。"}]},{"ID":"20250919161156-90b97fi","Type":"NodeParagraph","Properties":{"id":"20250919161156-90b97fi","updated":"20250919161157"},"Children":[{"Type":"NodeText","Data":"自被提出以来，情境学习一直是重要研究的焦点。先前的工作提出了更好的问题构建方式 (Zhao et al., 2021; Holtzman et al., 2021; Min et al., 2021a)，更好的演示带标签样本选择方法 (Liu et al., 2021; Lu et al., 2021; Rubin et al., 2021)，使用显式情境学习目标进行"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"元训练（meta-training）"},{"Type":"NodeText","Data":" (Chen et al., 2021; Min et al., 2021b)，以及将学习遵循指令作为情境学习的一种变体 (Mishra et al., 2021b; Efrat and Levy, 2020; Wei et al., 2022; Sanh et al., 2022)。与此同时，一些工作也报告了情境学习的"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"脆弱性（brittleness）"},{"Type":"NodeText","Data":" 和"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"过敏感性（over-sensitivity）"},{"Type":"NodeText","Data":" (Lu et al., 2021; Zhao et al., 2021; Mishra et al., 2021a)。"}]},{"ID":"20250919161156-valgkkk","Type":"NodeParagraph","Properties":{"id":"20250919161156-valgkkk","updated":"20250919161156"},"Children":[{"Type":"NodeText","Data":"关于情境学习为何起作用的研究相对较少。Xie等人 (2022) 提供了理论分析，认为情境学习可以被形式化为"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"贝叶斯推理"},{"Type":"NodeText","Data":"，即利用演示来恢复潜在概念。Razeghi等人 (2022) 表明，在情境学习中，性能与预训练语料中的"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"词频"},{"Type":"NodeText","Data":"高度相关。据我们所知，本文是第一个提供"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"实证分析"},{"Type":"NodeText","Data":"的研究，旨在调查为什么情境学习能实现超越零样本推理的性能增益。我们发现，演示中的"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"真实输入-标签映射"},{"Type":"NodeText","Data":"只起到微不足道的作用，并衡量了演示中更细粒度方面的影响。"}]},{"ID":"20250919161225-zigebon","Type":"NodeParagraph","Properties":{"id":"20250919161225-zigebon","updated":"20250919161225"},"Children":[{"Type":"NodeText","Data":"​"},{"Type":"NodeImage","Data":"span","Children":[{"Type":"NodeBang"},{"Type":"NodeOpenBracket"},{"Type":"NodeLinkText","Data":"image"},{"Type":"NodeCloseBracket"},{"Type":"NodeOpenParen"},{"Type":"NodeLinkDest","Data":"assets/image-20250919161225-eky478i.png"},{"Type":"NodeCloseParen"}]},{"Type":"NodeText","Data":"​"}]},{"ID":"20250919161156-ayl8y66","Type":"NodeBlockquote","Properties":{"fold":"1","id":"20250919161156-ayl8y66","updated":"20250919161157"},"Children":[{"Type":"NodeBlockquoteMarker","Data":"\u003e"},{"ID":"20250919161156-smztwx9","Type":"NodeHeading","HeadingLevel":5,"Properties":{"id":"20250919161156-smztwx9","updated":"20250919161156"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"图2：情境学习概览"}]},{"ID":"20250919161156-8oznmnf","Type":"NodeParagraph","Properties":{"id":"20250919161156-8oznmnf","updated":"20250919161156"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"图片内容翻译:"}]},{"ID":"20250919161156-umjtfdm","Type":"NodeList","ListData":{},"Properties":{"id":"20250919161156-umjtfdm","updated":"20250919161156"},"Children":[{"ID":"20250919161156-b358oay","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919161156-b358oay","updated":"20250919161156"},"Children":[{"ID":"20250919161156-cuyo5af","Type":"NodeParagraph","Properties":{"id":"20250919161156-cuyo5af","updated":"20250919161156"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"演示 (Demonstrations):"}]},{"ID":"20250919161156-3uwkf26","Type":"NodeList","ListData":{},"Properties":{"id":"20250919161156-3uwkf26","updated":"20250919161156"},"Children":[{"ID":"20250919161156-e3t3duf","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919161156-e3t3duf","updated":"20250919161156"},"Children":[{"ID":"20250919161156-12rz846","Type":"NodeParagraph","Properties":{"id":"20250919161156-12rz846","updated":"20250919161156"},"Children":[{"Type":"NodeText","Data":"芬兰的发行收入增长了5%。\\n -\u003e 正面 (Positive)"}]}]},{"ID":"20250919161156-2s1336a","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919161156-2s1336a","updated":"20250919161156"},"Children":[{"ID":"20250919161156-qsto95a","Type":"NodeParagraph","Properties":{"id":"20250919161156-qsto95a","updated":"20250919161156"},"Children":[{"Type":"NodeText","Data":"Panostaja没有披露收购价格。\\n -\u003e 中性 (Neutral)"}]}]},{"ID":"20250919161156-swpv6wf","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919161156-swpv6wf","updated":"20250919161156"},"Children":[{"ID":"20250919161156-gw1uumy","Type":"NodeParagraph","Properties":{"id":"20250919161156-gw1uumy","updated":"20250919161156"},"Children":[{"Type":"NodeText","Data":"偿还国债将是极其痛苦的。\\n -\u003e 负面 (Negative)"}]}]}]}]},{"ID":"20250919161156-5425ibs","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919161156-5425ibs","updated":"20250919161156"},"Children":[{"ID":"20250919161156-2l7vmw6","Type":"NodeParagraph","Properties":{"id":"20250919161156-2l7vmw6","updated":"20250919161156"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"测试输入 (Test input):"}]},{"ID":"20250919161156-lkibfw0","Type":"NodeList","ListData":{},"Properties":{"id":"20250919161156-lkibfw0","updated":"20250919161156"},"Children":[{"ID":"20250919161156-0l92arw","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919161156-0l92arw","updated":"20250919161156"},"Children":[{"ID":"20250919161156-ekoelzx","Type":"NodeParagraph","Properties":{"id":"20250919161156-ekoelzx","updated":"20250919161156"},"Children":[{"Type":"NodeText","Data":"这次收购将产生极其积极的影响。"}]}]}]}]},{"ID":"20250919161156-ev14nyu","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919161156-ev14nyu","updated":"20250919161156"},"Children":[{"ID":"20250919161156-afoyejb","Type":"NodeParagraph","Properties":{"id":"20250919161156-afoyejb","updated":"20250919161156"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"语言模型 (LM)"}]}]},{"ID":"20250919161156-3e9dp2j","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919161156-3e9dp2j","updated":"20250919161156"},"Children":[{"ID":"20250919161156-amm9jk9","Type":"NodeParagraph","Properties":{"id":"20250919161156-amm9jk9","updated":"20250919161156"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"预测 (Prediction):"}]},{"ID":"20250919161156-in3tisp","Type":"NodeList","ListData":{},"Properties":{"id":"20250919161156-in3tisp","updated":"20250919161156"},"Children":[{"ID":"20250919161156-r3t3vay","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919161156-r3t3vay","updated":"20250919161156"},"Children":[{"ID":"20250919161156-dkioviq","Type":"NodeParagraph","Properties":{"id":"20250919161156-dkioviq","updated":"20250919161156"},"Children":[{"Type":"NodeText","Data":"正面 (Positive)"}]}]}]}]},{"ID":"20250919161156-stradzn","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919161156-stradzn","updated":"20250919161156"},"Children":[{"ID":"20250919161156-orl1dx3","Type":"NodeParagraph","Properties":{"id":"20250919161156-orl1dx3","updated":"20250919161156"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"图解说明:"},{"Type":"NodeText","Data":" 情境学习概览。演示由k个来自训练数据的输入-标签对组成（图中k=3）。"}]}]}]},{"ID":"20250919161156-z8dd5v5","Type":"NodeHeading","HeadingLevel":3,"Properties":{"id":"20250919161156-z8dd5v5","updated":"20250919161156"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"内容解析"}]},{"ID":"20250919161156-hrqhvof","Type":"NodeList","ListData":{},"Properties":{"id":"20250919161156-hrqhvof","updated":"20250919161156"},"Children":[{"ID":"20250919161156-sr03r32","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919161156-sr03r32","updated":"20250919161156"},"Children":[{"ID":"20250919161156-pk7tuyo","Type":"NodeParagraph","Properties":{"id":"20250919161156-pk7tuyo","updated":"20250919161156"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"直观解释"},{"Type":"NodeText","Data":": 这张图非常直观地展示了“情境学习”的运作流程。它不是通过更新模型权重来“学习”，而是将"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"任务示例（演示）和新的问题（测试输入）打包成一个长的提示（prompt）"},{"Type":"NodeText","Data":" ，然后一次性输入给语言模型，让模型根据这个上下文“照葫芦画瓢”地生成答案。"}]}]},{"ID":"20250919161156-flwy8kd","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919161156-flwy8kd","updated":"20250919161156"},"Children":[{"ID":"20250919161156-flac2ck","Type":"NodeParagraph","Properties":{"id":"20250919161156-flac2ck","updated":"20250919161156"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"无梯度更新"},{"Type":"NodeText","Data":": 图中没有反向传播或参数更新的步骤，只有一个从输入到输出的"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"前向推理"},{"Type":"NodeText","Data":"过程。这是情境学习与传统微调（Finetuning）最根本的区别，后者需要通过梯度下降来调整模型参数。"}]}]},{"ID":"20250919161156-5yx1rs1","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919161156-5yx1rs1","updated":"20250919161156"},"Children":[{"ID":"20250919161156-uzzm868","Type":"NodeParagraph","Properties":{"id":"20250919161156-uzzm868","updated":"20250919161156"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"格式的重要性"},{"Type":"NodeText","Data":": 该图也暗示了"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"格式"},{"Type":"NodeText","Data":"在情境学习中的关键作用。示例都遵循“"},{"Type":"NodeTextMark","TextMarkType":"code","TextMarkTextContent":"句子 \\n -\u0026gt; 标签"},{"Type":"NodeText","Data":"​”的模式。当模型看到新的测试输入时，它会遵循这个已建立的模式来生成预测。这与本文的核心观点——模型学习的是格式而非逻辑——不谋而合。"}]}]}]}]},{"ID":"20250919161156-mh22nxh","Type":"NodeTable","TableAligns":[1,1,1,1],"Properties":{"colgroup":"|||","id":"20250919161156-mh22nxh","updated":"20250919161157"},"Children":[{"Type":"NodeTableHead","Data":"thead","Children":[{"Type":"NodeTableRow","Data":"tr","Children":[{"Type":"NodeTableCell","Data":"th","TableCellAlign":1,"Children":[{"Type":"NodeText","Data":"模型 (Model)"}]},{"Type":"NodeTableCell","Data":"th","TableCellAlign":1,"Children":[{"Type":"NodeText","Data":"参数量 (# Params)"}]},{"Type":"NodeTableCell","Data":"th","TableCellAlign":1,"Children":[{"Type":"NodeText","Data":"公开 (Public)"}]},{"Type":"NodeTableCell","Data":"th","TableCellAlign":1,"Children":[{"Type":"NodeText","Data":"元训练 (Meta-trained)"}]}]}]},{"Type":"NodeTableRow","Data":"tr","Children":[{"Type":"NodeTableCell","Data":"td","TableCellAlign":1,"Children":[{"Type":"NodeText","Data":"GPT-2 Large"}]},{"Type":"NodeTableCell","Data":"td","TableCellAlign":1,"Children":[{"Type":"NodeText","Data":"774M"}]},{"Type":"NodeTableCell","Data":"td","TableCellAlign":1,"Children":[{"Type":"NodeText","Data":"✓"}]},{"Type":"NodeTableCell","Data":"td","TableCellAlign":1,"Children":[{"Type":"NodeText","Data":"✗"}]}]},{"Type":"NodeTableRow","Data":"tr","Children":[{"Type":"NodeTableCell","Data":"td","TableCellAlign":1,"Children":[{"Type":"NodeText","Data":"MetaICL"}]},{"Type":"NodeTableCell","Data":"td","TableCellAlign":1,"Children":[{"Type":"NodeText","Data":"774M"}]},{"Type":"NodeTableCell","Data":"td","TableCellAlign":1,"Children":[{"Type":"NodeText","Data":"✓"}]},{"Type":"NodeTableCell","Data":"td","TableCellAlign":1,"Children":[{"Type":"NodeText","Data":"✓"}]}]},{"Type":"NodeTableRow","Data":"tr","Children":[{"Type":"NodeTableCell","Data":"td","TableCellAlign":1,"Children":[{"Type":"NodeText","Data":"GPT-J"}]},{"Type":"NodeTableCell","Data":"td","TableCellAlign":1,"Children":[{"Type":"NodeText","Data":"6B"}]},{"Type":"NodeTableCell","Data":"td","TableCellAlign":1,"Children":[{"Type":"NodeText","Data":"✓"}]},{"Type":"NodeTableCell","Data":"td","TableCellAlign":1,"Children":[{"Type":"NodeText","Data":"✗"}]}]},{"Type":"NodeTableRow","Data":"tr","Children":[{"Type":"NodeTableCell","Data":"td","TableCellAlign":1,"Children":[{"Type":"NodeText","Data":"fairseq 6.7B†"}]},{"Type":"NodeTableCell","Data":"td","TableCellAlign":1,"Children":[{"Type":"NodeText","Data":"6.7B"}]},{"Type":"NodeTableCell","Data":"td","TableCellAlign":1,"Children":[{"Type":"NodeText","Data":"✓"}]},{"Type":"NodeTableCell","Data":"td","TableCellAlign":1,"Children":[{"Type":"NodeText","Data":"✗"}]}]},{"Type":"NodeTableRow","Data":"tr","Children":[{"Type":"NodeTableCell","Data":"td","TableCellAlign":1,"Children":[{"Type":"NodeText","Data":"fairseq 13B†"}]},{"Type":"NodeTableCell","Data":"td","TableCellAlign":1,"Children":[{"Type":"NodeText","Data":"13B"}]},{"Type":"NodeTableCell","Data":"td","TableCellAlign":1,"Children":[{"Type":"NodeText","Data":"✓"}]},{"Type":"NodeTableCell","Data":"td","TableCellAlign":1,"Children":[{"Type":"NodeText","Data":"✗"}]}]},{"Type":"NodeTableRow","Data":"tr","Children":[{"Type":"NodeTableCell","Data":"td","TableCellAlign":1,"Children":[{"Type":"NodeText","Data":"GPT-3"}]},{"Type":"NodeTableCell","Data":"td","TableCellAlign":1,"Children":[{"Type":"NodeText","Data":"175B"}]},{"Type":"NodeTableCell","Data":"td","TableCellAlign":1,"Children":[{"Type":"NodeText","Data":"✗"}]},{"Type":"NodeTableCell","Data":"td","TableCellAlign":1,"Children":[{"Type":"NodeText","Data":"✗"}]}]}]},{"ID":"20250919161156-8b0qtnj","Type":"NodeBlockquote","Properties":{"fold":"1","id":"20250919161156-8b0qtnj","updated":"20250919161157"},"Children":[{"Type":"NodeBlockquoteMarker","Data":"\u003e"},{"ID":"20250919161156-ppgxpvq","Type":"NodeHeading","HeadingLevel":5,"Properties":{"id":"20250919161156-ppgxpvq","updated":"20250919161156"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"表1：实验中使用的语言模型列表"}]},{"ID":"20250919161156-eewlahs","Type":"NodeParagraph","Properties":{"id":"20250919161156-eewlahs","updated":"20250919161156"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"表格说明:"},{"Type":"NodeText","Data":" 实验中使用的LM列表：GPT-2 (Radford et al., 2019), MetaICL (Min et al., 2021b), GPT-J (Wang and Komatsuzaki, 2021), fairseq LMs (Artetxe et al., 2021) 和 GPT-3 (Brown et al., 2020)。‘公开’表示模型权重是否公开；‘元训练’表示模型是否使用情境学习目标进行过训练。†我们使用Artetxe等人(2021)的密集模型，并为方便起见将其称为fairseq LM。‡我们使用Davinci API（基础版，非指令版），并假设其参数量为175B，参考Gao等人(2021)和Artetxe等人(2021)的设定。"}]},{"ID":"20250919161156-qlxwixo","Type":"NodeHeading","HeadingLevel":3,"Properties":{"id":"20250919161156-qlxwixo","updated":"20250919161156"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"内容解析"}]},{"ID":"20250919161156-aovti9l","Type":"NodeList","ListData":{},"Properties":{"id":"20250919161156-aovti9l","updated":"20250919161156"},"Children":[{"ID":"20250919161156-cfw5rx9","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919161156-cfw5rx9","updated":"20250919161156"},"Children":[{"ID":"20250919161156-1zssony","Type":"NodeParagraph","Properties":{"id":"20250919161156-1zssony","updated":"20250919161156"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"多样化的实验对象"},{"Type":"NodeText","Data":": 这张表格展示了作者为了确保其研究结论的"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"普适性"},{"Type":"NodeText","Data":"而选用的模型阵容。这些模型覆盖了："}]},{"ID":"20250919161156-lic10n7","Type":"NodeList","ListData":{},"Properties":{"id":"20250919161156-lic10n7","updated":"20250919161156"},"Children":[{"ID":"20250919161156-y1pyyz5","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919161156-y1pyyz5","updated":"20250919161156"},"Children":[{"ID":"20250919161156-f46x4vg","Type":"NodeParagraph","Properties":{"id":"20250919161156-f46x4vg","updated":"20250919161156"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"不同规模"},{"Type":"NodeText","Data":": 从7.74亿到1750亿参数，跨越了多个数量级。"}]}]},{"ID":"20250919161156-e4i1w3v","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919161156-e4i1w3v","updated":"20250919161156"},"Children":[{"ID":"20250919161156-7voc98t","Type":"NodeParagraph","Properties":{"id":"20250919161156-7voc98t","updated":"20250919161156"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"不同来源"},{"Type":"NodeText","Data":": 来自OpenAI, EleutherAI, Meta AI等不同机构。"}]}]},{"ID":"20250919161156-il50fo9","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919161156-il50fo9","updated":"20250919161156"},"Children":[{"ID":"20250919161156-2xpo8mu","Type":"NodeParagraph","Properties":{"id":"20250919161156-2xpo8mu","updated":"20250919161156"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"不同可用性"},{"Type":"NodeText","Data":": 包含了权重公开和仅能通过API访问的模型。"}]}]}]}]},{"ID":"20250919161156-cwsaf3k","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919161156-cwsaf3k","updated":"20250919161156"},"Children":[{"ID":"20250919161156-e0q8116","Type":"NodeParagraph","Properties":{"id":"20250919161156-e0q8116","updated":"20250919161156"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"关键的实验变量"},{"Type":"NodeText","Data":": “"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"元训练 (Meta-trained)"},{"Type":"NodeText","Data":"”这一列是本研究的一个核心设计。通过对比标准训练的GPT-2 Large和经过元训练的MetaICL（两者参数量相同），作者可以直接研究“元训练”是否会改变模型利用演示信息的方式。这为后文的分析（即元训练会放大模型对格式的依赖）埋下了伏笔。"}]}]},{"ID":"20250919161156-xo2t6mm","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919161156-xo2t6mm","updated":"20250919161156"},"Children":[{"ID":"20250919161156-wkghn24","Type":"NodeParagraph","Properties":{"id":"20250919161156-wkghn24","updated":"20250919161156"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"研究的严谨性"},{"Type":"NodeText","Data":": 通过明确标注模型版本（例如GPT-3是基础版而非指令微调版），作者确保了实验的可复现性和结论的准确性，避免了不同模型能力差异带来的混淆。"}]}]}]}]},{"ID":"20250919161156-eagkkny","Type":"NodeBlockquote","Properties":{"id":"20250919161156-eagkkny","updated":"20250919161156"},"Children":[{"Type":"NodeBlockquoteMarker","Data":"\u003e"},{"ID":"20250919161156-gw4b5us","Type":"NodeHeading","HeadingLevel":3,"Properties":{"id":"20250919161156-gw4b5us","updated":"20250919161156"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"综合性解析"}]},{"ID":"20250919161156-38r91gk","Type":"NodeParagraph","Properties":{"id":"20250919161156-38r91gk","updated":"20250919161156"},"Children":[{"Type":"NodeText","Data":"本节“相关工作”清晰地梳理了情境学习（In-context Learning, ICL）领域的研究现状，并精准地定位了本文的独特贡献。"}]},{"ID":"20250919161156-vkqi8tq","Type":"NodeList","ListData":{"Typ":1},"Properties":{"id":"20250919161156-vkqi8tq","updated":"20250919161156"},"Children":[{"ID":"20250919161156-hnmp37y","Type":"NodeListItem","ListData":{"Typ":1,"Delimiter":46,"Marker":"MS4=","Num":1},"Properties":{"id":"20250919161156-hnmp37y","updated":"20250919161156"},"Children":[{"ID":"20250919161156-4nb8mht","Type":"NodeParagraph","Properties":{"id":"20250919161156-4nb8mht","updated":"20250919161156"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"ICL的背景与地位"},{"Type":"NodeText","Data":": 作者首先点明了ICL的出现是为了解决"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"超大模型微调不现实"},{"Type":"NodeText","Data":"的问题，将其定位为一种"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"替代微调"},{"Type":"NodeText","Data":"的高效学习范式。图2则直观地解释了这种“无梯度更新”的学习方式是如何工作的。"}]}]},{"ID":"20250919161156-5q03cfa","Type":"NodeListItem","ListData":{"Typ":1,"Delimiter":46,"Marker":"Mi4=","Num":2},"Properties":{"id":"20250919161156-5q03cfa","updated":"20250919161156"},"Children":[{"ID":"20250919161156-r4vo40x","Type":"NodeParagraph","Properties":{"id":"20250919161156-r4vo40x","updated":"20250919161156"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"现有研究的两个主要方向"},{"Type":"NodeText","Data":": 作者将先前的研究工作归纳为两大类："}]},{"ID":"20250919161156-mgcumqp","Type":"NodeList","ListData":{},"Properties":{"id":"20250919161156-mgcumqp","updated":"20250919161156"},"Children":[{"ID":"20250919161156-b41m7k2","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919161156-b41m7k2","updated":"20250919161156"},"Children":[{"ID":"20250919161156-qayd74v","Type":"NodeParagraph","Properties":{"id":"20250919161156-qayd74v","updated":"20250919161156"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"“如何做得更好” (How-to works)"},{"Type":"NodeText","Data":": 大部分研究都集中在"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"应用层面"},{"Type":"NodeText","Data":"，探索如何通过改进提示构建、示例选择、或者引入元训练等方法来提升ICL的性能。这些工作默认ICL是有效的，并试图将其效果最大化。"}]}]},{"ID":"20250919161156-mz88xde","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919161156-mz88xde","updated":"20250919161156"},"Children":[{"ID":"20250919161156-1cfr9tm","Type":"NodeParagraph","Properties":{"id":"20250919161156-1cfr9tm","updated":"20250919161156"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"“为何能奏效” (Why-it-works)"},{"Type":"NodeText","Data":": 只有极少数研究开始探索ICL背后的"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"根本机制"},{"Type":"NodeText","Data":"。作者引用了将ICL类比为“贝叶斯推理”的理论分析和发现其与“词频”相关的初步实证工作。"}]}]}]}]},{"ID":"20250919161156-31wfx43","Type":"NodeListItem","ListData":{"Typ":1,"Delimiter":46,"Marker":"My4=","Num":3},"Properties":{"id":"20250919161156-31wfx43","updated":"20250919161156"},"Children":[{"ID":"20250919161156-lx48mty","Type":"NodeParagraph","Properties":{"id":"20250919161156-lx48mty","updated":"20250919161156"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"本文的独特切入点"},{"Type":"NodeText","Data":": 作者明确指出，现有工作中缺乏对“"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"ICL为何比零样本（zero-shot）推理更有效"},{"Type":"NodeText","Data":"”这一根本问题的"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"系统性实证分析"},{"Type":"NodeText","Data":"。本文的贡献恰好填补了这一空白，它不满足于仅仅提升性能，而是要深入剖析ICL性能增益的来源，特别是"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"解构“演示”（demonstrations）本身各个组成部分所扮演的角色"},{"Type":"NodeText","Data":"。"}]}]},{"ID":"20250919161156-i5h3crm","Type":"NodeListItem","ListData":{"Typ":1,"Delimiter":46,"Marker":"NC4=","Num":4},"Properties":{"id":"20250919161156-i5h3crm","updated":"20250919161156"},"Children":[{"ID":"20250919161156-jxzvvmm","Type":"NodeParagraph","Properties":{"id":"20250919161156-jxzvvmm","updated":"20250919161156"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"实验设计的支撑"},{"Type":"NodeText","Data":": 表1展示了本文为支撑其普适性结论而精心挑选的一组多样化的模型，覆盖了不同的规模和训练方法（特别是是否经过“元训练”）。这表明本文的发现不是某个特定模型的偶然现象，而可能揭示了ICL的普遍规律。"}]}]}]},{"ID":"20250919161156-8di8m9x","Type":"NodeParagraph","Properties":{"id":"20250919161156-8di8m9x","updated":"20250919161156"},"Children":[{"Type":"NodeText","Data":"总结来说，这一部分成功地将本文定位为ICL领域的"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"开创性探索工作"},{"Type":"NodeText","Data":"，它从“知其然”的研究现状，迈向了“"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"知其所以然"},{"Type":"NodeText","Data":"”的深度探究，旨在揭示情境学习的内在奥秘。"}]}]},{"ID":"20250919161841-t35jx1c","Type":"NodeHeading","HeadingLevel":3,"Properties":{"id":"20250919161841-t35jx1c","updated":"20250920134433"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"3 实验设置"}]},{"ID":"20250919161841-9dvuvuy","Type":"NodeParagraph","Properties":{"id":"20250919161841-9dvuvuy","updated":"20250919161842"},"Children":[{"Type":"NodeText","Data":"我们描述了在我们的分析中（第4节和第5节）所使用的实验设置。"}]},{"ID":"20250919161841-110idhl","Type":"NodeHeading","HeadingLevel":5,"Properties":{"id":"20250919161841-110idhl","updated":"20250920134433"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"模型 (Models)"}]},{"ID":"20250919161841-fnzz32q","Type":"NodeParagraph","Properties":{"id":"20250919161841-fnzz32q","updated":"20250919161842"},"Children":[{"Type":"NodeText","Data":"我们总共对"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"12个模型"},{"Type":"NodeText","Data":"进行了实验。我们纳入了6个语言模型（表1），所有这些模型都是"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"仅解码器（decoder-only）"},{"Type":"NodeText","Data":" 的密集型语言模型。我们遵循Min等人(2021a)的方法，对每个语言模型使用两种推理方法："},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"直接（direct）"},{"Type":"NodeText","Data":" 和"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"通道（channel）"},{"Type":"NodeText","Data":"。语言模型的规模从7.74亿（774M）到1750亿（175B）参数不等。在进行实验时，我们纳入了当时最大的密集型语言模型（GPT-3）和最大的公开可用的密集型语言模型（fairseq 13B）。我们还纳入了MetaICL，该模型由GPT-2 Large初始化，然后在一系列有监督数据集上使用情境学习目标进行了"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"元训练（meta-trained）"},{"Type":"NodeText","Data":"，并确保我们的评估数据集与元训练时使用的数据集没有重叠。"}]},{"ID":"20250919161841-kvmdl29","Type":"NodeHeading","HeadingLevel":5,"Properties":{"id":"20250919161841-kvmdl29","updated":"20250920134433"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"评估数据 (Evaluation Data)"}]},{"ID":"20250919161841-j3j1oi9","Type":"NodeParagraph","Properties":{"id":"20250919161841-j3j1oi9","updated":"20250919161842"},"Children":[{"Type":"NodeText","Data":"我们在"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"26个数据集"},{"Type":"NodeText","Data":"上进行评估，涵盖了情感分析、释义检测、自然语言推断、仇恨言论检测、问答和句子补全等任务（完整的列表和参考文献见附录A）。我们使用这些数据集的原因是：(1) 它们是训练样本少于1万个的"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"低资源数据集"},{"Type":"NodeText","Data":"；(2) 它们包含了来自GLUE (Wang et al., 2018) 和 SuperGLUE (Wang et al., 2019a) 的经过充分研究的"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"基准测试"},{"Type":"NodeText","Data":"；以及 (3) 它们覆盖了包括科学、社交媒体、金融和更多领域的"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"多样化领域"},{"Type":"NodeText","Data":"。这26个数据集可以进一步分为16个分类任务和10个多项选择任务。"}]},{"ID":"20250919161841-ls5rgb9","Type":"NodeHeading","HeadingLevel":5,"Properties":{"id":"20250919161841-ls5rgb9","updated":"20250920134433"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"其他细节 (Other Details)"}]},{"ID":"20250919161841-tv4p5ny","Type":"NodeParagraph","Properties":{"id":"20250919161841-tv4p5ny","updated":"20250920134433"},"Children":[{"Type":"NodeText","Data":"在本文的所有实验中，我们默认使用 "},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"k = 16"},{"Type":"NodeText","Data":" 个样本作为演示，除非另有说明。样本是从训练数据中"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"均匀采样"},{"Type":"NodeText","Data":"的。我们使用5个不同的随机种子选择一组k个训练样本，并进行5次实验。由于资源有限，对于fairseq 13B和GPT-3，我们使用6个数据集的子集和3个随机种子进行实验。我们报告分类任务的"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"Macro-F1"},{"Type":"NodeText","Data":"分数和多项选择任务的"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"准确率（Accuracy）"},{"Type":"NodeText","Data":"。我们计算每个数据集在不同种子上的平均值，然后报告跨数据集的宏平均值。我们使用"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"最简洁的模板"},{"Type":"NodeText","Data":"来从一个样本构建输入序列。更多细节请参考附录B。"}]},{"ID":"20250919161920-m7h2hwo","Type":"NodeParagraph","Properties":{"id":"20250919161920-m7h2hwo","updated":"20250919161920"},"Children":[{"Type":"NodeText","Data":"​"},{"Type":"NodeImage","Data":"span","Children":[{"Type":"NodeBang"},{"Type":"NodeOpenBracket"},{"Type":"NodeLinkText","Data":"image"},{"Type":"NodeCloseBracket"},{"Type":"NodeOpenParen"},{"Type":"NodeLinkDest","Data":"assets/image-20250919161920-fuykmh6.png"},{"Type":"NodeCloseParen"}]},{"Type":"NodeText","Data":"​"}]},{"ID":"20250919161841-pyyxhaf","Type":"NodeBlockquote","Properties":{"id":"20250919161841-pyyxhaf","updated":"20250919161842"},"Children":[{"Type":"NodeBlockquoteMarker","Data":"\u003e"},{"ID":"20250919161841-7b770xx","Type":"NodeHeading","HeadingLevel":5,"Properties":{"id":"20250919161841-7b770xx","updated":"20250919161841"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"图3：在分类（顶部）和多项选择（底部）任务中，使用无演示、黄金标签演示和随机标签演示的结果"}]},{"ID":"20250919161841-ggz7hoz","Type":"NodeParagraph","Properties":{"id":"20250919161841-ggz7hoz","updated":"20250919161841"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"图片内容翻译:"}]},{"ID":"20250919161841-qihqfgf","Type":"NodeList","ListData":{},"Properties":{"id":"20250919161841-qihqfgf","updated":"20250919161841"},"Children":[{"ID":"20250919161841-olgmbgl","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919161841-olgmbgl","updated":"20250919161841"},"Children":[{"ID":"20250919161841-lmcx8ys","Type":"NodeParagraph","Properties":{"id":"20250919161841-lmcx8ys","updated":"20250919161841"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"图例:"}]},{"ID":"20250919161841-am9y5tp","Type":"NodeList","ListData":{},"Properties":{"id":"20250919161841-am9y5tp","updated":"20250919161841"},"Children":[{"ID":"20250919161841-vuq9o9t","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919161841-vuq9o9t","updated":"20250919161841"},"Children":[{"ID":"20250919161841-pyifs9y","Type":"NodeParagraph","Properties":{"id":"20250919161841-pyifs9y","updated":"20250919161841"},"Children":[{"Type":"NodeText","Data":"蓝色 (No Demos): 无演示"}]}]},{"ID":"20250919161841-ttl2d7i","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919161841-ttl2d7i","updated":"20250919161841"},"Children":[{"ID":"20250919161841-8xqcuoa","Type":"NodeParagraph","Properties":{"id":"20250919161841-8xqcuoa","updated":"20250919161841"},"Children":[{"Type":"NodeText","Data":"橙色 (Demos w/ gold labels): 使用黄金标签的演示（即正确的标签）"}]}]},{"ID":"20250919161841-ighjcsc","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919161841-ighjcsc","updated":"20250919161841"},"Children":[{"ID":"20250919161841-bwrushq","Type":"NodeParagraph","Properties":{"id":"20250919161841-bwrushq","updated":"20250919161841"},"Children":[{"Type":"NodeText","Data":"红色 (Demos w/ random labels): 使用随机标签的演示"}]}]}]}]},{"ID":"20250919161841-hytmaux","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919161841-hytmaux","updated":"20250919161841"},"Children":[{"ID":"20250919161841-a6ha7bv","Type":"NodeParagraph","Properties":{"id":"20250919161841-a6ha7bv","updated":"20250919161841"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"顶部图表: 分类 (Classification)"}]},{"ID":"20250919161841-5ycb53s","Type":"NodeList","ListData":{},"Properties":{"id":"20250919161841-5ycb53s","updated":"20250919161841"},"Children":[{"ID":"20250919161841-0n8scs5","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919161841-0n8scs5","updated":"20250919161841"},"Children":[{"ID":"20250919161841-5pvxuq6","Type":"NodeParagraph","Properties":{"id":"20250919161841-5pvxuq6","updated":"20250919161841"},"Children":[{"Type":"NodeText","Data":"纵坐标: Macro-F1 (%)，越高越好。"}]}]}]}]},{"ID":"20250919161841-4nri7jm","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919161841-4nri7jm","updated":"20250919161841"},"Children":[{"ID":"20250919161841-75c8m33","Type":"NodeParagraph","Properties":{"id":"20250919161841-75c8m33","updated":"20250919161841"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"底部图表: 多项选择 (Multi-choice)"}]},{"ID":"20250919161841-emuhgqn","Type":"NodeList","ListData":{},"Properties":{"id":"20250919161841-emuhgqn","updated":"20250919161841"},"Children":[{"ID":"20250919161841-wkamppu","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919161841-wkamppu","updated":"20250919161841"},"Children":[{"ID":"20250919161841-2ns624x","Type":"NodeParagraph","Properties":{"id":"20250919161841-2ns624x","updated":"20250919161841"},"Children":[{"Type":"NodeText","Data":"纵坐标: 准确率 (%) (Accuracy (%))，越高越好。"}]}]}]}]},{"ID":"20250919161841-ppu0rbz","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919161841-ppu0rbz","updated":"20250919161841"},"Children":[{"ID":"20250919161841-5r22hco","Type":"NodeParagraph","Properties":{"id":"20250919161841-5r22hco","updated":"20250919161841"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"横坐标:"}]},{"ID":"20250919161841-1lufuty","Type":"NodeList","ListData":{},"Properties":{"id":"20250919161841-1lufuty","updated":"20250919161841"},"Children":[{"ID":"20250919161841-uxsuk0s","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919161841-uxsuk0s","updated":"20250919161841"},"Children":[{"ID":"20250919161841-pv4p2vm","Type":"NodeParagraph","Properties":{"id":"20250919161841-pv4p2vm","updated":"20250919161841"},"Children":[{"Type":"NodeText","Data":"展示了从GPT-2到GPT-3等6种不同模型，每种模型都使用了“直接 (Direct)”和“通道 (Channel)”两种推理方法。"}]}]}]}]},{"ID":"20250919161841-v8v3fgb","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919161841-v8v3fgb","updated":"20250919161841"},"Children":[{"ID":"20250919161841-o4uwxjv","Type":"NodeParagraph","Properties":{"id":"20250919161841-o4uwxjv","updated":"20250919161841"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"图解说明:"},{"Type":"NodeText","Data":" 注意，前八个模型在16个分类和10个多项选择数据集上进行评估，而最后四个模型在3个分类和3个多项选择数据集上进行评估。图11提供了所有模型可比较的数字。"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"使用随机标签的模型性能与使用黄金标签的性能非常接近"},{"Type":"NodeText","Data":"（更多讨论见4.1节）。"}]}]}]},{"ID":"20250919161841-4zixx4d","Type":"NodeHeading","HeadingLevel":3,"Properties":{"id":"20250919161841-4zixx4d","updated":"20250919161841"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"内容解析"}]},{"ID":"20250919161841-xtnycma","Type":"NodeParagraph","Properties":{"id":"20250919161841-xtnycma","updated":"20250919161841"},"Children":[{"Type":"NodeText","Data":"这张图是本文核心发现的"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"全面展示"},{"Type":"NodeText","Data":"，比引言中的图1更加详尽。"}]},{"ID":"20250919161841-qhgjr53","Type":"NodeList","ListData":{},"Properties":{"id":"20250919161841-qhgjr53","updated":"20250919161841"},"Children":[{"ID":"20250919161841-hvnb0k8","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919161841-hvnb0k8","updated":"20250919161841"},"Children":[{"ID":"20250919161841-9sq1qfn","Type":"NodeParagraph","Properties":{"id":"20250919161841-9sq1qfn","updated":"20250919161841"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"核心发现的再确认"},{"Type":"NodeText","Data":": 图表最引人注目的信息是，在"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"所有12种实验组合"},{"Type":"NodeText","Data":"（6个模型 × 2种方法）和两种任务类型中，代表"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"随机标签的红色条"},{"Type":"NodeText","Data":"和代表"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"正确标签的橙色条"},{"Type":"NodeText","Data":"在高度上都惊人地一致。这无可辩驳地证明了论文的核心论点：标签的正确性对模型性能的影响微乎其微。"}]}]},{"ID":"20250919161841-u1mqdpz","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919161841-u1mqdpz","updated":"20250919161841"},"Children":[{"ID":"20250919161841-16p6af2","Type":"NodeParagraph","Properties":{"id":"20250919161841-16p6af2","updated":"20250919161841"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"演示的普遍有效性"},{"Type":"NodeText","Data":": 同样，在所有设置中，红色和橙色条都显著高于代表"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"无演示的蓝色条"},{"Type":"NodeText","Data":"。这强调了提供演示本身的重要性，无论其标签是否正确，它都能为模型提供必要的格式和上下文信息。"}]}]},{"ID":"20250919161841-muswwp8","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919161841-muswwp8","updated":"20250919161841"},"Children":[{"ID":"20250919161841-rop89ax","Type":"NodeParagraph","Properties":{"id":"20250919161841-rop89ax","updated":"20250919161841"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"通道方法（Channel Method）的优势"},{"Type":"NodeText","Data":": 一个清晰的趋势是，对于同一个模型，使用“通道”方法的性能（例如 Channel GPT-2）几乎总是"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"优于"},{"Type":"NodeText","Data":"“直接”方法（例如 Direct GPT-2）。这表明推理策略的选择对最终性能有显著影响，通道方法能更有效地利用演示提供的上下文。"}]}]},{"ID":"20250919161841-79hmenx","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919161841-79hmenx","updated":"20250919161841"},"Children":[{"ID":"20250919161841-8k8vaom","Type":"NodeParagraph","Properties":{"id":"20250919161841-8k8vaom","updated":"20250919161841"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"模型规模与元训练的效果"},{"Type":"NodeText","Data":":"}]},{"ID":"20250919161841-88ydxhf","Type":"NodeList","ListData":{},"Properties":{"id":"20250919161841-88ydxhf","updated":"20250919161841"},"Children":[{"ID":"20250919161841-w3qmvxk","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919161841-w3qmvxk","updated":"20250919161841"},"Children":[{"ID":"20250919161841-p494fto","Type":"NodeParagraph","Properties":{"id":"20250919161841-p494fto","updated":"20250919161841"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"规模效应"},{"Type":"NodeText","Data":": 总体趋势上，性能从左到右随着模型规模的增大而提升（从GPT-2到GPT-3）。"}]}]},{"ID":"20250919161841-xnajo4z","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919161841-xnajo4z","updated":"20250919161841"},"Children":[{"ID":"20250919161841-knvmn98","Type":"NodeParagraph","Properties":{"id":"20250919161841-knvmn98","updated":"20250919161841"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"元训练的威力"},{"Type":"NodeText","Data":": 对比GPT-2和MetaICL（两者参数量相同），MetaICL的性能有显著飞跃，尤其是在通道方法下。这有力地证明了，经过“元训练”的模型能更熟练地利用演示中的格式信息，从而在情境学习任务上表现得更好。"}]}]}]}]}]}]},{"ID":"20250919161841-aj8bq4k","Type":"NodeBlockquote","Properties":{"fold":"1","id":"20250919161841-aj8bq4k","updated":"20250919161842"},"Children":[{"Type":"NodeBlockquoteMarker","Data":"\u003e"},{"ID":"20250919161841-wgaidpx","Type":"NodeHeading","HeadingLevel":3,"Properties":{"id":"20250919161841-wgaidpx","updated":"20250919161841"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"内容解析"}]},{"ID":"20250919161841-35hkwur","Type":"NodeHeading","HeadingLevel":5,"Properties":{"id":"20250919161841-35hkwur","updated":"20250919161841"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"严谨而全面的实验设计"}]},{"ID":"20250919161841-7s30mwo","Type":"NodeParagraph","Properties":{"id":"20250919161841-7s30mwo","updated":"20250919161841"},"Children":[{"Type":"NodeText","Data":"这一部分详细描述了实验的设置，旨在证明其研究结论的"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"可靠性"},{"Type":"NodeText","Data":"和"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"普适性"},{"Type":"NodeText","Data":"。"}]},{"ID":"20250919161841-x9ei9se","Type":"NodeList","ListData":{},"Properties":{"id":"20250919161841-x9ei9se","updated":"20250919161841"},"Children":[{"ID":"20250919161841-27usm3e","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919161841-27usm3e","updated":"20250919161841"},"Children":[{"ID":"20250919161841-hlcmmz0","Type":"NodeParagraph","Properties":{"id":"20250919161841-hlcmmz0","updated":"20250919161841"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"模型多样性"},{"Type":"NodeText","Data":": 作者精心挑选了6个不同规模、来自不同机构的语言模型，并对每个模型应用两种不同的推理方法，总共构成了12个实验条件。这种设计确保了研究发现不是特定于某个模型或某种方法的偶然现象。特别是包含了"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"经过元训练的MetaICL"},{"Type":"NodeText","Data":"作为对照，这使得研究能够深入分析训练方式对情境学习机制的影响。"}]}]},{"ID":"20250919161841-ogjvcon","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919161841-ogjvcon","updated":"20250919161841"},"Children":[{"ID":"20250919161841-lqg48fv","Type":"NodeParagraph","Properties":{"id":"20250919161841-lqg48fv","updated":"20250919161841"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"数据广泛性"},{"Type":"NodeText","Data":": 实验评估了"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"26个"},{"Type":"NodeText","Data":"涵盖多种任务类型（分类、问答等）和领域（金融、社交媒体等）的数据集。这保证了结论的"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"通用性"},{"Type":"NodeText","Data":"，而不是仅在某个特定任务上成立。选择低资源数据集也更符合情境学习的实际应用场景。"}]}]},{"ID":"20250919161841-vhet3al","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919161841-vhet3al","updated":"20250919161841"},"Children":[{"ID":"20250919161841-1ovhjza","Type":"NodeParagraph","Properties":{"id":"20250919161841-1ovhjza","updated":"20250919161841"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"执行的规范性"},{"Type":"NodeText","Data":": 作者明确了实验的关键参数（如 "},{"Type":"NodeTextMark","TextMarkType":"code","TextMarkTextContent":"k=16"},{"Type":"NodeText","Data":"​ 个演示），并通过"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"多次随机抽样和重复实验"},{"Type":"NodeText","Data":"来消除偶然性，确保结果的稳定性。他们清晰地说明了评估指标（Macro-F1和Accuracy）和结果的统计方式（宏平均），展现了科学研究的严谨性。"}]}]}]}]},{"ID":"20250919161841-qviliic","Type":"NodeBlockquote","Properties":{"fold":"1","id":"20250919161841-qviliic","updated":"20250919161841"},"Children":[{"Type":"NodeBlockquoteMarker","Data":"\u003e"},{"ID":"20250919161841-ys49iel","Type":"NodeHeading","HeadingLevel":3,"Properties":{"id":"20250919161841-ys49iel","updated":"20250919161841"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"综合性解析"}]},{"ID":"20250919161841-zxrxl03","Type":"NodeParagraph","Properties":{"id":"20250919161841-zxrxl03","updated":"20250919161841"},"Children":[{"Type":"NodeText","Data":"“实验设置”这一章节是整篇论文科学性的基石。作者通过周密的设计，构建了一个强大且全面的实验框架，旨在无可辩驳地验证其核心假设。"}]},{"ID":"20250919161841-cxszcw1","Type":"NodeList","ListData":{"Typ":1},"Properties":{"id":"20250919161841-cxszcw1","updated":"20250919161841"},"Children":[{"ID":"20250919161841-tpyzd9s","Type":"NodeListItem","ListData":{"Typ":1,"Delimiter":46,"Marker":"MS4=","Num":1},"Properties":{"id":"20250919161841-tpyzd9s","updated":"20250919161841"},"Children":[{"ID":"20250919161841-4uc65kk","Type":"NodeParagraph","Properties":{"id":"20250919161841-4uc65kk","updated":"20250919161841"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"多维度验证"},{"Type":"NodeText","Data":": 通过组合"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"不同模型规模"},{"Type":"NodeText","Data":"、"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"不同训练方法（标准 vs. 元训练）"},{"Type":"NodeText","Data":"、"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"不同推理策略（直接 vs. 通道）"},{"Type":"NodeText","Data":" 和"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"多样的评估任务"},{"Type":"NodeText","Data":"，作者从多个维度交叉验证了他们的发现。这使得“随机标签与黄金标签性能相近”这一反直觉的结论极具说服力。"}]}]},{"ID":"20250919161841-tavcf8i","Type":"NodeListItem","ListData":{"Typ":1,"Delimiter":46,"Marker":"Mi4=","Num":2},"Properties":{"id":"20250919161841-tavcf8i","updated":"20250919161841"},"Children":[{"ID":"20250919161841-6hayiae","Type":"NodeParagraph","Properties":{"id":"20250919161841-6hayiae","updated":"20250919161841"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"可控的变量分析"},{"Type":"NodeText","Data":": 实验设计中包含了精妙的对照组。例如，通过比较参数量相同的GPT-2和MetaICL，可以直接剥离出"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"元训练"},{"Type":"NodeText","Data":"带来的影响。通过对比同一模型的“直接”和“通道”方法，可以分析"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"推理策略"},{"Type":"NodeText","Data":"的作用。"}]}]},{"ID":"20250919161841-n93ea7h","Type":"NodeListItem","ListData":{"Typ":1,"Delimiter":46,"Marker":"My4=","Num":3},"Properties":{"id":"20250919161841-n93ea7h","updated":"20250919161841"},"Children":[{"ID":"20250919161841-6ep6bzu","Type":"NodeParagraph","Properties":{"id":"20250919161841-6ep6bzu","updated":"20250919161841"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"结果的提前展示"},{"Type":"NodeText","Data":": 尽管详细的分析在后续章节，但这里的图3已经将最核心、最震撼的实验结果完整地呈现给读者。这张图不仅是对引言中图1的扩展，更是一份全面的证据清单，清晰地展示了在所有测试条件下，模型性能对标签正确性的"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"不敏感性"},{"Type":"NodeText","Data":"，以及对演示格式的"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"依赖性"},{"Type":"NodeText","Data":"。"}]}]}]},{"ID":"20250919161841-z3lcqdq","Type":"NodeParagraph","Properties":{"id":"20250919161841-z3lcqdq","updated":"20250919161841"},"Children":[{"Type":"NodeText","Data":"总结而言，这一部分不仅仅是实验流程的简单陈述，更是对研究方法论严谨性的有力证明。它向读者保证，后续章节中提出的深刻见解，都是建立在坚实、广泛且可信的实验数据之上的。"}]}]},{"ID":"20250919161934-t8t2niu","Type":"NodeHeading","HeadingLevel":3,"Properties":{"id":"20250919161934-t8t2niu","updated":"20250919161934"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"4 真实标签无关紧要"}]},{"ID":"20250919161934-gcv5qrz","Type":"NodeHeading","HeadingLevel":5,"Properties":{"id":"20250919161934-gcv5qrz","updated":"20250919161934"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"4.1 黄金标签 vs. 随机标签"}]},{"ID":"20250919161934-jmas93n","Type":"NodeParagraph","Properties":{"id":"20250919161934-jmas93n","updated":"20250919161934"},"Children":[{"Type":"NodeText","Data":"为了探究演示中正确配对的输入和标签——我们称之为"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"真实输入-标签映射"},{"Type":"NodeText","Data":"——的影响，我们比较了以下三种方法："}]},{"ID":"20250919161934-tjz10qh","Type":"NodeParagraph","Properties":{"id":"20250919161934-tjz10qh","updated":"20250919161934"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"无演示 (No demonstrations)"},{"Type":"NodeText","Data":" 是一种典型的"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"零样本（zero-shot）"},{"Type":"NodeText","Data":" 方法，不使用任何带标签的数据。预测是通过 "},{"Type":"NodeTextMark","TextMarkType":"code","TextMarkTextContent":"argmax_{y∈C} P(y|x)"},{"Type":"NodeText","Data":"​ 来进行的，其中 "},{"Type":"NodeTextMark","TextMarkType":"code","TextMarkTextContent":"x"},{"Type":"NodeText","Data":"​ 是测试输入，"},{"Type":"NodeTextMark","TextMarkType":"code","TextMarkTextContent":"C"},{"Type":"NodeText","Data":"​ 是一个小的离散可能标签集。"}]},{"ID":"20250919161934-zgsnzzk","Type":"NodeParagraph","Properties":{"id":"20250919161934-zgsnzzk","updated":"20250919161934"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"使用黄金标签的演示 (Demonstrations w/ gold labels)"},{"Type":"NodeText","Data":" 被用于典型的情境学习方法中，其中包含 "},{"Type":"NodeTextMark","TextMarkType":"code","TextMarkTextContent":"k"},{"Type":"NodeText","Data":"​ 个带标签的样本 "},{"Type":"NodeTextMark","TextMarkType":"code","TextMarkTextContent":"(x₁, y₁)...(xₖ, yₖ)"},{"Type":"NodeText","Data":"​。这 "},{"Type":"NodeTextMark","TextMarkType":"code","TextMarkTextContent":"k"},{"Type":"NodeText","Data":"​ 个输入-标签对的拼接被用来进行预测："},{"Type":"NodeTextMark","TextMarkType":"code","TextMarkTextContent":"argmax_{y∈C} P(y|x₁, y₁...xₖ, yₖ, x)"},{"Type":"NodeText","Data":"​。"}]},{"ID":"20250919161934-agbp6vf","Type":"NodeParagraph","Properties":{"id":"20250919161934-agbp6vf","updated":"20250919161934"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"使用随机标签的演示 (Demonstrations w/ random labels)"},{"Type":"NodeText","Data":" 是用随机标签构建的，而不是使用来自带标签数据的黄金标签。每个 "},{"Type":"NodeTextMark","TextMarkType":"code","TextMarkTextContent":"xᵢ"},{"Type":"NodeText","Data":"​ (1 ≤ i ≤ k) 都与一个从 "},{"Type":"NodeTextMark","TextMarkType":"code","TextMarkTextContent":"C"},{"Type":"NodeText","Data":"​ 中均匀随机抽样的 "},{"Type":"NodeTextMark","TextMarkType":"code","TextMarkTextContent":"ỹᵢ"},{"Type":"NodeText","Data":"​ 配对。然后，"},{"Type":"NodeTextMark","TextMarkType":"code","TextMarkTextContent":"(x₁, ỹ₁)...(xₖ, ỹₖ)"},{"Type":"NodeText","Data":"​ 的拼接被用来进行预测："},{"Type":"NodeTextMark","TextMarkType":"code","TextMarkTextContent":"argmax_{y∈C} P(y|x₁, ỹ₁...xₖ, ỹₖ, x)"},{"Type":"NodeText","Data":"​。"}]},{"ID":"20250919161934-2tqqlxi","Type":"NodeParagraph","Properties":{"id":"20250919161934-2tqqlxi","updated":"20250919161934"},"Children":[{"Type":"NodeText","Data":"结果报告在图3中。首先，使用带有黄金标签的演示，性能相比无演示有显著提升，这在许多先前的工作中都得到了证实 (Brown et al., 2020; Zhao et al., 2021; Liu et al., 2021)。然后我们发现，"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"用随机标签替换黄金标签只会对性能造成轻微的损害"},{"Type":"NodeText","Data":"。这一趋势在几乎所有模型上都是一致的；模型在替换标签后性能下降的幅度在0-5%绝对值之间。在多项选择任务中替换标签的影响（平均1.7%绝对值）比在分类任务中（2.6%绝对值）要小。"}]},{"ID":"20250919161934-gpvkvds","Type":"NodeParagraph","Properties":{"id":"20250919161934-gpvkvds","updated":"20250919161934"},"Children":[{"Type":"NodeText","Data":"这个结果表明，"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"真实的输入-标签对对于实现性能提升并非必需"},{"Type":"NodeText","Data":"。考虑到在典型的监督学习中，正确配对的训练数据至关重要——因为它告知模型执行下游任务所需的预期输入-标签对应关系——这一发现是"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"反直觉的"},{"Type":"NodeText","Data":"。尽管如此，这些模型在下游任务上确实取得了不俗的性能。这有力地表明，模型有能力恢复任务所需的预期输入-标签对应关系；然而，这种恢复"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"并非直接来自演示中的配对"},{"Type":"NodeText","Data":"。"}]},{"ID":"20250919161934-18u8mjv","Type":"NodeParagraph","Properties":{"id":"20250919161934-18u8mjv","updated":"20250919161934"},"Children":[{"Type":"NodeText","Data":"值得注意的是，在MetaICL中性能下降尤其小：仅为0.1-0.9%绝对值。这表明，使用显式的情境学习目标进行"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"元训练"},{"Type":"NodeText","Data":"，实际上鼓励了模型"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"基本忽略输入-标签映射"},{"Type":"NodeText","Data":"，而去利用演示的其他组成部分（更多讨论见5.4节）。"}]},{"ID":"20250919161934-op23cu1","Type":"NodeHeading","HeadingLevel":5,"Properties":{"id":"20250919161934-op23cu1","updated":"20250919161934"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"4.2 消融实验 (Ablations)"}]},{"ID":"20250919161934-7ngfzr8","Type":"NodeParagraph","Properties":{"id":"20250919161934-7ngfzr8","updated":"20250919161934"},"Children":[{"Type":"NodeText","Data":"为了进行额外的消融实验，我们在5个分类和4个多项选择数据集上进行了实验。"}]},{"ID":"20250919161934-m9vd6sc","Type":"NodeParagraph","Properties":{"id":"20250919161934-m9vd6sc","updated":"20250919161934"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"正确标签的数量重要吗？ (Does the number of correct labels matter?)"},{"Type":"NodeText","Data":" 为了进一步检验演示中标签正确性的影响，我们通过改变演示中正确标签的数量进行了一项消融研究。我们评估了“包含a%正确标签的演示”，其中包含 "},{"Type":"NodeTextMark","TextMarkType":"code","TextMarkTextContent":"k × a/100"},{"Type":"NodeText","Data":"​ 个正确配对和 "},{"Type":"NodeTextMark","TextMarkType":"code","TextMarkTextContent":"k × (1 - a/100)"},{"Type":"NodeText","Data":"​ 个错误配对（算法见附录B中的算法1）。这里，"},{"Type":"NodeTextMark","TextMarkType":"code","TextMarkTextContent":"a = 100"},{"Type":"NodeText","Data":"​ 与典型的有黄金标签的情境学习演示相同。"}]},{"ID":"20250919162027-x2t87we","Type":"NodeParagraph","Properties":{"id":"20250919162027-x2t87we","updated":"20250919162027"},"Children":[{"Type":"NodeText","Data":"​"},{"Type":"NodeImage","Data":"span","Children":[{"Type":"NodeBang"},{"Type":"NodeOpenBracket"},{"Type":"NodeLinkText","Data":"image"},{"Type":"NodeCloseBracket"},{"Type":"NodeOpenParen"},{"Type":"NodeLinkDest","Data":"assets/image-20250919162027-wt2zzh7.png"},{"Type":"NodeCloseParen"}]},{"Type":"NodeText","Data":"​"}]},{"ID":"20250919161934-0rmhqwe","Type":"NodeBlockquote","Properties":{"fold":"1","id":"20250919161934-0rmhqwe","updated":"20250919161934"},"Children":[{"Type":"NodeBlockquoteMarker","Data":"\u003e"},{"ID":"20250919161934-eznrct6","Type":"NodeHeading","HeadingLevel":5,"Properties":{"id":"20250919161934-eznrct6","updated":"20250919161934"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"图4：演示中包含不同数量正确标签的结果"}]},{"ID":"20250919161934-2imflkj","Type":"NodeParagraph","Properties":{"id":"20250919161934-2imflkj","updated":"20250919161934"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"图片内容翻译:"}]},{"ID":"20250919161934-lsdi3do","Type":"NodeList","ListData":{},"Properties":{"id":"20250919161934-lsdi3do","updated":"20250919161934"},"Children":[{"ID":"20250919161934-rempevw","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919161934-rempevw","updated":"20250919161934"},"Children":[{"ID":"20250919161934-y3pujw3","Type":"NodeParagraph","Properties":{"id":"20250919161934-y3pujw3","updated":"20250919161934"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"图例:"},{"Type":"NodeText","Data":" 从左到右的颜色梯度代表正确标签的比例。"}]},{"ID":"20250919161934-qgblp97","Type":"NodeList","ListData":{},"Properties":{"id":"20250919161934-qgblp97","updated":"20250919161934"},"Children":[{"ID":"20250919161934-33cu8m5","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919161934-33cu8m5","updated":"20250919161934"},"Children":[{"ID":"20250919161934-vxjrct3","Type":"NodeParagraph","Properties":{"id":"20250919161934-vxjrct3","updated":"20250919161934"},"Children":[{"Type":"NodeText","Data":"深红色: 100% 正确 (100% correct)"}]}]},{"ID":"20250919161934-5rdtxo0","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919161934-5rdtxo0","updated":"20250919161934"},"Children":[{"ID":"20250919161934-j56u8f7","Type":"NodeParagraph","Properties":{"id":"20250919161934-j56u8f7","updated":"20250919161934"},"Children":[{"Type":"NodeText","Data":"橙红色: 75% 正确 (75% correct)"}]}]},{"ID":"20250919161934-vddpuh2","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919161934-vddpuh2","updated":"20250919161934"},"Children":[{"ID":"20250919161934-wdt4eij","Type":"NodeParagraph","Properties":{"id":"20250919161934-wdt4eij","updated":"20250919161934"},"Children":[{"Type":"NodeText","Data":"橙色: 50% 正确 (50% correct)"}]}]},{"ID":"20250919161934-xti63t8","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919161934-xti63t8","updated":"20250919161934"},"Children":[{"ID":"20250919161934-ecbjehs","Type":"NodeParagraph","Properties":{"id":"20250919161934-ecbjehs","updated":"20250919161934"},"Children":[{"Type":"NodeText","Data":"浅橙色: 25% 正确 (25% correct)"}]}]},{"ID":"20250919161934-8l7ltah","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919161934-8l7ltah","updated":"20250919161934"},"Children":[{"ID":"20250919161934-y0ua7ue","Type":"NodeParagraph","Properties":{"id":"20250919161934-y0ua7ue","updated":"20250919161934"},"Children":[{"Type":"NodeText","Data":"黄色: 0% 正确 (0% correct) (即全部随机)"}]}]},{"ID":"20250919161934-yfnvmvh","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919161934-yfnvmvh","updated":"20250919161934"},"Children":[{"ID":"20250919161934-hsbitz1","Type":"NodeParagraph","Properties":{"id":"20250919161934-hsbitz1","updated":"20250919161934"},"Children":[{"Type":"NodeText","Data":"蓝色: 无演示 (No Demos)"}]}]}]}]},{"ID":"20250919161934-tde1baf","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919161934-tde1baf","updated":"20250919161934"},"Children":[{"ID":"20250919161934-i5v1b8h","Type":"NodeParagraph","Properties":{"id":"20250919161934-i5v1b8h","updated":"20250919161934"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"横坐标:"},{"Type":"NodeText","Data":" 展示了MetaICL和GPT-J在分类和多项选择任务上的表现。"}]}]},{"ID":"20250919161934-olz1n59","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919161934-olz1n59","updated":"20250919161934"},"Children":[{"ID":"20250919161934-zq1bcvv","Type":"NodeParagraph","Properties":{"id":"20250919161934-zq1bcvv","updated":"20250919161934"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"纵坐标:"},{"Type":"NodeText","Data":" 准确率 (%)"}]}]},{"ID":"20250919161934-penzk1q","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919161934-penzk1q","updated":"20250919161934"},"Children":[{"ID":"20250919161934-ftr7z7d","Type":"NodeParagraph","Properties":{"id":"20250919161934-ftr7z7d","updated":"20250919161934"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"图解说明:"},{"Type":"NodeText","Data":" 演示中包含不同数量正确标签的结果。分类和多项选择任务分别使用通道（Channel）和直接（Direct）方法。无演示（蓝色）的性能作为参考报告。"}]}]}]},{"ID":"20250919161934-mzfrl7r","Type":"NodeHeading","HeadingLevel":3,"Properties":{"id":"20250919161934-mzfrl7r","updated":"20250919161934"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"内容解析"}]},{"ID":"20250919161934-315rdpq","Type":"NodeList","ListData":{},"Properties":{"id":"20250919161934-315rdpq","updated":"20250919161934"},"Children":[{"ID":"20250919161934-d52w2uz","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919161934-d52w2uz","updated":"20250919161934"},"Children":[{"ID":"20250919161934-1ano5p7","Type":"NodeParagraph","Properties":{"id":"20250919161934-1ano5p7","updated":"20250919161934"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"平滑但微弱的下降"},{"Type":"NodeText","Data":": 这张图的核心信息是，随着正确标签比例从100%下降到0%，性能的柱状图"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"只有非常平缓的下降"},{"Type":"NodeText","Data":"。即使在所有标签都错误的情况下（黄色条），性能也远高于没有演示（蓝色条）。"}]}]},{"ID":"20250919161934-n0srfhl","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919161934-n0srfhl","updated":"20250919161934"},"Children":[{"ID":"20250919161934-36c487k","Type":"NodeParagraph","Properties":{"id":"20250919161934-36c487k","updated":"20250919161934"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"非线性关系"},{"Type":"NodeText","Data":": 性能与正确标签数量之间并非线性关系。例如，从100%正确下降到50%正确，性能损失非常小，这进一步证明了模型对输入-标签映射的"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"不敏感性"},{"Type":"NodeText","Data":"。"}]}]},{"ID":"20250919161934-uicxs3c","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919161934-uicxs3c","updated":"20250919161934"},"Children":[{"ID":"20250919161934-mfq70x5","Type":"NodeParagraph","Properties":{"id":"20250919161934-mfq70x5","updated":"20250919161934"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"GPT-J的特例"},{"Type":"NodeText","Data":": 作者在正文中也提到，GPT-J在分类任务上是一个“异常值”，它对正确标签的数量相对更敏感一些，性能下降的坡度更陡。然而，即便如此，0%正确率的演示仍然显著优于无演示，这说明基本结论依然成立。"}]}]},{"ID":"20250919161934-rxlxz34","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919161934-rxlxz34","updated":"20250919161934"},"Children":[{"ID":"20250919161934-smk0ued","Type":"NodeParagraph","Properties":{"id":"20250919161934-smk0ued","updated":"20250919161934"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"结论强化"},{"Type":"NodeText","Data":": 该图通过更细粒度的实验，强有力地支持了“标签正确性不重要”的观点。它表明，只要提供了演示的“架子”（格式、标签空间等），即使里面的内容是错误的，模型也能取得不错的表现。"}]}]}]}]},{"ID":"20250919161934-zei2609","Type":"NodeParagraph","Properties":{"id":"20250919161934-zei2609","updated":"20250919161934"},"Children":[{"Type":"NodeText","Data":"结果报告在图4中。"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"模型性能对演示中正确标签的数量相当不敏感"},{"Type":"NodeText","Data":"。事实上，总是使用不正确的标签（例如，保留了使用演示所带来的92%和97%的性能提升，分别对应MetaICL分类任务和GPT-J多项选择任务）显著优于不使用演示。GPT-J在分类任务上是一个异常值，其性能相对依赖于正确标签的数量——它在有更多正确标签时表现更好。尽管如此，总是使用不正确的标签仍然显著优于不使用演示。"}]},{"ID":"20250919161934-h4scbmz","Type":"NodeParagraph","Properties":{"id":"20250919161934-h4scbmz","updated":"20250919161934"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"结果是否随着k的变化而保持一致？ (Is the result consistent with varying k?)"},{"Type":"NodeText","Data":" 我们研究了输入-标签对数量（k）的影响。结果报告在图5中。首先，即使k很小（k=4），使用演示也显著优于不使用演示，并且在不同的k值下，从黄金标签到随机标签的性能下降始终在0.8-1.6%的范围内。有趣的是，"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"当k≥8时，无论使用黄金标签还是随机标签，模型性能的增长都不如k增加时那么大"},{"Type":"NodeText","Data":"。这与典型的监督学习形成对比，在监督学习中，模型性能会随着k的增加而迅速提升，尤其是在k很小的时候。我们假设，更大的带标签数据主要有益于监督输入-标签的对应关系，而其他数据组成部分，如示例输入、示例标签和数据格式，更容易从少量数据中恢复，这可能是导致k较大时性能增益最小的原因（更多讨论见第5节）。"}]},{"ID":"20250919162057-t1zc06u","Type":"NodeParagraph","Properties":{"id":"20250919162057-t1zc06u","updated":"20250919162057"},"Children":[{"Type":"NodeText","Data":"​"},{"Type":"NodeImage","Data":"span","Children":[{"Type":"NodeBang"},{"Type":"NodeOpenBracket"},{"Type":"NodeLinkText","Data":"image"},{"Type":"NodeCloseBracket"},{"Type":"NodeOpenParen"},{"Type":"NodeLinkDest","Data":"assets/image-20250919162057-azza74q.png"},{"Type":"NodeCloseParen"}]},{"Type":"NodeText","Data":"​"}]},{"ID":"20250919161934-sa0usux","Type":"NodeBlockquote","Properties":{"fold":"1","id":"20250919161934-sa0usux","updated":"20250919161934"},"Children":[{"Type":"NodeBlockquoteMarker","Data":"\u003e"},{"ID":"20250919161934-31qjllt","Type":"NodeHeading","HeadingLevel":5,"Properties":{"id":"20250919161934-31qjllt","updated":"20250919161934"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"图5：演示中样本数量（k）变化的消融实验"}]},{"ID":"20250919161934-qrl7ryt","Type":"NodeParagraph","Properties":{"id":"20250919161934-qrl7ryt","updated":"20250919161934"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"图片内容翻译:"}]},{"ID":"20250919161934-z007xoy","Type":"NodeList","ListData":{},"Properties":{"id":"20250919161934-z007xoy","updated":"20250919161934"},"Children":[{"ID":"20250919161934-l562eyg","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919161934-l562eyg","updated":"20250919161934"},"Children":[{"ID":"20250919161934-v889b49","Type":"NodeParagraph","Properties":{"id":"20250919161934-v889b49","updated":"20250919161934"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"图例:"}]},{"ID":"20250919161934-k391fjd","Type":"NodeList","ListData":{},"Properties":{"id":"20250919161934-k391fjd","updated":"20250919161934"},"Children":[{"ID":"20250919161934-sbgu7au","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919161934-sbgu7au","updated":"20250919161934"},"Children":[{"ID":"20250919161934-6vie45s","Type":"NodeParagraph","Properties":{"id":"20250919161934-6vie45s","updated":"20250919161934"},"Children":[{"Type":"NodeText","Data":"橙色线: 使用黄金标签的演示 (Demos w/ gold)"}]}]},{"ID":"20250919161934-6aww2tm","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919161934-6aww2tm","updated":"20250919161934"},"Children":[{"ID":"20250919161934-x279cts","Type":"NodeParagraph","Properties":{"id":"20250919161934-x279cts","updated":"20250919161934"},"Children":[{"Type":"NodeText","Data":"蓝色线: 使用随机标签的演示 (Demos w/ random)"}]}]}]}]},{"ID":"20250919161934-afkjqew","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919161934-afkjqew","updated":"20250919161934"},"Children":[{"ID":"20250919161934-cvjwltd","Type":"NodeParagraph","Properties":{"id":"20250919161934-cvjwltd","updated":"20250919161934"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"左图: 分类 (Classification)"}]}]},{"ID":"20250919161934-m7hwrtp","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919161934-m7hwrtp","updated":"20250919161934"},"Children":[{"ID":"20250919161934-s1a217d","Type":"NodeParagraph","Properties":{"id":"20250919161934-s1a217d","updated":"20250919161934"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"右图: 多项选择 (Multi-choice)"}]}]},{"ID":"20250919161934-zgzipce","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919161934-zgzipce","updated":"20250919161934"},"Children":[{"ID":"20250919161934-6cu5sj1","Type":"NodeParagraph","Properties":{"id":"20250919161934-6cu5sj1","updated":"20250919161934"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"横坐标:"},{"Type":"NodeText","Data":" 演示中的样本数量 (k)"}]}]},{"ID":"20250919161934-k19zsux","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919161934-k19zsux","updated":"20250919161934"},"Children":[{"ID":"20250919161934-0lwr40s","Type":"NodeParagraph","Properties":{"id":"20250919161934-0lwr40s","updated":"20250919161934"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"纵坐标:"},{"Type":"NodeText","Data":" Macro-F1 (%) 或 准确率 (%)"}]}]},{"ID":"20250919161934-qs7qmzw","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919161934-qs7qmzw","updated":"20250919161934"},"Children":[{"ID":"20250919161934-p4nslh6","Type":"NodeParagraph","Properties":{"id":"20250919161934-p4nslh6","updated":"20250919161934"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"图解说明:"},{"Type":"NodeText","Data":" 在演示中改变样本数量（k）的消融实验。模型分别是各自任务类别下13B参数以下表现最好的（分别是通道MetaICL和直接GPT-J）。"}]}]}]},{"ID":"20250919161934-lh1jzz6","Type":"NodeHeading","HeadingLevel":3,"Properties":{"id":"20250919161934-lh1jzz6","updated":"20250919161934"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"内容解析"}]},{"ID":"20250919161934-0pth7pt","Type":"NodeList","ListData":{},"Properties":{"id":"20250919161934-0pth7pt","updated":"20250919161934"},"Children":[{"ID":"20250919161934-nngqvvl","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919161934-nngqvvl","updated":"20250919161934"},"Children":[{"ID":"20250919161934-f5gzsbv","Type":"NodeParagraph","Properties":{"id":"20250919161934-f5gzsbv","updated":"20250919161934"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"一致的微小差距"},{"Type":"NodeText","Data":": 两条曲线（黄金标签和随机标签）在所有k值上都"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"几乎重合"},{"Type":"NodeText","Data":"，它们之间的差距非常小且稳定。这再次证明，无论提供多少个示例，标签的正确性影响甚微。"}]}]},{"ID":"20250919161934-qee7tss","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919161934-qee7tss","updated":"20250919161934"},"Children":[{"ID":"20250919161934-ky4ycr6","Type":"NodeParagraph","Properties":{"id":"20250919161934-ky4ycr6","updated":"20250919161934"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"边际效益递减"},{"Type":"NodeText","Data":": 两条曲线都在k值较小（如从0到8）时快速上升，但在"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"k≥8之后变得非常平坦"},{"Type":"NodeText","Data":"。这揭示了一个重要现象：情境学习从示例中获取的“元信息”（格式、标签空间等）很快就饱和了。超过一定数量后，提供更多的示例并不能带来显著的性能提升。"}]}]},{"ID":"20250919161934-c5dltv7","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919161934-c5dltv7","updated":"20250919161934"},"Children":[{"ID":"20250919161934-4q12cpb","Type":"NodeParagraph","Properties":{"id":"20250919161934-4q12cpb","updated":"20250919161934"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"与监督学习的对比"},{"Type":"NodeText","Data":": 作者敏锐地指出了这与传统监督学习的根本不同。在监督学习中，数据量通常是越多越好。而在情境学习中，似乎只需要少量示例来“激活”模型对任务格式的理解，之后更多的示例就显得冗余。"}]}]}]}]},{"ID":"20250919161934-gnxn14f","Type":"NodeParagraph","Properties":{"id":"20250919161934-gnxn14f","updated":"20250919162146"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"结果是否与更好的模板一致？ (Is the result consistent with better templates?)"},{"Type":"NodeText","Data":" 虽然我们默认使用最简洁的模板，但我们也探索了"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"手动模板"},{"Type":"NodeText","Data":"，即以特定于数据集的方式编写的模板，这些模板取自先前的工作（细节见附录B）。图6显示，"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"用随机标签替换黄金标签会损害性能的趋势在使用手动模板时依然成立"},{"Type":"NodeText","Data":"。值得注意的是，使用手动模板并不总是优于使用最简洁的模板。"}]},{"ID":"20250919162126-ynjekfy","Type":"NodeParagraph","Properties":{"id":"20250919162126-ynjekfy","updated":"20250919162126"},"Children":[{"Type":"NodeText","Data":"​"},{"Type":"NodeImage","Data":"span","Children":[{"Type":"NodeBang"},{"Type":"NodeOpenBracket"},{"Type":"NodeLinkText","Data":"image"},{"Type":"NodeCloseBracket"},{"Type":"NodeOpenParen"},{"Type":"NodeLinkDest","Data":"assets/image-20250919162126-0edorzd.png"},{"Type":"NodeCloseParen"}]},{"Type":"NodeText","Data":"​"}]},{"ID":"20250919161934-ue5it7e","Type":"NodeBlockquote","Properties":{"fold":"1","id":"20250919161934-ue5it7e","updated":"20250919162158"},"Children":[{"ID":"20250919161934-x0ahuhs","Type":"NodeHeading","HeadingLevel":5,"Properties":{"id":"20250919161934-x0ahuhs","updated":"20250919162158"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"图6：使用最简洁模板和手动模板的结果"}]},{"Type":"NodeBlockquoteMarker","Data":"\u003e","Properties":{"updated":"20250919162158"}},{"ID":"20250919161934-9jtfsrv","Type":"NodeParagraph","Properties":{"id":"20250919161934-9jtfsrv","updated":"20250919162158"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"图片内容翻译:"}]},{"ID":"20250919161934-6l8k7bl","Type":"NodeList","ListData":{},"Properties":{"id":"20250919161934-6l8k7bl","updated":"20250919162158"},"Children":[{"ID":"20250919161934-5qd6l5s","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919161934-5qd6l5s","updated":"20250919161934"},"Children":[{"ID":"20250919161934-p53a9s8","Type":"NodeParagraph","Properties":{"id":"20250919161934-p53a9s8","updated":"20250919161934"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"图例:"}]},{"ID":"20250919161934-i02o69v","Type":"NodeList","ListData":{},"Properties":{"id":"20250919161934-i02o69v","updated":"20250919161934"},"Children":[{"ID":"20250919161934-vjje8qw","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919161934-vjje8qw","updated":"20250919161934"},"Children":[{"ID":"20250919161934-q5f8kyd","Type":"NodeParagraph","Properties":{"id":"20250919161934-q5f8kyd","updated":"20250919161934"},"Children":[{"Type":"NodeText","Data":"蓝色: 无演示 (No demos)"}]}]},{"ID":"20250919161934-b6w45m0","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919161934-b6w45m0","updated":"20250919161934"},"Children":[{"ID":"20250919161934-ppru9ms","Type":"NodeParagraph","Properties":{"id":"20250919161934-ppru9ms","updated":"20250919161934"},"Children":[{"Type":"NodeText","Data":"橙色: 黄金标签 (Gold labels)"}]}]},{"ID":"20250919161934-v4msd30","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919161934-v4msd30","updated":"20250919161934"},"Children":[{"ID":"20250919161934-oe9i957","Type":"NodeParagraph","Properties":{"id":"20250919161934-oe9i957","updated":"20250919161934"},"Children":[{"Type":"NodeText","Data":"浅橙色: 随机标签 (Random labels)"}]}]},{"ID":"20250919161934-9dnst3i","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919161934-9dnst3i","updated":"20250919161934"},"Children":[{"ID":"20250919161934-7uq4gy8","Type":"NodeParagraph","Properties":{"id":"20250919161934-7uq4gy8","updated":"20250919161934"},"Children":[{"Type":"NodeText","Data":"​"},{"Type":"NodeTextMark","TextMarkType":"code","TextMarkTextContent":"+T"},{"Type":"NodeText","Data":"​ 表示使用了手动模板 (manual templates)。"}]}]}]}]},{"ID":"20250919161934-y5zhqpa","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919161934-y5zhqpa","updated":"20250919161934"},"Children":[{"ID":"20250919161934-fv0c0ct","Type":"NodeParagraph","Properties":{"id":"20250919161934-fv0c0ct","updated":"20250919161934"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"横坐标:"},{"Type":"NodeText","Data":" 展示了MetaICL和GPT-J在分类和多项选择任务上的表现。"}]}]},{"ID":"20250919161934-3uw5dw6","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919161934-3uw5dw6","updated":"20250919161934"},"Children":[{"ID":"20250919161934-sp3x2lq","Type":"NodeParagraph","Properties":{"id":"20250919161934-sp3x2lq","updated":"20250919161934"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"纵坐标:"},{"Type":"NodeText","Data":" 准确率 (%)"}]}]},{"ID":"20250919161934-loa0nc3","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919161934-loa0nc3","updated":"20250919161934"},"Children":[{"ID":"20250919161934-9g0twnv","Type":"NodeParagraph","Properties":{"id":"20250919161934-9g0twnv","updated":"20250919161934"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"图解说明:"},{"Type":"NodeText","Data":" 使用最簡潔模板和手动模板的结果。‘+T’表示使用了手动模板。分类和多项选择任务分别使用通道（Channel）和直接（Direct）方法。"}]}]}]},{"ID":"20250919161934-nz59v7m","Type":"NodeHeading","HeadingLevel":3,"Properties":{"id":"20250919161934-nz59v7m","updated":"20250919161934"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"内容解析"}]},{"ID":"20250919161934-kmnp3rh","Type":"NodeList","ListData":{},"Properties":{"id":"20250919161934-kmnp3rh","updated":"20250919161934"},"Children":[{"ID":"20250919161934-1mz8ldm","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919161934-1mz8ldm","updated":"20250919161934"},"Children":[{"ID":"20250919161934-sqq0u9g","Type":"NodeParagraph","Properties":{"id":"20250919161934-sqq0u9g","updated":"20250919161934"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"结论的稳健性"},{"Type":"NodeText","Data":": 该图的主要目的是进行稳健性检验。通过对比不带"},{"Type":"NodeTextMark","TextMarkType":"code","TextMarkTextContent":"+T"},{"Type":"NodeText","Data":"​的组（最簡潔模板）和带"},{"Type":"NodeTextMark","TextMarkType":"code","TextMarkTextContent":"+T"},{"Type":"NodeText","Data":"​的组（手动模板），我们可以看到，在两种模板设置下，"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"随机标签（浅橙/粉色）和黄金标签（橙色）的性能都非常接近"},{"Type":"NodeText","Data":"。这表明“标签正确性不重要”的结论不受模板设计的影响，具有很强的普适性。"}]}]},{"ID":"20250919161934-n9ue71r","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919161934-n9ue71r","updated":"20250919161934"},"Children":[{"ID":"20250919161934-pq10qjf","Type":"NodeParagraph","Properties":{"id":"20250919161934-pq10qjf","updated":"20250919161934"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"模板并非万能"},{"Type":"NodeText","Data":": 一个有趣的附加发现是，精心设计的手动模板（带"},{"Type":"NodeTextMark","TextMarkType":"code","TextMarkTextContent":"+T"},{"Type":"NodeText","Data":"​的条）并不一定比最簡潔的模板效果更好。在某些情况下（如GPT-J分类），手动模板甚至可能降低性能。这暗示了在情境学习中，过于复杂的模板可能会引入噪声或误导，简单的格式指令可能更有效。"}]}]}]}]},{"ID":"20250919161934-k54a0ex","Type":"NodeBlockquote","Properties":{"fold":"1","id":"20250919161934-k54a0ex","updated":"20250919161934"},"Children":[{"Type":"NodeBlockquoteMarker","Data":"\u003e"},{"ID":"20250919161934-schbvw5","Type":"NodeHeading","HeadingLevel":3,"Properties":{"id":"20250919161934-schbvw5","updated":"20250919161934"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"综合性解析"}]},{"ID":"20250919161934-0hj89au","Type":"NodeParagraph","Properties":{"id":"20250919161934-0hj89au","updated":"20250919161934"},"Children":[{"Type":"NodeText","Data":"本章节是论文的核心实验部分，通过一系列精心设计的实验和消融研究，系统性地、全方位地论证了核心观点："},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"在情境学习中，演示的真实输入-标签映射（即标签的正确性）无关紧要。"}]},{"ID":"20250919161934-yhh98nx","Type":"NodeList","ListData":{"Typ":1},"Properties":{"id":"20250919161934-yhh98nx","updated":"20250919161934"},"Children":[{"ID":"20250919161934-ul5bh2r","Type":"NodeListItem","ListData":{"Typ":1,"Delimiter":46,"Marker":"MS4=","Num":1},"Properties":{"id":"20250919161934-ul5bh2r","updated":"20250919161934"},"Children":[{"ID":"20250919161934-6cumiw8","Type":"NodeParagraph","Properties":{"id":"20250919161934-6cumiw8","updated":"20250919161934"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"核心论证 (4.1节)"},{"Type":"NodeText","Data":": 作者首先定义了三种实验条件（无演示、黄金标签、随机标签），并通过实验结果（图3）直接展示了核心发现：随机标签演示的性能与黄金标签演示的性能惊人地接近，且两者都远超无演示。这直接挑战了我们对“学习”的传统认知，并指出模型并非从示例中学习“知识”，而是学习“模式”。"}]}]},{"ID":"20250919161934-akawuya","Type":"NodeListItem","ListData":{"Typ":1,"Delimiter":46,"Marker":"Mi4=","Num":2},"Properties":{"id":"20250919161934-akawuya","updated":"20250919161934"},"Children":[{"ID":"20250919161934-rxeq1q6","Type":"NodeParagraph","Properties":{"id":"20250919161934-rxeq1q6","updated":"20250919161934"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"深度挖掘与稳健性检验 (4.2节)"},{"Type":"NodeText","Data":": 为了使结论更加坚实，作者进行了一系列消融实验，从不同角度审视这一现象："}]},{"ID":"20250919161934-4ycqp1x","Type":"NodeList","ListData":{},"Properties":{"id":"20250919161934-4ycqp1x","updated":"20250919161934"},"Children":[{"ID":"20250919161934-bxt43lm","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919161934-bxt43lm","updated":"20250919161934"},"Children":[{"ID":"20250919161934-fdlh9md","Type":"NodeParagraph","Properties":{"id":"20250919161934-fdlh9md","updated":"20250919161934"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"正确率的影响 (图4)"},{"Type":"NodeText","Data":": 实验表明，性能对演示中正确标签的比例"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"不敏感"},{"Type":"NodeText","Data":"。即使所有标签都是错误的，模型依然能取得很好的效果。这说明模型需要的不是“正确”的关联，而仅仅是“存在”关联的这个格式。"}]}]},{"ID":"20250919161934-acfimar","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919161934-acfimar","updated":"20250919161934"},"Children":[{"ID":"20250919161934-4y3srkf","Type":"NodeParagraph","Properties":{"id":"20250919161934-4y3srkf","updated":"20250919161934"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"示例数量的影响 (图5)"},{"Type":"NodeText","Data":": 实验揭示了情境学习的"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"边际效益递减"},{"Type":"NodeText","Data":"特性。模型性能在少量示例（k\u003c8）后就迅速饱和，更多的示例几乎带不来提升。这与传统监督学习中数据越多越好的规律形成鲜明对比，暗示情境学习获取的是有限的“元信息”，而非无限的“知识”。"}]}]},{"ID":"20250919161934-8xysj07","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919161934-8xysj07","updated":"20250919161934"},"Children":[{"ID":"20250919161934-c3uelkv","Type":"NodeParagraph","Properties":{"id":"20250919161934-c3uelkv","updated":"20250919161934"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"模板设计的影响 (图6)"},{"Type":"NodeText","Data":": 实验证明，核心发现"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"不受提示模板（template）复杂度的影响"},{"Type":"NodeText","Data":"。无论使用最简单的模板还是精心设计的手动模板，随机标签的效果都与黄金标签相近。"}]}]}]}]},{"ID":"20250919161934-7s9mvkw","Type":"NodeListItem","ListData":{"Typ":1,"Delimiter":46,"Marker":"My4=","Num":3},"Properties":{"id":"20250919161934-7s9mvkw","updated":"20250919161934"},"Children":[{"ID":"20250919161934-l0wfqdj","Type":"NodeParagraph","Properties":{"id":"20250919161934-l0wfqdj","updated":"20250919161934"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"元训练的特殊作用"},{"Type":"NodeText","Data":": 一个贯穿本章的发现是，经过"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"元训练（meta-training）的MetaICL模型"},{"Type":"NodeText","Data":"对错误标签的“免疫力”更强。这表明，在训练阶段就接触过情境学习任务的模型，会更加倾向于依赖演示的格式、标签空间等结构性信息，从而彻底忽略了输入与标签之间的具体映射关系。"}]}]}]},{"ID":"20250919161934-8x7p39n","Type":"NodeParagraph","Properties":{"id":"20250919161934-8x7p39n","updated":"20250919161934"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"总结"},{"Type":"NodeText","Data":": 整个第四章通过层层递进的实验，构建了一个无可辩驳的证据链，证明了大型语言模型在情境学习中扮演的角色更像一个“"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"格式遵循者"},{"Type":"NodeText","Data":"”而非“"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"逻辑学习者"},{"Type":"NodeText","Data":"”。这一发现深刻地重塑了我们对情境学习机制的理解。"}]}]},{"ID":"20250919163223-27guclt","Type":"NodeHeading","HeadingLevel":5,"Properties":{"id":"20250919163223-27guclt","updated":"20250919163438"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"5.2 标签空间的影响"}]},{"ID":"20250919163223-krihk50","Type":"NodeParagraph","Properties":{"id":"20250919163223-krihk50","updated":"20250919163224"},"Children":[{"Type":"NodeText","Data":"我们还对"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"使用随机英文单词的演示"},{"Type":"NodeText","Data":"进行了实验，即用随机的英文单词作为所有 "},{"Type":"NodeTextMark","TextMarkType":"code","TextMarkTextContent":"k"},{"Type":"NodeText","Data":"​ 个配对的标签。具体来说，我们采样了一个与原标签集大小相同的随机英文单词子集 "},{"Type":"NodeTextMark","TextMarkType":"code","TextMarkTextContent":"C_rand"},{"Type":"NodeText","Data":"​，并随机地将 "},{"Type":"NodeTextMark","TextMarkType":"code","TextMarkTextContent":"ỹᵢ ∈ C_rand"},{"Type":"NodeText","Data":"​ 与 "},{"Type":"NodeTextMark","TextMarkType":"code","TextMarkTextContent":"xᵢ"},{"Type":"NodeText","Data":"​ 进行配对。这个变体评估了"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"标签空间"},{"Type":"NodeText","Data":"的影响，同时保持了"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"输入文本的分布"},{"Type":"NodeText","Data":"和"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"演示的格式"},{"Type":"NodeText","Data":"不变。"}]},{"ID":"20250919163223-fo9i4ol","Type":"NodeParagraph","Properties":{"id":"20250919163223-fo9i4ol","updated":"20250919163224"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"结果。"},{"Type":"NodeText","Data":" 根据图9，直接模型和通道模型表现出不同的模式。对于"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"直接模型"},{"Type":"NodeText","Data":"，在使用标签空间内的随机标签和使用随机英文单词之间，性能差距显著，范围在5-16%绝对值之间。这表明"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"对标签空间进行条件化对性能增益有显著贡献"},{"Type":"NodeText","Data":"。这甚至对于没有固定标签集的多项选择任务也成立——我们假设多项选择任务的选项仍然具有特定的分布（例如，OpenBookQA数据集中像“螺栓”或“螺丝”这样的物体），模型会利用这一点。"}]},{"ID":"20250919163223-ucgfirl","Type":"NodeParagraph","Properties":{"id":"20250919163223-ucgfirl","updated":"20250919163224"},"Children":[{"Type":"NodeText","Data":"另一方面，在"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"通道模型"},{"Type":"NodeText","Data":"中，移除输出空间（即标签空间）并不会导致性能显著下降：绝对值下降幅度为0-2%，有时甚至有所增加。我们假设这是因为通道模型只对标签进行条件化，因此"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"并不能从知晓标签空间中受益"},{"Type":"NodeText","Data":"。这与直接模型形成对比，后者需要生成正确的标签。"}]},{"ID":"20250919163223-ubdume0","Type":"NodeHeading","HeadingLevel":5,"Properties":{"id":"20250919163223-ubdume0","updated":"20250919163438"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"5.3 输入-标签配对的影响"}]},{"ID":"20250919163223-ocmuqq4","Type":"NodeParagraph","Properties":{"id":"20250919163223-ocmuqq4","updated":"20250919163224"},"Children":[{"Type":"NodeText","Data":"第5.1节和5.2节关注的是尽可能保持演示格式不变的变体。本节探讨改变格式的变体。虽然改变格式的方式有很多种，我们只做了最小的修改来移除输入到输出的配对。具体来说，我们评估了 (1) "},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"无标签的演示"},{"Type":"NodeText","Data":"，其中语言模型仅对 "},{"Type":"NodeTextMark","TextMarkType":"code","TextMarkTextContent":"x₁...xₖ"},{"Type":"NodeText","Data":"​ 的拼接进行条件化，以及 (2) "},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"仅有标签的演示"},{"Type":"NodeText","Data":"，其中语言模型仅对 "},{"Type":"NodeTextMark","TextMarkType":"code","TextMarkTextContent":"y₁...yₖ"},{"Type":"NodeText","Data":"​ 的拼接进行条件化。这些消融实验分别为“使用随机英文单词的演示”和“使用分布外输入的演示”提供了“无格式”的对应项。"}]},{"ID":"20250919163223-9a9hsy1","Type":"NodeParagraph","Properties":{"id":"20250919163223-9a9hsy1","updated":"20250919163224"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"结果。"},{"Type":"NodeText","Data":" 根据图10，"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"移除格式"},{"Type":"NodeText","Data":"后的性能接近甚至差于无演示，这表明了"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"格式的重要性"},{"Type":"NodeText","Data":"。这很可能是因为对输入-标签对序列进行条件化会"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"触发模型模仿整体格式"},{"Type":"NodeText","Data":"，并在给定测试输入时按预期完成新样本。"}]},{"ID":"20250919163223-a1rr0mj","Type":"NodeParagraph","Properties":{"id":"20250919163223-a1rr0mj","updated":"20250919163223"},"Children":[{"Type":"NodeText","Data":"更有趣的是，"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"保留格式"},{"Type":"NodeText","Data":"在仅使用输入或仅使用标签来保留大部分性能增益方面发挥着重要作用。例如，对于Direct MetaICL，仅通过从语料库中采样随机句子并将其与标签空间中的标签随机配对，就可以保留95%和82%的来自情境学习（使用黄金标签的演示）的性能提升（见图10中分类和多项选择任务的"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"浅紫色条"},{"Type":"NodeText","Data":"）。类似地，对于通道模型，仅通过将未标记的训练数据中的每个输入与一个随机英文单词配对，就可以保留82%，86%和75%的性能提升（见图10中MetaICL分类、GPT-J分类、MetaICL多项选择和GPT-J多项选择任务的"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"绿色条"},{"Type":"NodeText","Data":"）。在所有这些案例中，移除输入或标签，而不是使用分布外输入或随机英文单词，性能会显著更差，这表明"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"保持输入-标签配对的格式是关键"},{"Type":"NodeText","Data":"。"}]},{"ID":"20250919163304-uwo41f7","Type":"NodeParagraph","Properties":{"id":"20250919163304-uwo41f7","updated":"20250919163304"},"Children":[{"Type":"NodeText","Data":"​"},{"Type":"NodeImage","Data":"span","Children":[{"Type":"NodeBang"},{"Type":"NodeOpenBracket"},{"Type":"NodeLinkText","Data":"image"},{"Type":"NodeCloseBracket"},{"Type":"NodeOpenParen"},{"Type":"NodeLinkDest","Data":"assets/image-20250919163304-gu7n7ic.png"},{"Type":"NodeCloseParen"}]},{"Type":"NodeText","Data":"​"}]},{"ID":"20250919163223-e79gcls","Type":"NodeBlockquote","Properties":{"fold":"1","id":"20250919163223-e79gcls","updated":"20250919163224"},"Children":[{"Type":"NodeBlockquoteMarker","Data":"\u003e"},{"ID":"20250919163223-gcj8oz8","Type":"NodeHeading","HeadingLevel":5,"Properties":{"id":"20250919163223-gcj8oz8","updated":"20250919163223"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"图10：格式的影响，即输入-标签对的使用"}]},{"ID":"20250919163223-kel3n1m","Type":"NodeParagraph","Properties":{"id":"20250919163223-kel3n1m","updated":"20250919163223"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"图片内容翻译:"}]},{"ID":"20250919163223-migjfnd","Type":"NodeList","ListData":{},"Properties":{"id":"20250919163223-migjfnd","updated":"20250919163223"},"Children":[{"ID":"20250919163223-6k1uavu","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919163223-6k1uavu","updated":"20250919163223"},"Children":[{"ID":"20250919163223-bir6ool","Type":"NodeParagraph","Properties":{"id":"20250919163223-bir6ool","updated":"20250919163223"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"图例 (部分):"}]},{"ID":"20250919163223-divdjvw","Type":"NodeList","ListData":{},"Properties":{"id":"20250919163223-divdjvw","updated":"20250919163223"},"Children":[{"ID":"20250919163223-ya15a9x","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919163223-ya15a9x","updated":"20250919163223"},"Children":[{"ID":"20250919163223-y7tp65w","Type":"NodeParagraph","Properties":{"id":"20250919163223-y7tp65w","updated":"20250919163223"},"Children":[{"Type":"NodeText","Data":"浅紫色 (OOD + Random labels): 分布外输入 + 随机标签"}]}]},{"ID":"20250919163223-arfv9kl","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919163223-arfv9kl","updated":"20250919163223"},"Children":[{"ID":"20250919163223-f15wl5u","Type":"NodeParagraph","Properties":{"id":"20250919163223-f15wl5u","updated":"20250919163223"},"Children":[{"Type":"NodeText","Data":"深紫色 (Random labels only): 仅随机标签 (无输入)"}]}]},{"ID":"20250919163223-ls7rr4m","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919163223-ls7rr4m","updated":"20250919163223"},"Children":[{"ID":"20250919163223-7e6lv9m","Type":"NodeParagraph","Properties":{"id":"20250919163223-7e6lv9m","updated":"20250919163223"},"Children":[{"Type":"NodeText","Data":"绿色 (Random English words): (同分布输入 +) 随机英文单词"}]}]},{"ID":"20250919163223-6gdc5cy","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919163223-6gdc5cy","updated":"20250919163223"},"Children":[{"ID":"20250919163223-osqp9zc","Type":"NodeParagraph","Properties":{"id":"20250919163223-osqp9zc","updated":"20250919163223"},"Children":[{"Type":"NodeText","Data":"深绿色 (No labels): 无标签 (仅输入)"}]}]}]}]},{"ID":"20250919163223-wero7pi","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919163223-wero7pi","updated":"20250919163223"},"Children":[{"ID":"20250919163223-pjr4zfu","Type":"NodeParagraph","Properties":{"id":"20250919163223-pjr4zfu","updated":"20250919163223"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"右侧矩阵 (F, L, I, M):"},{"Type":"NodeText","Data":"  ✓表示保留该方面, ✗表示破坏该方面。"}]}]},{"ID":"20250919163223-r8wovq1","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919163223-r8wovq1","updated":"20250919163223"},"Children":[{"ID":"20250919163223-yedm97x","Type":"NodeParagraph","Properties":{"id":"20250919163223-yedm97x","updated":"20250919163223"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"图解说明:"},{"Type":"NodeText","Data":" 格式的影响，即输入-标签对的使用。在分类（顶部）和多项选择（底部）任务上评估。不保持格式的演示变体（"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"深紫色条"},{"Type":"NodeText","Data":"和"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"深绿色条"},{"Type":"NodeText","Data":"）的性能都不比无演示（蓝色条）好。当可以在有标签空间但无输入（比较"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"浅紫色条"},{"Type":"NodeText","Data":" vs. "},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"深紫色条"},{"Type":"NodeText","Data":" in Direct MetaICL），或者有输入分布但无标签（比较"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"绿色条"},{"Type":"NodeText","Data":" vs. "},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"深绿色条"},{"Type":"NodeText","Data":" in Channel MetaICL 和 Channel GPT-J）的情况下实现显著增益时，保持格式尤其重要。"}]}]}]},{"ID":"20250919163223-m1gm24n","Type":"NodeHeading","HeadingLevel":3,"Properties":{"id":"20250919163223-m1gm24n","updated":"20250919163223"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"内容解析"}]},{"ID":"20250919163223-yjkp3fg","Type":"NodeList","ListData":{},"Properties":{"id":"20250919163223-yjkp3fg","updated":"20250919163223"},"Children":[{"ID":"20250919163223-4dd25ia","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919163223-4dd25ia","updated":"20250919163223"},"Children":[{"ID":"20250919163223-dfd3lcr","Type":"NodeParagraph","Properties":{"id":"20250919163223-dfd3lcr","updated":"20250919163223"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"格式的决定性作用"},{"Type":"NodeText","Data":": 这是本章最关键的一张图。通过引入两个新的实验条件——“仅标签”（深紫色）和“仅输入”（深绿色），作者成功地"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"破坏了“格式（F）”"},{"Type":"NodeText","Data":"。结果一目了然："},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"深紫色和深绿色的条都大幅下跌，性能基本等同于甚至差于“无演示”（蓝色条）"},{"Type":"NodeText","Data":"。这无可辩驳地证明了，"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"输入-标签配对的格式本身是情境学习能够生效的“地基”"},{"Type":"NodeText","Data":"。"}]}]},{"ID":"20250919163223-2rjkzqt","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919163223-2rjkzqt","updated":"20250919163223"},"Children":[{"ID":"20250919163223-gmojg0t","Type":"NodeParagraph","Properties":{"id":"20250919163223-gmojg0t","updated":"20250919163223"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"格式的“粘合剂”效应"},{"Type":"NodeText","Data":": 这张图最巧妙的地方在于它的对比分析。"}]},{"ID":"20250919163223-y0h4pmw","Type":"NodeList","ListData":{},"Properties":{"id":"20250919163223-y0h4pmw","updated":"20250919163223"},"Children":[{"ID":"20250919163223-xgf91wj","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919163223-xgf91wj","updated":"20250919163223"},"Children":[{"ID":"20250919163223-8ww2zs9","Type":"NodeParagraph","Properties":{"id":"20250919163223-8ww2zs9","updated":"20250919163223"},"Children":[{"Type":"NodeText","Data":"比较"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"浅紫色条"},{"Type":"NodeText","Data":" (F✓ L✓ I✗ M✗) 和"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"深紫色条"},{"Type":"NodeText","Data":" (F✗ L✓ I✗ M✗)，两者唯一的区别就是有没有格式 (F)。保留格式带来了巨大的性能提升。这说明，即使输入是完全无关的，"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"格式也能将“标签空间”这个有效信息“粘合”起来，使其发挥作用"},{"Type":"NodeText","Data":"。"}]}]},{"ID":"20250919163223-afda7lt","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919163223-afda7lt","updated":"20250919163223"},"Children":[{"ID":"20250919163223-c1t02y1","Type":"NodeParagraph","Properties":{"id":"20250919163223-c1t02y1","updated":"20250919163223"},"Children":[{"Type":"NodeText","Data":"比较"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"绿色条"},{"Type":"NodeText","Data":" (F✓ L✗ I✓ M✗) 和"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"深绿色条"},{"Type":"NodeText","Data":" (F✗ L✗ I✓ M✗)，同样，唯一的区别也是格式 (F)。保留格式同样带来了显著的性能提升。这说明，即使标签是完全无关的，"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"格式也能将“输入分布”这个有效信息“粘合”起来，使其发挥作用"},{"Type":"NodeText","Data":"。"}]}]}]}]},{"ID":"20250919163223-vnu6s89","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919163223-vnu6s89","updated":"20250919163223"},"Children":[{"ID":"20250919163223-l7q79st","Type":"NodeParagraph","Properties":{"id":"20250919163223-l7q79st","updated":"20250919163223"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"结论"},{"Type":"NodeText","Data":": 格式本身就像是一个“开关”或“催化剂”。如果没有输入-标签配对的格式，即使提供了正确的输入分布或标签空间，模型也无法有效利用这些信息。"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"格式激活了模型进行“模式模仿”的能力"},{"Type":"NodeText","Data":"。"}]}]}]}]},{"ID":"20250919163223-zqbq3eh","Type":"NodeHeading","HeadingLevel":5,"Properties":{"id":"20250919163223-zqbq3eh","updated":"20250919163438"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"5.4 元训练的影响"}]},{"ID":"20250919163223-d05b9ie","Type":"NodeParagraph","Properties":{"id":"20250919163223-d05b9ie","updated":"20250919163224"},"Children":[{"Type":"NodeText","Data":"与其他模型不同，MetaICL是使用情境学习目标进行训练的，这与近期许多使用多任务训练（在一个大型监督数据集集合上）以泛化到新任务的工作一致 (Aghajanyan et al., 2021; Khashabi et al., 2020; Wei et al., 2022; Sanh et al., 2022)。我们的目标是通过仔细检查MetaICL的结果，来更好地理解元训练在我们发现中的作用。特别是，我们观察到目前为止看到的模式在MetaICL上比在其他模型上"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"显著更明显"},{"Type":"NodeText","Data":"。例如，"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"真实输入-标签映射的影响更小，而保持演示格式的影响更大"},{"Type":"NodeText","Data":"。在Direct MetaICL中，输入-标签映射和输入分布几乎没有影响；在Channel MetaICL中，输入-标签映射和输出空间几乎没有影响。"}]},{"ID":"20250919163223-6djzeqa","Type":"NodeParagraph","Properties":{"id":"20250919163223-6djzeqa","updated":"20250919163224"},"Children":[{"Type":"NodeText","Data":"基于这一观察，我们假设"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"元训练鼓励模型专门利用演示中更简单的方面，而忽略其他方面"},{"Type":"NodeText","Data":"。这是基于我们的直觉：(1) 输入-标签映射更难利用，(2) 格式可能更容易利用，以及 (3) 模型被训练生成的文本空间可能比模型进行条件化的文本空间更容易利用。"}]},{"ID":"20250919163223-ou32bz1","Type":"NodeBlockquote","Properties":{"fold":"1","id":"20250919163223-ou32bz1","updated":"20250919163224"},"Children":[{"Type":"NodeBlockquoteMarker","Data":"\u003e"},{"ID":"20250919163223-eck619h","Type":"NodeHeading","HeadingLevel":3,"Properties":{"id":"20250919163223-eck619h","updated":"20250919163223"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"内容解析"}]},{"ID":"20250919163223-vrni0ac","Type":"NodeHeading","HeadingLevel":5,"Properties":{"id":"20250919163223-vrni0ac","updated":"20250919163223"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"元训练：学习如何“学习格式”"}]},{"ID":"20250919163223-2trmx0o","Type":"NodeList","ListData":{},"Properties":{"id":"20250919163223-2trmx0o","updated":"20250919163223"},"Children":[{"ID":"20250919163223-t6huljm","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919163223-t6huljm","updated":"20250919163223"},"Children":[{"ID":"20250919163223-rgzxrts","Type":"NodeParagraph","Properties":{"id":"20250919163223-rgzxrts","updated":"20250919163223"},"Children":[{"Type":"NodeText","Data":"本节对MetaICL模型的特殊表现进行了专题讨论，揭示了"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"元训练"},{"Type":"NodeText","Data":"的深层影响。"}]}]},{"ID":"20250919163223-xogaxsj","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919163223-xogaxsj","updated":"20250919163223"},"Children":[{"ID":"20250919163223-rqn49rq","Type":"NodeParagraph","Properties":{"id":"20250919163223-rqn49rq","updated":"20250919163223"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"现象总结"},{"Type":"NodeText","Data":": 作者观察到，在MetaICL上，之前发现的所有趋势都"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"被放大了"},{"Type":"NodeText","Data":"。它对输入-标签映射（M）和输入分布（I）这类“内容”信息"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"更加不敏感"},{"Type":"NodeText","Data":"，但对“格式（F）”这类结构性信息"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"更加依赖"},{"Type":"NodeText","Data":"。"}]}]},{"ID":"20250919163223-u0zdr2x","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919163223-u0zdr2x","updated":"20250919163223"},"Children":[{"ID":"20250919163223-rdpjdhv","Type":"NodeParagraph","Properties":{"id":"20250919163223-rdpjdhv","updated":"20250919163223"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"核心假设"},{"Type":"NodeText","Data":": 作者提出了一个深刻的假设——"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"元训练教会了模型走“捷径”"},{"Type":"NodeText","Data":"。在多任务的情境学习训练中，模型发现最普适、最容易迁移的“知识”不是每个任务的具体逻辑（这太难了），而是所有任务共有的"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"交互格式"},{"Type":"NodeText","Data":"。因此，模型学会了"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"优先利用最简单、最稳定的信号（如格式和标签空间），并忽略更复杂、更易变的信号（如输入-标签映射）"},{"Type":"NodeText","Data":"。"}]}]},{"ID":"20250919163223-capdyru","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919163223-capdyru","updated":"20250919163223"},"Children":[{"ID":"20250919163223-ji6p67m","Type":"NodeParagraph","Properties":{"id":"20250919163223-ji6p67m","updated":"20250919163223"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"Implication"},{"Type":"NodeText","Data":": 这意味着，元训练可能并没有让模型变得更“聪明”（即更能理解任务逻辑），而是让它变得更“熟练”（即更擅长识别和遵循指令格式）。这是一个对于如何构建和理解这类模型非常重要的洞见。"}]}]}]}]},{"ID":"20250919163223-dh16iey","Type":"NodeBlockquote","Properties":{"fold":"1","id":"20250919163223-dh16iey","updated":"20250919163224"},"Children":[{"Type":"NodeBlockquoteMarker","Data":"\u003e"},{"ID":"20250919163223-8365wej","Type":"NodeHeading","HeadingLevel":3,"Properties":{"id":"20250919163223-8365wej","updated":"20250919163223"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"综合性解析"}]},{"ID":"20250919163223-t1mz4sy","Type":"NodeParagraph","Properties":{"id":"20250919163223-t1mz4sy","updated":"20250919163223"},"Children":[{"Type":"NodeText","Data":"这一部分是论文论证过程的高潮，它在识别出情境学习的有效成分后，进一步揭示了这些成分之间的"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"重要性层级"},{"Type":"NodeText","Data":"，并对特殊的"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"元训练现象"},{"Type":"NodeText","Data":"给出了深刻的解释。"}]},{"ID":"20250919163223-4h6ln1o","Type":"NodeList","ListData":{"Typ":1},"Properties":{"id":"20250919163223-4h6ln1o","updated":"20250919163223"},"Children":[{"ID":"20250919163223-yujju9f","Type":"NodeListItem","ListData":{"Typ":1,"Delimiter":46,"Marker":"MS4=","Num":1},"Properties":{"id":"20250919163223-yujju9f","updated":"20250919163223"},"Children":[{"ID":"20250919163223-ei7ilnz","Type":"NodeParagraph","Properties":{"id":"20250919163223-ei7ilnz","updated":"20250919163223"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"验证“标签空间 (L)”的重要性 (5.2节)"},{"Type":"NodeText","Data":":"}]},{"ID":"20250919163223-9mhpwq3","Type":"NodeList","ListData":{},"Properties":{"id":"20250919163223-9mhpwq3","updated":"20250919163223"},"Children":[{"ID":"20250919163223-gzlcgdj","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919163223-gzlcgdj","updated":"20250919163223"},"Children":[{"ID":"20250919163223-964pxv7","Type":"NodeParagraph","Properties":{"id":"20250919163223-964pxv7","updated":"20250919163223"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"实验"},{"Type":"NodeText","Data":": 通过将演示中的标签替换为完全不相关的随机英文单词，来破坏标签空间。"}]}]},{"ID":"20250919163223-7bzxtli","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919163223-7bzxtli","updated":"20250919163223"},"Children":[{"ID":"20250919163223-48a5ucf","Type":"NodeParagraph","Properties":{"id":"20250919163223-48a5ucf","updated":"20250919163223"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"发现"},{"Type":"NodeText","Data":": 性能显著下降（尤其在Direct模型中），证明了"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"为模型提供一个明确的“答案选项范围”是有效的"},{"Type":"NodeText","Data":"。这帮助模型约束其输出，而不是进行毫无头绪的开放式生成。"}]}]}]}]},{"ID":"20250919163223-w8h4b9r","Type":"NodeListItem","ListData":{"Typ":1,"Delimiter":46,"Marker":"Mi4=","Num":2},"Properties":{"id":"20250919163223-w8h4b9r","updated":"20250919163223"},"Children":[{"ID":"20250919163223-to40ltt","Type":"NodeParagraph","Properties":{"id":"20250919163223-to40ltt","updated":"20250919163223"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"确立“格式 (F)”的核心地位 (5.3节 \u0026amp; 图10)"},{"Type":"NodeText","Data":":"}]},{"ID":"20250919163223-eao2k59","Type":"NodeList","ListData":{},"Properties":{"id":"20250919163223-eao2k59","updated":"20250919163223"},"Children":[{"ID":"20250919163223-124c6hs","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919163223-124c6hs","updated":"20250919163223"},"Children":[{"ID":"20250919163223-hpj4ao1","Type":"NodeParagraph","Properties":{"id":"20250919163223-hpj4ao1","updated":"20250919163223"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"实验"},{"Type":"NodeText","Data":": 通过只提供输入或只提供标签来彻底破坏“输入-标签配对”的格式。"}]}]},{"ID":"20250919163223-vepe1rn","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919163223-vepe1rn","updated":"20250919163223"},"Children":[{"ID":"20250919163223-limrows","Type":"NodeParagraph","Properties":{"id":"20250919163223-limrows","updated":"20250919163223"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"发现"},{"Type":"NodeText","Data":": 性能骤降至与无演示相当甚至更差的水平。这决定性地证明了"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"“格式”是所有其他信息能够生效的基础"},{"Type":"NodeText","Data":"。没有格式这个“骨架”，输入分布（I）和标签空间（L）这些“血肉”就无所依附。"}]}]},{"ID":"20250919163223-j0bcee9","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919163223-j0bcee9","updated":"20250919163223"},"Children":[{"ID":"20250919163223-yaxtcg4","Type":"NodeParagraph","Properties":{"id":"20250919163223-yaxtcg4","updated":"20250919163223"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"“粘合剂”效应"},{"Type":"NodeText","Data":": 更进一步的分析表明，"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"格式起到了“粘合剂”的作用"},{"Type":"NodeText","Data":"。它可以将单独来看相对较弱的信号（如分布外的输入或错误的标签）组合起来，使其共同发挥出远超各自独立作用的强大效果。"}]}]}]}]},{"ID":"20250919163223-udku7uf","Type":"NodeListItem","ListData":{"Typ":1,"Delimiter":46,"Marker":"My4=","Num":3},"Properties":{"id":"20250919163223-udku7uf","updated":"20250919163223"},"Children":[{"ID":"20250919163223-zc1fgpt","Type":"NodeParagraph","Properties":{"id":"20250919163223-zc1fgpt","updated":"20250919163223"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"揭示“元训练”的机制 (5.4节)"},{"Type":"NodeText","Data":":"}]},{"ID":"20250919163223-eh3upjs","Type":"NodeList","ListData":{},"Properties":{"id":"20250919163223-eh3upjs","updated":"20250919163223"},"Children":[{"ID":"20250919163223-oxyypua","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919163223-oxyypua","updated":"20250919163223"},"Children":[{"ID":"20250919163223-ypemp76","Type":"NodeParagraph","Properties":{"id":"20250919163223-ypemp76","updated":"20250919163223"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"观察"},{"Type":"NodeText","Data":": 元训练模型MetaICL将上述所有趋势都"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"极端化"},{"Type":"NodeText","Data":"了：它极度依赖格式，同时极度忽略输入-标签映射等内容信息。"}]}]},{"ID":"20250919163223-xu7xh05","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919163223-xu7xh05","updated":"20250919163223"},"Children":[{"ID":"20250919163223-wc3zad3","Type":"NodeParagraph","Properties":{"id":"20250919163223-wc3zad3","updated":"20250919163223"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"解释"},{"Type":"NodeText","Data":": 作者提出了一个精辟的见解——元训练是一种"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"“学习捷径”"},{"Type":"NodeText","Data":"的过程。模型在大量的任务中发现，掌握通用的“格式”是比学习每个任务特定“逻辑”更高效、更稳健的策略。因此，元训练强化了模型利用“简单信号”（如格式）并忽略“复杂信号”（如逻辑映射）的倾向。"}]}]}]}]}]},{"ID":"20250919163223-f374c0t","Type":"NodeParagraph","Properties":{"id":"20250919163223-f374c0t","updated":"20250919163223"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"最终结论层级"},{"Type":"NodeText","Data":":\n综合整个第5章，作者建立了一个关于情境学习有效成分的重要性层级："}]},{"ID":"20250919163223-vk7fart","Type":"NodeList","ListData":{},"Properties":{"id":"20250919163223-vk7fart","updated":"20250919163223"},"Children":[{"ID":"20250919163223-ciap98o","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919163223-ciap98o","updated":"20250919163223"},"Children":[{"ID":"20250919163223-vvvxl44","Type":"NodeParagraph","Properties":{"id":"20250919163223-vvvxl44","updated":"20250919163223"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"绝对核心 (Foundation)"},{"Type":"NodeText","Data":": "},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"格式 (F)"},{"Type":"NodeText","Data":"，即输入-标签配对的结构。"}]}]},{"ID":"20250919163223-xa14s8i","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919163223-xa14s8i","updated":"20250919163223"},"Children":[{"ID":"20250919163223-p3cprhx","Type":"NodeParagraph","Properties":{"id":"20250919163223-p3cprhx","updated":"20250919163223"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"重要支柱 (Pillars)"},{"Type":"NodeText","Data":": "},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"输入分布 (I)"},{"Type":"NodeText","Data":" 和 "},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"标签空间 (L)"},{"Type":"NodeText","Data":"。"}]}]},{"ID":"20250919163223-haus1sl","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919163223-haus1sl","updated":"20250919163223"},"Children":[{"ID":"20250919163223-6klb3hl","Type":"NodeParagraph","Properties":{"id":"20250919163223-6klb3hl","updated":"20250919163223"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"几乎无关 (Irrelevant)"},{"Type":"NodeText","Data":": "},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"输入-标签映射 (M)"},{"Type":"NodeText","Data":"。"}]}]}]},{"ID":"20250919163223-btqjua6","Type":"NodeParagraph","Properties":{"id":"20250919163223-btqjua6","updated":"20250919163223"},"Children":[{"Type":"NodeText","Data":"这个清晰的层级结构，是本文对理解“为什么情-境学习有效”这一问题所做出的最核心、最深刻的贡献。"}]}]},{"ID":"20250919163438-uenhfbg","Type":"NodeBlockquote","Properties":{"id":"20250919163438-uenhfbg","updated":"20250919163438"},"Children":[{"Type":"NodeBlockquoteMarker","Data":"\u003e"},{"ID":"20250919163438-gxg44yk","Type":"NodeHeading","HeadingLevel":3,"Properties":{"id":"20250919163438-gxg44yk","updated":"20250919163438"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"综合性解析"}]},{"ID":"20250919163438-tpb7o0r","Type":"NodeParagraph","Properties":{"id":"20250919163438-tpb7o0r","updated":"20250919163438"},"Children":[{"Type":"NodeText","Data":"本文通过一系列严谨而精妙的实验，对大型语言模型（LLM）的"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"情境学习（In-Context Learning, ICL）"},{"Type":"NodeText","Data":" 机制进行了深入的剖析，最终得出了一个颠覆传统认知的结论："},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"情境学习的核心并非模型在推理时进行的真正“学习”，而是一种基于格式匹配、由演示（demonstrations）中的元信息引导的模式模仿过程。"}]},{"ID":"20250919163438-jwu7v9q","Type":"NodeHeading","HeadingLevel":5,"Properties":{"id":"20250919163438-jwu7v9q","updated":"20250919163438"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"颠覆性发现：情境学习的核心并非“学习”"}]},{"ID":"20250919163438-8y3gx1q","Type":"NodeParagraph","Properties":{"id":"20250919163438-8y3gx1q","updated":"20250919163438"},"Children":[{"Type":"NodeText","Data":"论文开篇即提出了一个惊人的核心发现：在提供给模型的演示示例中，"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"输入与标签之间的逻辑对应关系（即输入-标签映射）是否正确，对模型的最终性能影响微乎其微"},{"Type":"NodeText","Data":"。实验（图1, 3, 4）反复证明，使用完全随机、错误的标签构建演示，其效果与使用完全正确的“黄金标签”相比，性能仅有轻微下降，但两者都远远优于不提供任何演示的零样本（zero-shot）情况。"}]},{"ID":"20250919163438-1aqd4vq","Type":"NodeParagraph","Properties":{"id":"20250919163438-1aqd4vq","updated":"20250919163438"},"Children":[{"Type":"NodeText","Data":"这一发现直接挑战了“模型从示例中归纳推理出解题方法”的普遍假设，暗示模型在情境学习中利用的并非示例蕴含的知识或逻辑。"}]},{"ID":"20250919163438-6x90huf","Type":"NodeHeading","HeadingLevel":5,"Properties":{"id":"20250919163438-6x90huf","updated":"20250919163438"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"系统性拆解：什么才是演示中的有效信号？"}]},{"ID":"20250919163438-k6h6zxu","Type":"NodeParagraph","Properties":{"id":"20250919163438-k6h6zxu","updated":"20250919163438"},"Children":[{"Type":"NodeText","Data":"在证明了“什么不重要”之后，论文的核心贡献在于系统性地回答了“那什么才重要？”。作者将一个演示拆解为四个可供独立分析的方面（图7）："}]},{"ID":"20250919163438-lwmao16","Type":"NodeList","ListData":{"Typ":1},"Properties":{"id":"20250919163438-lwmao16","updated":"20250919163438"},"Children":[{"ID":"20250919163437-lzbkpud","Type":"NodeListItem","ListData":{"Typ":1,"Delimiter":46,"Marker":"MS4=","Num":1},"Properties":{"id":"20250919163437-lzbkpud","updated":"20250919163437"},"Children":[{"ID":"20250919163438-2cukmkw","Type":"NodeParagraph","Properties":{"id":"20250919163438-2cukmkw","updated":"20250919163438"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"输入-标签映射 (Input-label Mapping)"},{"Type":"NodeText","Data":": 输入与标签间的逻辑关系。（已被证明不重要）"}]}]},{"ID":"20250919163437-7bu2mgg","Type":"NodeListItem","ListData":{"Typ":1,"Delimiter":46,"Marker":"Mi4=","Num":2},"Properties":{"id":"20250919163437-7bu2mgg","updated":"20250919163437"},"Children":[{"ID":"20250919163438-7h0akzs","Type":"NodeParagraph","Properties":{"id":"20250919163438-7h0akzs","updated":"20250919163438"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"输入分布 (Input Distribution)"},{"Type":"NodeText","Data":": 演示中的输入文本是否与任务文本同属一个分布。"}]}]},{"ID":"20250919163437-twt3tbi","Type":"NodeListItem","ListData":{"Typ":1,"Delimiter":46,"Marker":"My4=","Num":3},"Properties":{"id":"20250919163437-twt3tbi","updated":"20250919163437"},"Children":[{"ID":"20250919163438-ydxrxny","Type":"NodeParagraph","Properties":{"id":"20250919163438-ydxrxny","updated":"20250919163438"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"标签空间 (Label Space)"},{"Type":"NodeText","Data":": 演示中出现的标签集合，即告知模型可能的答案范围。"}]}]},{"ID":"20250919163437-8n1qnv7","Type":"NodeListItem","ListData":{"Typ":1,"Delimiter":46,"Marker":"NC4=","Num":4},"Properties":{"id":"20250919163437-8n1qnv7","updated":"20250919163437"},"Children":[{"ID":"20250919163438-d9ts0y0","Type":"NodeParagraph","Properties":{"id":"20250919163438-d9ts0y0","updated":"20250919163438"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"格式 (Format)"},{"Type":"NodeText","Data":": 演示所呈现的“输入-标签”成对出现的结构。"}]}]}]},{"ID":"20250919163438-uaafvnb","Type":"NodeParagraph","Properties":{"id":"20250919163438-uaafvnb","updated":"20250919163438"},"Children":[{"Type":"NodeText","Data":"通过精巧的控制变量实验，论文逐一揭示了后三者的重要性。实验表明，破坏"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"输入分布"},{"Type":"NodeText","Data":"（图8）或"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"标签空间"},{"Type":"NodeText","Data":"（图9）都会导致性能显著下降，证明了它们是情境学习的有效信号。"}]},{"ID":"20250919163438-lil4phx","Type":"NodeHeading","HeadingLevel":5,"Properties":{"id":"20250919163438-lil4phx","updated":"20250919163438"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"重要性层级：格式是地基，内容是砖瓦"}]},{"ID":"20250919163438-noooexc","Type":"NodeParagraph","Properties":{"id":"20250919163438-noooexc","updated":"20250919163438"},"Children":[{"Type":"NodeText","Data":"本文最深刻的洞见在于揭示了上述有效信号之间的重要性层级。通过设计实验彻底破坏“输入-标签配对”的"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"格式"},{"Type":"NodeText","Data":"（图10），例如只给模型看一连串的输入或一连串的标签，其性能会骤降至甚至不如零样本的水平。"}]},{"ID":"20250919163438-bth2uj5","Type":"NodeParagraph","Properties":{"id":"20250919163438-bth2uj5","updated":"20250919163438"},"Children":[{"Type":"NodeText","Data":"这决定性地证明了："},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"格式是情境学习能够生效的绝对核心和基础"},{"Type":"NodeText","Data":"。它扮演着“粘合剂”或“催化剂”的角色，如果没有格式这个“骨架”，即便提供了有效的输入分布和标签空间（血肉），模型也无法利用它们。格式本身触发了模型进行“模式模仿”的行为，使得其他信息能够被正确地整合和利用。由此，我们可以构建出情境学习有效成分的重要性金字塔："}]},{"ID":"20250919163438-4wgpfku","Type":"NodeList","ListData":{},"Properties":{"id":"20250919163438-4wgpfku","updated":"20250919163438"},"Children":[{"ID":"20250919163437-qk9dujy","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919163437-qk9dujy","updated":"20250919163437"},"Children":[{"ID":"20250919163438-cczlwej","Type":"NodeParagraph","Properties":{"id":"20250919163438-cczlwej","updated":"20250919163438"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"基石（Foundation）"},{"Type":"NodeText","Data":": "},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"格式 (Format)"}]}]},{"ID":"20250919163437-icqk10x","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919163437-icqk10x","updated":"20250919163437"},"Children":[{"ID":"20250919163438-2cmwq9q","Type":"NodeParagraph","Properties":{"id":"20250919163438-2cmwq9q","updated":"20250919163438"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"支柱（Pillars）"},{"Type":"NodeText","Data":": "},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"输入分布 (Input Distribution)"},{"Type":"NodeText","Data":" \u0026 "},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"标签空间 (Label Space)"}]}]},{"ID":"20250919163437-0b3eldh","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250919163437-0b3eldh","updated":"20250919163437"},"Children":[{"ID":"20250919163438-fm02z0m","Type":"NodeParagraph","Properties":{"id":"20250919163438-fm02z0m","updated":"20250919163438"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"几乎无关（Irrelevant）"},{"Type":"NodeText","Data":": "},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"输入-标签映射 (Input-label Mapping)"}]}]}]},{"ID":"20250919163438-nnvsbxb","Type":"NodeHeading","HeadingLevel":5,"Properties":{"id":"20250919163438-nnvsbxb","updated":"20250919163438"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"元训练的启示：学会“走捷径”的专家"}]},{"ID":"20250919163438-cp4ekm8","Type":"NodeParagraph","Properties":{"id":"20250919163438-cp4ekm8","updated":"20250919163438"},"Children":[{"Type":"NodeText","Data":"论文对经过"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"元训练（meta-training）"},{"Type":"NodeText","Data":" 的MetaICL模型的分析，为上述结论提供了完美的佐证。该模型将所有趋势都表现得更为极端：它对格式的依赖性最强，而对输入-标签映射等内容信息的忽略也最彻底。"}]},{"ID":"20250919163438-sdlatyr","Type":"NodeParagraph","Properties":{"id":"20250919163438-sdlatyr","updated":"20250919163438"},"Children":[{"Type":"NodeText","Data":"作者据此推断，元训练的过程可能教会了模型去"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"寻找并利用跨任务最通用、最简单的“捷径”"},{"Type":"NodeText","Data":"。在成千上万的任务中，具体的解题逻辑千变万化且难以学习，而“遵循指令格式”这一元技能却是普适的。因此，元训练强化了模型依赖格式、忽略逻辑的倾向，使其成为一个高效的“格式遵循者”，而非一个更聪明的“逻辑推理者”。"}]},{"ID":"20250919163438-3ftn6ql","Type":"NodeHeading","HeadingLevel":5,"Properties":{"id":"20250919163438-3ftn6ql","updated":"20250919163438"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"结论与深远影响"}]},{"ID":"20250919163438-7cpvfaj","Type":"NodeParagraph","Properties":{"id":"20250919163438-7cpvfaj","updated":"20250919163438"},"Children":[{"Type":"NodeText","Data":"总结而言，这篇论文的研究成果系统性地重塑了我们对大型语言模型“情境学习”能力的认知。它告诉我们，当我们向模型提供少量示例时，我们可能并非在“教会”它知识，而是在用一种结构化的方式“激活”并“引导”其庞大预训练知识库中早已存在的能力。这一洞见对于如何设计更有效的提示（prompt engineering）、理解模型的局限性以及未来模型训练方法的发展方向，都具有极其重要的指导意义。"}]}]}]}