{"ID":"20250808114316-m313co9","Spec":"1","Type":"NodeDocument","Properties":{"id":"20250808114316-m313co9","title":"卷积序列到序列学习","type":"doc","updated":"20250809090141"},"Children":[{"ID":"20250808114316-vkov64r","Type":"NodeHeading","HeadingLevel":1,"Properties":{"id":"20250808114316-vkov64r","updated":"20250809090141"},"Children":[{"Type":"NodeText","Data":"摘要"}]},{"ID":"20250808115306-ou4a9io","Type":"NodeParagraph","Properties":{"id":"20250808115306-ou4a9io","updated":"20250808115534"},"Children":[{"Type":"NodeText","Data":"目前流行的序列到序列学习方法是通过循环神经网络将输入序列映射到可变长度的输出序列。我们引入了一种完全基于卷积神经网络的架构。与循环模型相比，在训练过程中，所有元素的计算可以完全并行化，从而更好地利用GPU硬件；同时，由于非线性单元的数量是固定的，且与输入长度无关，因此优化也更容易。我们使用了门控线性单元（gated linear units）来简化梯度传播，并为每个解码器层配备了独立的注意力模块。在WMT’14英德和WMT’14英法翻译任务上，我们的模型在准确性上超越了Wu等人（2016）的深度LSTM模型，并且在GPU和CPU上的运行速度都快了一个数量级。"}]},{"ID":"20250808115546-ecx0tz9","Type":"NodeBlockquote","Properties":{"id":"20250808115546-ecx0tz9","updated":"20250808115548"},"Children":[{"Type":"NodeBlockquoteMarker","Data":"\u003e"},{"ID":"20250808115547-kn8taeq","Type":"NodeParagraph","Properties":{"id":"20250808115547-kn8taeq","updated":"20250808115548"},"Children":[{"Type":"NodeText","Data":"摘要介绍了一种用于序列到序列（Seq2Seq）任务的、完全基于卷积神经网络（CNN）的新架构，并将其与当时主流的循环神经网络（RNN）方法进行了对比。"}]}]},{"ID":"20250808115720-y3fxli1","Type":"NodeBlockquote","Properties":{"fold":"1","id":"20250808115720-y3fxli1","updated":"20250808115732"},"Children":[{"Type":"NodeBlockquoteMarker","Data":"\u003e"},{"ID":"20250808115728-qqz71en","Type":"NodeList","ListData":{"Typ":1},"Properties":{"id":"20250808115728-qqz71en","updated":"20250808115732"},"Children":[{"ID":"20250808115732-xt8j8jz","Type":"NodeListItem","ListData":{"Typ":1,"Delimiter":46,"Marker":"MS4=","Num":1},"Properties":{"id":"20250808115732-xt8j8jz","updated":"20250808115732"},"Children":[{"ID":"20250808115732-7juguyl","Type":"NodeParagraph","Properties":{"id":"20250808115732-7juguyl","updated":"20250808115732"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"背景：主流的Seq2Seq学习方法"}]}]}]},{"ID":"20250808115728-zyzn9l0","Type":"NodeList","ListData":{},"Properties":{"id":"20250808115728-zyzn9l0","updated":"20250808115728"},"Children":[{"ID":"20250808115728-0jciaty","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808115728-0jciaty","updated":"20250808115728"},"Children":[{"ID":"20250808115728-jyjrdxu","Type":"NodeParagraph","Properties":{"id":"20250808115728-jyjrdxu","updated":"20250808115728"},"Children":[{"Type":"NodeText","Data":"摘要首先指出，当时处理序列到序列问题的普遍方法是使用循环神经网络（RNN），特别是像长短期记忆网络（LSTM）这样的变体。 这种模型通过一个编码器（Encoder）将输入序列（如一句德语）压缩成一个固定大小的向量，然后由一个解码器（Decoder）将这个向量解码为输出序列（如一句英语）。"}]}]},{"ID":"20250808115728-3785z5w","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808115728-3785z5w","updated":"20250808115728"},"Children":[{"ID":"20250808115728-yph9ezt","Type":"NodeParagraph","Properties":{"id":"20250808115728-yph9ezt","updated":"20250808115728"},"Children":[{"Type":"NodeText","Data":"这种方法的显著缺点是其顺序处理数据的特性，即必须处理完前一个数据点才能处理后一个，这使得它难以在GPU上进行并行计算，从而导致训练速度较慢。"}]}]}]}]},{"ID":"20250808115749-0id43nb","Type":"NodeBlockquote","Properties":{"fold":"1","id":"20250808115749-0id43nb","updated":"20250808115803"},"Children":[{"Type":"NodeBlockquoteMarker","Data":"\u003e"},{"ID":"20250808115800-e4m4u0e","Type":"NodeList","ListData":{"Typ":1},"Properties":{"id":"20250808115800-e4m4u0e","updated":"20250808115803"},"Children":[{"ID":"20250808115803-b7kfr3x","Type":"NodeListItem","ListData":{"Typ":1,"Delimiter":46,"Marker":"Mi4=","Num":2},"Properties":{"id":"20250808115803-b7kfr3x","updated":"20250808115803"},"Children":[{"ID":"20250808115803-x6y8cv2","Type":"NodeParagraph","Properties":{"id":"20250808115803-x6y8cv2","updated":"20250808115803"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"提出的新架构：基于卷积神经网络（CNN）"}]}]}]},{"ID":"20250808115800-osgjhio","Type":"NodeList","ListData":{},"Properties":{"id":"20250808115800-osgjhio","updated":"20250808115800"},"Children":[{"ID":"20250808115800-7gpzke7","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808115800-7gpzke7","updated":"20250808115800"},"Children":[{"ID":"20250808115800-hhiha1s","Type":"NodeParagraph","Properties":{"id":"20250808115800-hhiha1s","updated":"20250808115800"},"Children":[{"Type":"NodeText","Data":"作者提出了一种完全基于卷积神经网络的新架构来替代RNN。 CNN通常用于计算机视觉领域，其核心是使用卷积核（滤波器）来捕捉局部特征。 在这项工作中，CNN被应用于序列数据，通过层级结构来捕捉序列中的依赖关系。"}]}]}]}]},{"ID":"20250808115757-j2wr7ul","Type":"NodeBlockquote","Properties":{"fold":"1","id":"20250808115757-j2wr7ul","updated":"20250808115822"},"Children":[{"Type":"NodeBlockquoteMarker","Data":"\u003e"},{"ID":"20250808115813-rb1eqe4","Type":"NodeList","ListData":{"Typ":1},"Properties":{"id":"20250808115813-rb1eqe4","updated":"20250808115822"},"Children":[{"ID":"20250808115822-78bn4t7","Type":"NodeListItem","ListData":{"Typ":1,"Delimiter":46,"Marker":"My4=","Num":3},"Properties":{"id":"20250808115822-78bn4t7","updated":"20250808115822"},"Children":[{"ID":"20250808115822-xbn6rm3","Type":"NodeParagraph","Properties":{"id":"20250808115822-xbn6rm3","updated":"20250808115822"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"CNN架构的优势"}]}]}]},{"ID":"20250808115813-nas2cyb","Type":"NodeList","ListData":{},"Properties":{"id":"20250808115813-nas2cyb","updated":"20250808115813"},"Children":[{"ID":"20250808115813-6rnqowd","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808115813-6rnqowd","updated":"20250808115813"},"Children":[{"ID":"20250808115813-oqt8jm7","Type":"NodeParagraph","Properties":{"id":"20250808115813-oqt8jm7","updated":"20250808115813"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"并行计算"},{"Type":"NodeText","Data":"：与RNN必须按顺序处理数据不同，CNN可以同时处理序列中的所有元素。这极大地提高了计算效率，尤其能充分利用GPU的并行处理能力，从而在训练速度上获得巨大提升。"}]}]},{"ID":"20250808115813-ns8pu09","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808115813-ns8pu09","updated":"20250808115813"},"Children":[{"ID":"20250808115813-y0peq2m","Type":"NodeParagraph","Properties":{"id":"20250808115813-y0peq2m","updated":"20250808115813"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"优化更容易"},{"Type":"NodeText","Data":"：在RNN中，计算路径的长度和非线性操作的数量取决于输入序列的长度，这可能导致梯度消失或梯度爆炸等问题。 而在CNN架构中，非线性激活函数的数量是固定的，不随序列长度变化，这使得网络结构更稳定，优化过程也更为简单。"}]}]}]}]},{"ID":"20250808115806-g8pjug7","Type":"NodeBlockquote","Properties":{"fold":"1","id":"20250808115806-g8pjug7","updated":"20250808115835"},"Children":[{"Type":"NodeBlockquoteMarker","Data":"\u003e"},{"ID":"20250808115832-aeme18r","Type":"NodeList","ListData":{"Typ":1},"Properties":{"id":"20250808115832-aeme18r","updated":"20250808115835"},"Children":[{"ID":"20250808115835-7tzyq0p","Type":"NodeListItem","ListData":{"Typ":1,"Delimiter":46,"Marker":"NC4=","Num":4},"Properties":{"id":"20250808115835-7tzyq0p","updated":"20250808115835"},"Children":[{"ID":"20250808115835-ap1gw4d","Type":"NodeParagraph","Properties":{"id":"20250808115835-ap1gw4d","updated":"20250808115835"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"关键技术组件"}]}]}]},{"ID":"20250808115832-mrpip0b","Type":"NodeList","ListData":{},"Properties":{"id":"20250808115832-mrpip0b","updated":"20250808115832"},"Children":[{"ID":"20250808115832-n159z14","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808115832-n159z14","updated":"20250808115832"},"Children":[{"ID":"20250808115832-hhd147x","Type":"NodeParagraph","Properties":{"id":"20250808115832-hhd147x","updated":"20250808115832"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"门控线性单元 (Gated Linear Units, GLU)"},{"Type":"NodeText","Data":"：这是一种激活函数，它通过一个“门控”机制来控制信息的流动。 摘要中提到，使用GLU可以缓解梯度消失问题，因为其结构中包含一条线性的路径，有助于梯度在网络中更顺畅地传播，从而使训练更稳定和快速。"}]}]},{"ID":"20250808115832-4pfgncg","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808115832-4pfgncg","updated":"20250808115832"},"Children":[{"ID":"20250808115832-frsiiil","Type":"NodeParagraph","Properties":{"id":"20250808115832-frsiiil","updated":"20250808115832"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"独立的注意力模块 (Separate Attention Module)"},{"Type":"NodeText","Data":"：在解码器的每一层都配备了独立的注意力模块。注意力机制允许解码器在生成输出的每一步，都能关注到输入序列中不同部分的信息。 在每一层都设置注意力模块，可以帮助模型在不同抽象层次上更好地捕捉输入和输出之间的对齐关系。"}]}]}]}]},{"ID":"20250808115818-ifxluq2","Type":"NodeBlockquote","Properties":{"fold":"1","id":"20250808115818-ifxluq2","updated":"20250808115847"},"Children":[{"Type":"NodeBlockquoteMarker","Data":"\u003e"},{"ID":"20250808115844-evqicos","Type":"NodeList","ListData":{"Typ":1},"Properties":{"id":"20250808115844-evqicos","updated":"20250808115847"},"Children":[{"ID":"20250808115847-d8xtjy7","Type":"NodeListItem","ListData":{"Typ":1,"Delimiter":46,"Marker":"NS4=","Num":5},"Properties":{"id":"20250808115847-d8xtjy7","updated":"20250808115847"},"Children":[{"ID":"20250808115847-57tg8nm","Type":"NodeParagraph","Properties":{"id":"20250808115847-57tg8nm","updated":"20250808115847"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"实验结果与性能"}]}]}]},{"ID":"20250808115844-1r3409o","Type":"NodeList","ListData":{},"Properties":{"id":"20250808115844-1r3409o","updated":"20250808115844"},"Children":[{"ID":"20250808115844-vfgss0t","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808115844-vfgss0t","updated":"20250808115844"},"Children":[{"ID":"20250808115844-odjgxcf","Type":"NodeParagraph","Properties":{"id":"20250808115844-odjgxcf","updated":"20250808115844"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"准确性"},{"Type":"NodeText","Data":"：该模型在两个公开的机器翻译基准数据集——WMT'14英德翻译和WMT'14英法翻译上，其准确性（通常用BLEU分数衡量）超越了当时由Wu等人（2016）提出的深度LSTM模型。"}]}]},{"ID":"20250808115844-82r33j6","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808115844-82r33j6","updated":"20250808115844"},"Children":[{"ID":"20250808115844-yj2gyzb","Type":"NodeParagraph","Properties":{"id":"20250808115844-yj2gyzb","updated":"20250808115844"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"速度"},{"Type":"NodeText","Data":"：无论是在GPU还是CPU上，该模型的训练速度都比基于RNN的模型快一个数量级，这充分体现了其并行计算的优势。"}]}]}]}]},{"ID":"20250808120002-dkjtdmm","Type":"NodeHeading","HeadingLevel":1,"Properties":{"id":"20250808120002-dkjtdmm","updated":"20250809090141"},"Children":[{"Type":"NodeText","Data":"引言"}]},{"ID":"20250808120007-t5m3fol","Type":"NodeParagraph","Properties":{"id":"20250808120007-t5m3fol","updated":"20250808120008"},"Children":[{"Type":"NodeText","Data":"序列到序列学习在许多任务中都取得了成功，例如机器翻译（Sutskever 等人，2014；Chorowski 等人，2015）、语音识别（Sutskever 等人，2014；Chorowski 等人，2015）和文本摘要（Rush 等人，2015；Nallapati 等人，2016；Shen 等人，2016）等。迄今为止，主流的方法是使用一系列双向循环神经网络（RNN）来编码输入序列，并用另一组解码器RNN生成一个可变长度的输出，这两部分通过一个软注意力机制（soft-attention mechanism）进行连接（Bahdanau 等人，2014；Luong 等人，2015）。在机器翻译领域，这种架构已被证明以巨大优势超越了传统的基于短语的模型（Sennrich 等人，2016b；Zhou 等人，2016；Wu 等人，2016；§2）。"}]},{"ID":"20250808120018-9xzt5nk","Type":"NodeBlockquote","Properties":{"id":"20250808120018-9xzt5nk","updated":"20250808120021"},"Children":[{"Type":"NodeBlockquoteMarker","Data":"\u003e"},{"ID":"20250808120020-l93mhlo","Type":"NodeParagraph","Properties":{"id":"20250808120020-l93mhlo","updated":"20250808120021"},"Children":[{"Type":"NodeText","Data":"引言部分介绍了“序列到序列”（Sequence to Sequence, Seq2Seq）学习的背景、主流实现方式及其在当时取得的成就。"}]}]},{"ID":"20250808120105-jwtq850","Type":"NodeBlockquote","Properties":{"fold":"1","id":"20250808120105-jwtq850","updated":"20250808120116"},"Children":[{"Type":"NodeBlockquoteMarker","Data":"\u003e"},{"ID":"20250808120107-w4qea0m","Type":"NodeList","ListData":{"Typ":1},"Properties":{"id":"20250808120107-w4qea0m","updated":"20250808120116"},"Children":[{"ID":"20250808120116-0nnc69m","Type":"NodeListItem","ListData":{"Typ":1,"Delimiter":46,"Marker":"Mi4=","Num":2},"Properties":{"id":"20250808120116-0nnc69m","updated":"20250808120116"},"Children":[{"ID":"20250808120116-f8tqfuu","Type":"NodeParagraph","Properties":{"id":"20250808120116-f8tqfuu","updated":"20250808120116"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"当时主流的Seq2Seq架构"},{"Type":"NodeText","Data":"："}]}]}]},{"ID":"20250808120107-m7bhd8z","Type":"NodeList","ListData":{},"Properties":{"id":"20250808120107-m7bhd8z","updated":"20250808120113"},"Children":[{"ID":"20250808120107-5v2rdkk","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808120107-5v2rdkk","updated":"20250808120107"},"Children":[{"ID":"20250808120107-vho0wrr","Type":"NodeParagraph","Properties":{"id":"20250808120107-vho0wrr","updated":"20250808120107"},"Children":[{"Type":"NodeText","Data":"文章接着描述了当时最主流和最强大的Seq2Seq实现方案，其核心组件包括："}]},{"ID":"20250808120107-19j0dhb","Type":"NodeList","ListData":{},"Properties":{"id":"20250808120107-19j0dhb","updated":"20250808120107"},"Children":[{"ID":"20250808120107-b2re7uu","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808120107-b2re7uu","updated":"20250808120107"},"Children":[{"ID":"20250808120107-ib51lc0","Type":"NodeParagraph","Properties":{"id":"20250808120107-ib51lc0","updated":"20250808120107"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"编码器 (Encoder)"},{"Type":"NodeText","Data":"：使用"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"双向循环神经网络 (bi-directional RNN)"},{"Type":"NodeText","Data":"。双向RNN能够同时从前向和后向两个方向上读取整个输入序列，从而让每个时间点的编码表示都能包含完整的上下文信息，而不仅仅是过去的信息。"}]}]},{"ID":"20250808120107-3b1r8pq","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808120107-3b1r8pq","updated":"20250808120107"},"Children":[{"ID":"20250808120107-4ps7tu3","Type":"NodeParagraph","Properties":{"id":"20250808120107-4ps7tu3","updated":"20250808120107"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"解码器 (Decoder)"},{"Type":"NodeText","Data":"：使用另一组"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"循环神经网络 (RNN)"},{"Type":"NodeText","Data":" 来生成输出序列。"}]}]},{"ID":"20250808120107-j6vp3jk","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808120107-j6vp3jk","updated":"20250808120107"},"Children":[{"ID":"20250808120107-uzps29u","Type":"NodeParagraph","Properties":{"id":"20250808120107-uzps29u","updated":"20250808120107"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"软注意力机制 (Soft-Attention Mechanism)"},{"Type":"NodeText","Data":"：这是连接编码器和解码器的关键。它允许解码器在生成输出的每一步时，能够“关注”输入序列的不同部分，并为更相关的部分分配更高的“权重”。这解决了早期Seq2Seq模型中将整个输入序列压缩成单一固定长度向量时造成的信息瓶颈问题。"}]}]}]}]}]}]},{"ID":"20250808125251-0n50ypl","Type":"NodeBlockquote","Properties":{"fold":"1","id":"20250808125251-0n50ypl","updated":"20250808125257"},"Children":[{"Type":"NodeBlockquoteMarker","Data":"\u003e"},{"ID":"20250808125257-hcp9del","Type":"NodeList","ListData":{"Typ":1},"Properties":{"id":"20250808125257-hcp9del","updated":"20250808125257"},"Children":[{"ID":"20250808125257-6t2gmz8","Type":"NodeListItem","ListData":{"Typ":1,"Delimiter":46,"Marker":"My4=","Num":3},"Properties":{"id":"20250808125257-6t2gmz8","updated":"20250808125257"},"Children":[{"ID":"20250808125257-reykbby","Type":"NodeParagraph","Properties":{"id":"20250808125257-reykbby","updated":"20250808125257"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"架构的性能与地位"},{"Type":"NodeText","Data":"："}]}]}]},{"ID":"20250808125257-n713eyl","Type":"NodeList","ListData":{},"Properties":{"id":"20250808125257-n713eyl","updated":"20250808125257"},"Children":[{"ID":"20250808125257-wdpqmt4","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808125257-wdpqmt4","updated":"20250808125257"},"Children":[{"ID":"20250808125257-rdxl2sy","Type":"NodeParagraph","Properties":{"id":"20250808125257-rdxl2sy","updated":"20250808125257"},"Children":[{"Type":"NodeText","Data":"该架构在"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"机器翻译"},{"Type":"NodeText","Data":"领域取得了突破性进展。"}]}]},{"ID":"20250808125257-l7ak2j6","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808125257-l7ak2j6","updated":"20250808125257"},"Children":[{"ID":"20250808125257-9n1qetb","Type":"NodeParagraph","Properties":{"id":"20250808125257-9n1qetb","updated":"20250808125257"},"Children":[{"Type":"NodeText","Data":"其性能"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"以巨大优势 (by large margins)"},{"Type":"NodeText","Data":" 超越了传统的"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"基于短语的统计机器翻译 (phrase-based models)"},{"Type":"NodeText","Data":" 方法。这标志着神经网络机器翻译（NMT）时代的到来，并使其成为该领域的主导范式。"}]}]},{"ID":"20250808125257-9n7jzhh","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808125257-9n7jzhh","updated":"20250808125257"},"Children":[{"ID":"20250808125257-bbngevg","Type":"NodeParagraph","Properties":{"id":"20250808125257-bbngevg","updated":"20250808125257"},"Children":[{"Type":"NodeText","Data":"引用的文献（如Sennrich、Zhou、Wu等人的工作）都是当时神经网络机器翻译领域的权威研究，证明了这一架构的先进性和有效性。"}]}]}]}]},{"ID":"20250808130445-uxode5j","Type":"NodeParagraph","Properties":{"id":"20250808130445-uxode5j","updated":"20250808130445"},"Children":[{"Type":"NodeText","Data":"尽管卷积神经网络在序列建模方面有一些优势（Waibel 等人，1989；LeCun \u0026 Bengio，1995），但它并不那么常见。与循环层相比，卷积虽然是为固定大小的上下文创建表示，但通过将多个卷积层堆叠在一起，可以很容易地扩大网络的有效上下文大小。这使得我们能够精确地控制需要建模的依赖关系的最大长度。卷积网络的计算不依赖于前一个时间步的计算，因此允许对序列中的每个元素进行并行化处理。这与RNNs形成了对比，RNNs需要维持一个包含整个过去信息的隐藏状态，从而阻碍了序列内的并行计算。"}]},{"ID":"20250808130445-0ga4cue","Type":"NodeParagraph","Properties":{"id":"20250808130445-0ga4cue","updated":"20250808135536"},"Children":[{"Type":"NodeText","Data":"多层卷积神经网络在输入序列上创建了分层的表示，其中邻近的元素在较低的层进行交互，而距离较远的元素在较高的层进行交互。与循环网络所建模的链式结构相比，这种分层结构为捕捉长距离依赖关系提供了更短的路径。例如，对于宽度为k的卷积核，我们只需应用"},{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":"O(\\frac{n}{k})"},{"Type":"NodeText","Data":"次卷积操作就能获得一个捕捉n个词窗口内关系的特征表示，而循环神经网络则需要线性的"},{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":"O(n)"},{"Type":"NodeText","Data":"次操作。输入到卷积网络的数据会通过固定数量的卷积核和非线性单元，而循环网络对第一个词最多应用"},{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":"n"},{"Type":"NodeText","Data":"次操作和非线性变换，对最后一个词则只应用一组操作。固定应用于输入的非线性单元的数量也使得学习过程变得更加容易。"}]},{"ID":"20250808130459-tcvofvb","Type":"NodeBlockquote","Properties":{"id":"20250808130459-tcvofvb","updated":"20250808130501"},"Children":[{"Type":"NodeBlockquoteMarker","Data":"\u003e"},{"ID":"20250808130501-yhh3any","Type":"NodeParagraph","Properties":{"id":"20250808130501-yhh3any","updated":"20250808130501"},"Children":[{"Type":"NodeText","Data":"这段文字详细阐述了为什么卷积神经网络（CNN）适合用于序列建模任务，并将其与更传统的循环神经网络（RNN）进行了深入对比，突出了CNN的几大核心优势。"}]}]},{"ID":"20250808130503-h9dkall","Type":"NodeBlockquote","Properties":{"fold":"1","id":"20250808130503-h9dkall","updated":"20250808132705"},"Children":[{"Type":"NodeBlockquoteMarker","Data":"\u003e"},{"ID":"20250808132705-1teevsj","Type":"NodeList","ListData":{"Typ":1},"Properties":{"id":"20250808132705-1teevsj","updated":"20250808132705"},"Children":[{"ID":"20250808132705-o08bx3p","Type":"NodeListItem","ListData":{"Typ":1,"Delimiter":46,"Marker":"MS4=","Num":1},"Properties":{"id":"20250808132705-o08bx3p","updated":"20250808132705"},"Children":[{"ID":"20250808132705-nl7ydex","Type":"NodeParagraph","Properties":{"id":"20250808132705-nl7ydex","updated":"20250808132705"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"并行计算能力 (Parallelization)"}]},{"ID":"20250808132705-hy22pmx","Type":"NodeList","ListData":{},"Properties":{"id":"20250808132705-hy22pmx","updated":"20250808132705"},"Children":[{"ID":"20250808132705-slv5dez","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808132705-slv5dez","updated":"20250808132705"},"Children":[{"ID":"20250808132705-2ow65nm","Type":"NodeParagraph","Properties":{"id":"20250808132705-2ow65nm","updated":"20250808132705"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"CNN"},{"Type":"NodeText","Data":": CNN的核心优势在于其计算不依赖于前一个时间步的结果。卷积操作可以在整个序列上同时进行，这意味着所有时间步的计算可以完全并行化。这极大地提高了计算效率，尤其能充分利用GPU的并行处理能力。"}]}]},{"ID":"20250808132705-gmv2nup","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808132705-gmv2nup","updated":"20250808132705"},"Children":[{"ID":"20250808132705-282y698","Type":"NodeParagraph","Properties":{"id":"20250808132705-282y698","updated":"20250808132705"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"RNN"},{"Type":"NodeText","Data":": RNN的本质是顺序的。要计算当前时间步 "},{"Type":"NodeTextMark","TextMarkType":"code","TextMarkTextContent":"t"},{"Type":"NodeText","Data":"​ 的隐藏状态，必须先完成时间步 "},{"Type":"NodeTextMark","TextMarkType":"code","TextMarkTextContent":"t-1"},{"Type":"NodeText","Data":"​ 的计算。这种对“整个过去历史”的依赖性（即隐藏状态的链式传递）从根本上阻碍了并行计算，导致其处理长序列时速度较慢。"}]}]}]}]}]}]},{"ID":"20250808132708-cacvkgy","Type":"NodeBlockquote","Properties":{"fold":"1","id":"20250808132708-cacvkgy","updated":"20250808135414"},"Children":[{"Type":"NodeBlockquoteMarker","Data":"\u003e"},{"ID":"20250808132734-rcgkwww","Type":"NodeList","ListData":{"Typ":1},"Properties":{"id":"20250808132734-rcgkwww","updated":"20250808135414"},"Children":[{"ID":"20250808132734-bu28sd8","Type":"NodeListItem","ListData":{"Typ":1,"Delimiter":46,"Marker":"Mi4=","Num":2},"Properties":{"id":"20250808132734-bu28sd8","updated":"20250808135414"},"Children":[{"ID":"20250808132734-zst7qsc","Type":"NodeParagraph","Properties":{"id":"20250808132734-zst7qsc","updated":"20250808132734"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"长距离依赖的捕捉方式 (Long-range Dependencies)"}]},{"ID":"20250808132734-drqnm40","Type":"NodeList","ListData":{},"Properties":{"id":"20250808132734-drqnm40","updated":"20250808135414"},"Children":[{"ID":"20250808132734-052qkr1","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808132734-052qkr1","updated":"20250808135414"},"Children":[{"ID":"20250808132734-hbwgmu0","Type":"NodeParagraph","Properties":{"id":"20250808132734-hbwgmu0","updated":"20250808132734"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"CNN"},{"Type":"NodeText","Data":": CNN通过"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"堆叠卷积层"},{"Type":"NodeText","Data":"来构建"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"分层表示 (hierarchical representations)"},{"Type":"NodeText","Data":"，从而捕捉长距离依赖。"}]},{"ID":"20250808132734-s7cnenp","Type":"NodeList","ListData":{},"Properties":{"id":"20250808132734-s7cnenp","updated":"20250808135414"},"Children":[{"ID":"20250808132734-q12walp","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808132734-q12walp","updated":"20250808132734"},"Children":[{"ID":"20250808132734-vkcyq5k","Type":"NodeParagraph","Properties":{"id":"20250808132734-vkcyq5k","updated":"20250808132734"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"分层结构"},{"Type":"NodeText","Data":": 在底层卷积层，网络主要学习相邻词之间的局部关系（例如，2-3个词的短语）。随着层数加深，更高层的卷积核操作在前一层输出的特征图上，其“感受野”（receptive field）或“有效上下文大小”（effective context size）会随之增大，从而能捕捉到距离更远的词之间的关系。"}]}]},{"ID":"20250808132734-1w7n0t9","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808132734-1w7n0t9","updated":"20250808135414"},"Children":[{"ID":"20250808132734-xi43p0d","Type":"NodeParagraph","Properties":{"id":"20250808132734-xi43p0d","updated":"20250808135414"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"更短的路径"},{"Type":"NodeText","Data":": 从输入到捕捉到长距离依赖，信息在CNN中需要经过的路径长度是 "},{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":"O(log_kn)"},{"Type":"NodeText","Data":"（其中k是卷积核大小，n是序列长度），路径相对较短。相比之下，在RNN中，第一个词的信息要传递到最后一个词，需要经过 O(n) 步的顺序操作，路径很长，容易导致信息丢失（即梯度消失）。"}]}]}]}]},{"ID":"20250808132734-ee3qxt2","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808132734-ee3qxt2","updated":"20250808132734"},"Children":[{"ID":"20250808132734-g0sc3po","Type":"NodeParagraph","Properties":{"id":"20250808132734-g0sc3po","updated":"20250808132734"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"RNN"},{"Type":"NodeText","Data":": RNN通过隐藏状态的不断更新来传递和记忆长距离信息，形成一个链式结构。如上所述，这种长路径使得梯度传播困难，长距离依赖的捕捉效果往往不佳。"}]}]}]}]}]}]},{"ID":"20250808132745-98pfie1","Type":"NodeBlockquote","Properties":{"fold":"1","id":"20250808132745-98pfie1","updated":"20250808135516"},"Children":[{"Type":"NodeBlockquoteMarker","Data":"\u003e"},{"ID":"20250808132747-uducmdv","Type":"NodeList","ListData":{"Typ":1},"Properties":{"id":"20250808132747-uducmdv","updated":"20250808135516"},"Children":[{"ID":"20250808132747-gy2qw7q","Type":"NodeListItem","ListData":{"Typ":1,"Delimiter":46,"Marker":"My4=","Num":3},"Properties":{"id":"20250808132747-gy2qw7q","updated":"20250808135516"},"Children":[{"ID":"20250808132747-jvqhp4o","Type":"NodeParagraph","Properties":{"id":"20250808132747-jvqhp4o","updated":"20250808132747"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"计算复杂度和优化的稳定性"}]},{"ID":"20250808132747-cjaboid","Type":"NodeList","ListData":{},"Properties":{"id":"20250808132747-cjaboid","updated":"20250808135516"},"Children":[{"ID":"20250808132747-sq32ie8","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808132747-sq32ie8","updated":"20250808135516"},"Children":[{"ID":"20250808132747-oeh7har","Type":"NodeParagraph","Properties":{"id":"20250808132747-oeh7har","updated":"20250808132747"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"CNN"},{"Type":"NodeText","Data":":"}]},{"ID":"20250808132747-h83l47c","Type":"NodeList","ListData":{},"Properties":{"id":"20250808132747-h83l47c","updated":"20250808135516"},"Children":[{"ID":"20250808132747-hhl1ibt","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808132747-hhl1ibt","updated":"20250808135516"},"Children":[{"ID":"20250808132747-grpc7a8","Type":"NodeParagraph","Properties":{"id":"20250808132747-grpc7a8","updated":"20250808135516"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"计算量"},{"Type":"NodeText","Data":": 文中提到，为了捕捉一个大小为 "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"n"},{"Type":"NodeText","Data":"​ 的窗口内的关系，CNN（使用宽度为 "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"k"},{"Type":"NodeText","Data":"​ 的卷积核）只需要 "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"O(n/k)"},{"Type":"NodeText","Data":"​ 次操作。"}]}]},{"ID":"20250808132747-5f2axkz","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808132747-5f2axkz","updated":"20250808132747"},"Children":[{"ID":"20250808132747-z5wfgz4","Type":"NodeParagraph","Properties":{"id":"20250808132747-z5wfgz4","updated":"20250808132747"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"固定的非线性"},{"Type":"NodeText","Data":": 对于任何输入序列，无论其长短，数据都流经一个"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"固定深度"},{"Type":"NodeText","Data":"的CNN网络，即经历的非线性激活函数的数量是恒定的。这使得网络结构更加稳定，梯度传播的路径长度固定，从而缓解了梯度消失/爆炸问题，让学习过程（优化）变得更加容易。"}]}]}]}]},{"ID":"20250808132747-k665x2a","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808132747-k665x2a","updated":"20250808135502"},"Children":[{"ID":"20250808132747-hb31929","Type":"NodeParagraph","Properties":{"id":"20250808132747-hb31929","updated":"20250808132747"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"RNN"},{"Type":"NodeText","Data":":"}]},{"ID":"20250808132747-ifcxy6o","Type":"NodeList","ListData":{},"Properties":{"id":"20250808132747-ifcxy6o","updated":"20250808135502"},"Children":[{"ID":"20250808132747-khe62nr","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808132747-khe62nr","updated":"20250808135500"},"Children":[{"ID":"20250808132747-qkftdz3","Type":"NodeParagraph","Properties":{"id":"20250808132747-qkftdz3","updated":"20250808135500"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"计算量"},{"Type":"NodeText","Data":": RNN需要 "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"O(n)"},{"Type":"NodeText","Data":"​ 次操作来处理长度为 "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"n"},{"Type":"NodeText","Data":"​ 的序列。"}]}]},{"ID":"20250808132747-jmwks6j","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808132747-jmwks6j","updated":"20250808135502"},"Children":[{"ID":"20250808132747-bvewqrg","Type":"NodeParagraph","Properties":{"id":"20250808132747-bvewqrg","updated":"20250808135502"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"可变的非线性"},{"Type":"NodeText","Data":": RNN处理序列时，操作和非线性变换的数量与序列的长度 "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"n"},{"Type":"NodeText","Data":"​ 成正比。一个长序列意味着一个非常“深”的计算图（按时间步展开后），这使得优化变得困难。"}]}]}]}]}]}]}]}]},{"ID":"20250808133508-urwvvjz","Type":"NodeParagraph","Properties":{"id":"20250808133508-urwvvjz","updated":"20250808133508"},"Children":[{"Type":"NodeText","Data":"最近的研究已将卷积神经网络应用于序列建模，例如Bradbury等人（2016）在一系列连续的卷积层之间引入了循环池化（recurrent pooling），或者Kalchbrenner等人（2016）在没有注意力机制的情况下处理神经机器翻译。然而，这些方法都未能在大型基准数据集上展示出超越当前最先进（state of the art）水平的结果。"}]},{"ID":"20250808133508-927q1cg","Type":"NodeParagraph","Properties":{"id":"20250808133508-927q1cg","updated":"20250808133508"},"Children":[{"Type":"NodeText","Data":"Meng等人（2015）曾探索过将门控卷积（Gated convolutions）用于机器翻译，但他们的评估仅限于一个小数据集，并且该模型是与一个传统的基于计数的模型协同使用的。部分采用卷积的架构在较大型任务上表现出强劲性能，但它们的解码器仍然是循环式的（Gehring 等人，2016）。"}]},{"ID":"20250808133508-rabzvum","Type":"NodeParagraph","Properties":{"id":"20250808133508-rabzvum","updated":"20250808133508"},"Children":[{"Type":"NodeText","Data":"在本文中，我们提出了一种完全基于卷积的序列到序列建模架构。我们的模型配备了门控线性单元（Gated Linear Units, GLU）（Dauphin 等人，2016）和残差连接（residual connections）（He 等人，2015a）。我们还在每个解码器层都使用了注意力机制，并证明了每个注意力层只增加了可忽略不计的开销。这些选择的结合使我们能够处理大规模问题（§3）。"}]},{"ID":"20250808133508-hmz17hn","Type":"NodeParagraph","Properties":{"id":"20250808133508-hmz17hn","updated":"20250808133508"},"Children":[{"Type":"NodeText","Data":"我们在几个大型机器翻译和摘要数据集上评估了我们的方法，并与文献中报道的当前最佳架构进行了比较。在WMT’16英-罗翻译任务上，我们取得了新的业界最佳成绩，比之前的最好结果高出1.9 BLEU。在WMT’14英-德翻译任务上，我们比Wu等人（2016）强大的LSTM模型高出0.5 BLEU；在WMT’14英-法翻译任务上，我们比Wu等人（2016）经过似然训练的系统高出1.6 BLEU。此外，我们的模型在GPU和CPU硬件上翻译未见过句子的速度比Wu等人（2016）的模型快一个数量级（§4, §5）。"}]},{"ID":"20250808133515-2feel7y","Type":"NodeBlockquote","Properties":{"id":"20250808133515-2feel7y","updated":"20250808133531"},"Children":[{"Type":"NodeBlockquoteMarker","Data":"\u003e"},{"ID":"20250808133530-qj2jupe","Type":"NodeParagraph","Properties":{"id":"20250808133530-qj2jupe","updated":"20250808133531"},"Children":[{"Type":"NodeText","Data":"这段文字是论文引言的后半部分，主要论述了相关工作的不足，引出了本文提出的新架构及其核心贡献和优越的实验结果。"}]}]},{"ID":"20250808133540-mwxv3l3","Type":"NodeBlockquote","Properties":{"fold":"1","id":"20250808133540-mwxv3l3","updated":"20250808133542"},"Children":[{"Type":"NodeBlockquoteMarker","Data":"\u003e"},{"ID":"20250808133542-2h8stri","Type":"NodeList","ListData":{"Typ":1},"Properties":{"id":"20250808133542-2h8stri","updated":"20250808133542"},"Children":[{"ID":"20250808133542-ewn5rvh","Type":"NodeListItem","ListData":{"Typ":1,"Delimiter":46,"Marker":"MS4=","Num":1},"Properties":{"id":"20250808133542-ewn5rvh","updated":"20250808133542"},"Children":[{"ID":"20250808133542-9zg1oxo","Type":"NodeParagraph","Properties":{"id":"20250808133542-9zg1oxo","updated":"20250808133542"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"对现有相关工作的评述与定位 (Literature Review)"},{"Type":"NodeText","Data":"："}]},{"ID":"20250808133542-28ae91v","Type":"NodeList","ListData":{},"Properties":{"id":"20250808133542-28ae91v","updated":"20250808133542"},"Children":[{"ID":"20250808133542-oii4lax","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808133542-oii4lax","updated":"20250808133542"},"Children":[{"ID":"20250808133542-3k27c6j","Type":"NodeParagraph","Properties":{"id":"20250808133542-3k27c6j","updated":"20250808133542"},"Children":[{"Type":"NodeText","Data":"作者首先承认已经有研究尝试将CNN用于序列任务，但指出了它们的局限性："}]},{"ID":"20250808133542-4eidbah","Type":"NodeList","ListData":{},"Properties":{"id":"20250808133542-4eidbah","updated":"20250808133542"},"Children":[{"ID":"20250808133542-92si7uy","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808133542-92si7uy","updated":"20250808133542"},"Children":[{"ID":"20250808133542-3c0ghy4","Type":"NodeParagraph","Properties":{"id":"20250808133542-3c0ghy4","updated":"20250808133542"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"Bradbury等人"},{"Type":"NodeText","Data":" 和 "},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"Kalchbrenner等人"},{"Type":"NodeText","Data":" 的工作虽然探索了CNN，但其模型性能"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"未能在大规模标准数据集上超越当时最先进的（基于RNN的）模型"},{"Type":"NodeText","Data":"。这暗示了他们的CNN架构可能不够强大或优化不足。"}]}]},{"ID":"20250808133542-v5ox272","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808133542-v5ox272","updated":"20250808133542"},"Children":[{"ID":"20250808133542-jjm15ot","Type":"NodeParagraph","Properties":{"id":"20250808133542-jjm15ot","updated":"20250808133542"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"Meng等人"},{"Type":"NodeText","Data":" 的工作虽然用了门控卷积，但实验"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"规模太小"},{"Type":"NodeText","Data":"，且模型并非独立工作，而是与传统模型结合，说服力不强。"}]}]},{"ID":"20250808133542-icea58h","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808133542-icea58h","updated":"20250808133542"},"Children":[{"ID":"20250808133542-sn7cht8","Type":"NodeParagraph","Properties":{"id":"20250808133542-sn7cht8","updated":"20250808133542"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"Gehring等人"},{"Type":"NodeText","Data":" 的工作虽然性能不错，但其架构是“部分卷积”的——编码器是卷积，但"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"解码器仍然是循环式（RNN）的"},{"Type":"NodeText","Data":"，因此没有完全摆脱RNN的顺序计算瓶颈。"}]}]}]}]},{"ID":"20250808133542-8ji56o3","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808133542-8ji56o3","updated":"20250808133542"},"Children":[{"ID":"20250808133542-np8yu54","Type":"NodeParagraph","Properties":{"id":"20250808133542-np8yu54","updated":"20250808133542"},"Children":[{"Type":"NodeText","Data":"通过这番评述，作者清晰地定位了自己工作的创新点：提出一个"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"完全基于卷积"},{"Type":"NodeText","Data":"的架构，并且能在"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"大规模任务"},{"Type":"NodeText","Data":"上取得"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"业界顶尖（SOTA）"},{"Type":"NodeText","Data":"的性能。"}]}]}]}]}]}]},{"ID":"20250808133546-61atlbe","Type":"NodeBlockquote","Properties":{"fold":"1","id":"20250808133546-61atlbe","updated":"20250808133555"},"Children":[{"Type":"NodeBlockquoteMarker","Data":"\u003e"},{"ID":"20250808133555-enfnkyj","Type":"NodeList","ListData":{"Typ":1},"Properties":{"id":"20250808133555-enfnkyj","updated":"20250808133555"},"Children":[{"ID":"20250808133555-3kka3co","Type":"NodeListItem","ListData":{"Typ":1,"Delimiter":46,"Marker":"Mi4=","Num":2},"Properties":{"id":"20250808133555-3kka3co","updated":"20250808133555"},"Children":[{"ID":"20250808133555-81h95g2","Type":"NodeParagraph","Properties":{"id":"20250808133555-81h95g2","updated":"20250808133555"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"提出的新架构的核心组件 (Proposed Architecture)"},{"Type":"NodeText","Data":"："}]},{"ID":"20250808133555-lof0p55","Type":"NodeList","ListData":{},"Properties":{"id":"20250808133555-lof0p55","updated":"20250808133555"},"Children":[{"ID":"20250808133555-17k8w1g","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808133555-17k8w1g","updated":"20250808133555"},"Children":[{"ID":"20250808133555-0uycv63","Type":"NodeParagraph","Properties":{"id":"20250808133555-0uycv63","updated":"20250808133555"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"完全卷积 (Entirely Convolutional)"},{"Type":"NodeText","Data":"：这是最核心的特点，编码器和解码器都使用CNN，彻底抛弃了RNN。"}]}]},{"ID":"20250808133555-utwxkha","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808133555-utwxkha","updated":"20250808133555"},"Children":[{"ID":"20250808133555-uu7914d","Type":"NodeParagraph","Properties":{"id":"20250808133555-uu7914d","updated":"20250808133555"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"门控线性单元 (Gated Linear Units, GLU)"},{"Type":"NodeText","Data":"：这是一种先进的激活函数，通过门控机制控制信息流，帮助梯度更好地传播，从而更容易训练更深的网络。"}]}]},{"ID":"20250808133555-ibyfda0","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808133555-ibyfda0","updated":"20250808133555"},"Children":[{"ID":"20250808133555-ghbtg37","Type":"NodeParagraph","Properties":{"id":"20250808133555-ghbtg37","updated":"20250808133555"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"残差连接 (Residual Connections)"},{"Type":"NodeText","Data":"：源自ResNet，通过“快捷连接”（shortcuts）允许信息直接跳过某些层，极大地缓解了深度网络中的梯度消失问题，使得构建非常深的网络成为可能。"}]}]},{"ID":"20250808133555-c956gvw","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808133555-c956gvw","updated":"20250808133555"},"Children":[{"ID":"20250808133555-vsafkn0","Type":"NodeParagraph","Properties":{"id":"20250808133555-vsafkn0","updated":"20250808133555"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"每层解码器都使用注意力机制 (Attention in every decoder layer)"},{"Type":"NodeText","Data":"：这是一种精细的设计。在解码器的每一层都加入注意力，可以让模型在不同的抽象层次上都能聚焦于输入序列的相关部分，增强了模型捕捉对齐信息的能力。作者还特意强调，这种设计增加的计算开销"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"可以忽略不计"},{"Type":"NodeText","Data":"。"}]}]}]}]}]}]},{"ID":"20250808133559-85d4hvt","Type":"NodeBlockquote","Properties":{"fold":"1","id":"20250808133559-85d4hvt","updated":"20250808133618"},"Children":[{"Type":"NodeBlockquoteMarker","Data":"\u003e"},{"ID":"20250808133618-4nfqyfk","Type":"NodeList","ListData":{"Typ":1},"Properties":{"id":"20250808133618-4nfqyfk","updated":"20250808133618"},"Children":[{"ID":"20250808133618-if2e4bl","Type":"NodeListItem","ListData":{"Typ":1,"Delimiter":46,"Marker":"My4=","Num":3},"Properties":{"id":"20250808133618-if2e4bl","updated":"20250808133618"},"Children":[{"ID":"20250808133618-klyn5dr","Type":"NodeParagraph","Properties":{"id":"20250808133618-klyn5dr","updated":"20250808133618"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"实验结果与贡献总结 (Results and Contributions)"},{"Type":"NodeText","Data":"："}]},{"ID":"20250808133618-jevolcp","Type":"NodeList","ListData":{},"Properties":{"id":"20250808133618-jevolcp","updated":"20250808133618"},"Children":[{"ID":"20250808133618-hstaatz","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808133618-hstaatz","updated":"20250808133618"},"Children":[{"ID":"20250808133618-qwt2nc7","Type":"NodeParagraph","Properties":{"id":"20250808133618-qwt2nc7","updated":"20250808133618"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"性能超越SOTA"},{"Type":"NodeText","Data":"：文章用具体的数据高调宣布了模型的成功。"}]},{"ID":"20250808133618-2eiq0f5","Type":"NodeList","ListData":{},"Properties":{"id":"20250808133618-2eiq0f5","updated":"20250808133618"},"Children":[{"ID":"20250808133618-twajoxv","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808133618-twajoxv","updated":"20250808133618"},"Children":[{"ID":"20250808133618-i85z9bf","Type":"NodeParagraph","Properties":{"id":"20250808133618-i85z9bf","updated":"20250808133618"},"Children":[{"Type":"NodeText","Data":"在WMT'16英-罗翻译任务上，"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"创造了新的SOTA记录"},{"Type":"NodeText","Data":"，BLEU值提升了1.9之多，这是一个非常显著的进步。"}]}]},{"ID":"20250808133618-065hhtx","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808133618-065hhtx","updated":"20250808133618"},"Children":[{"ID":"20250808133618-7kfdc9k","Type":"NodeParagraph","Properties":{"id":"20250808133618-7kfdc9k","updated":"20250808133618"},"Children":[{"Type":"NodeText","Data":"在竞争激烈的WMT'14英-德和英-法任务上，"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"全面超越了当时非常强大的基准模型"},{"Type":"NodeText","Data":"——谷歌的GNMT（Wu et al., 2016），BLEU值分别提升了0.5和1.6。"}]}]}]}]},{"ID":"20250808133618-ecc09d7","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808133618-ecc09d7","updated":"20250808133618"},"Children":[{"ID":"20250808133618-j4jhzg6","Type":"NodeParagraph","Properties":{"id":"20250808133618-j4jhzg6","updated":"20250808133618"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"速度优势"},{"Type":"NodeText","Data":"：除了准确率，模型在"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"推理速度"},{"Type":"NodeText","Data":"上也有巨大优势。翻译速度比基于RNN的强基准模型"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"快一个数量级"},{"Type":"NodeText","Data":"（即大约10倍），这直接得益于其全卷积架构的并行计算能力。"}]}]}]}]}]}]},{"ID":"20250808133644-3bnh4dw","Type":"NodeHeading","HeadingLevel":1,"Properties":{"id":"20250808133644-3bnh4dw","updated":"20250809090141"},"Children":[{"Type":"NodeText","Data":"循环式序列到序列学习"}]},{"ID":"20250808133813-42uaogd","Type":"NodeParagraph","Properties":{"id":"20250808133813-42uaogd","updated":"20250808140058"},"Children":[{"Type":"NodeText","Data":"序列到序列建模一直以来都是基于循环神经网络的编码器-解码器架构的代名词（Sutskever 等人，2014；Bahdanau 等人，2014）。编码器RNN处理一个由"},{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":"m"},{"Type":"NodeText","Data":"个元素组成的输入序列 "},{"Type":"NodeTextMark","TextMarkType":"strong inline-math","TextMarkInlineMathContent":"x"},{"Type":"NodeText","Data":"​"},{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":" = (x₁, ..., xₘ)"},{"Type":"NodeText","Data":"，并返回状态表示 "},{"Type":"NodeTextMark","TextMarkType":"strong inline-math","TextMarkInlineMathContent":"z"},{"Type":"NodeText","Data":"​"},{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":" = (z₁, ..., zₘ)"},{"Type":"NodeText","Data":"​。解码器RNN接收这些状态表示，并从左到右、一个元素接一个元素地生成输出序列 "},{"Type":"NodeTextMark","TextMarkType":"strong inline-math","TextMarkInlineMathContent":"y"},{"Type":"NodeText","Data":"​"},{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":" = (y₁, ..., yₙ)"},{"Type":"NodeText","Data":"。为了生成输出 "},{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":"yᵢ₊₁"},{"Type":"NodeText","Data":"，解码器会基于前一个隐藏状态 "},{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":"hᵢ"},{"Type":"NodeText","Data":"、前一个目标语言词 "},{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":"yᵢ"},{"Type":"NodeText","Data":" 的嵌入 "},{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":"gᵢ"},{"Type":"NodeText","Data":"，以及一个从编码器输出 "},{"Type":"NodeTextMark","TextMarkType":"strong inline-math","TextMarkInlineMathContent":"z"},{"Type":"NodeText","Data":" 派生出的条件输入 "},{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":"cᵢ"},{"Type":"NodeText","Data":"，来计算一个新的隐藏状态 "},{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":"hᵢ₊₁"},{"Type":"NodeText","Data":"。基于这个通用的公式，已经提出了各种各样的编码器-解码器架构，它们的主要区别在于条件输入 "},{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":"cᵢ"},{"Type":"NodeText","Data":" 和所使用的RNN类型。"}]},{"ID":"20250808133813-vfx05eh","Type":"NodeParagraph","Properties":{"id":"20250808133813-vfx05eh","updated":"20250808140044"},"Children":[{"Type":"NodeText","Data":"不带注意力机制的模型，通过为所有的 "},{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":"i"},{"Type":"NodeText","Data":" 设置 "},{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":"cᵢ = zₘ"},{"Type":"NodeText","Data":" 来只考虑最后的编码器状态 "},{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":"zₘ"},{"Type":"NodeText","Data":"（Cho 等人，2014），或者干脆用 "},{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":"zₘ"},{"Type":"NodeText","Data":" 来初始化第一个解码器状态，这种情况下 "},{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":"cᵢ"},{"Type":"NodeText","Data":" 就不会被使用（Sutskever 等人，2014）。带有注意力机制的架构（Bahdanau 等人，2014；Luong 等人，2015）则在每个时间步都将 "},{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":"cᵢ"},{"Type":"NodeText","Data":" 计算为 "},{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":"(z₁, ..., zₘ)"},{"Type":"NodeText","Data":" 的加权和。这个加权和的权重被称为注意力分数，它允许网络在生成输出序列时，能够聚焦于输入序列的不同部分。注意力分数本质上是通过比较前一个解码器状态 "},{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":"hᵢ"},{"Type":"NodeText","Data":" 和最后一个预测 "},{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":"yᵢ"},{"Type":"NodeText","Data":" 的组合与每个编码器状态 "},{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":"zⱼ"},{"Type":"NodeText","Data":" 来计算的；其结果会被归一化，形成一个在输入元素上的分布。"}]},{"ID":"20250808133813-sbvghr0","Type":"NodeParagraph","Properties":{"id":"20250808133813-sbvghr0","updated":"20250808133813"},"Children":[{"Type":"NodeText","Data":"在编码器-解码器模型中，流行的循环网络选择是长短期记忆网络（LSTM；Hochreiter \u0026 Schmidhuber, 1997）和门控循环单元（GRU；Cho 等人，2014）。两者都通过门控机制扩展了Elman RNNs（Elman, 1990），该机制允许记忆来自先前时间步的信息，以便为长期依赖关系建模。最近的大多数方法还依赖于双向编码器来构建包含过去和未来上下文的表示（Bahdanau 等人，2014；Zhou 等人，2016；Wu 等人，2016）。拥有多层的模型通常依赖于快捷连接或残差连接（He 等人，2015a；Zhou 等人，2016；Wu 等人，2016）。"}]},{"ID":"20250808133821-jg4xr64","Type":"NodeBlockquote","Properties":{"id":"20250808133821-jg4xr64","updated":"20250808133823"},"Children":[{"Type":"NodeBlockquoteMarker","Data":"\u003e"},{"ID":"20250808133822-1lhk5qh","Type":"NodeParagraph","Properties":{"id":"20250808133822-1lhk5qh","updated":"20250808133823"},"Children":[{"Type":"NodeText","Data":"这段文字详细回顾了基于循环神经网络（RNN）的传统序列到序列（Seq2Seq）学习框架，是理解本文所提出的卷积模型（ConvS2S）所要替代和改进的对象的基础。"}]}]},{"ID":"20250808133824-zsqgwga","Type":"NodeBlockquote","Properties":{"fold":"1","id":"20250808133824-zsqgwga","updated":"20250808140654"},"Children":[{"Type":"NodeBlockquoteMarker","Data":"\u003e"},{"ID":"20250808133833-9awudw2","Type":"NodeList","ListData":{"Typ":1},"Properties":{"id":"20250808133833-9awudw2","updated":"20250808140654"},"Children":[{"ID":"20250808133833-xzfy1p7","Type":"NodeListItem","ListData":{"Typ":1,"Delimiter":46,"Marker":"MS4=","Num":1},"Properties":{"id":"20250808133833-xzfy1p7","updated":"20250808140654"},"Children":[{"ID":"20250808133833-oaziy6g","Type":"NodeParagraph","Properties":{"id":"20250808133833-oaziy6g","updated":"20250808133833"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"基本的编码器-解码器（Encoder-Decoder）框架"},{"Type":"NodeText","Data":"："}]},{"ID":"20250808133833-1ubz7dk","Type":"NodeList","ListData":{},"Properties":{"id":"20250808133833-1ubz7dk","updated":"20250808140654"},"Children":[{"ID":"20250808133833-3sxu1r9","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808133833-3sxu1r9","updated":"20250808140616"},"Children":[{"ID":"20250808133833-pf5axtc","Type":"NodeParagraph","Properties":{"id":"20250808133833-pf5axtc","updated":"20250808140616"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"编码器 (Encoder)"},{"Type":"NodeText","Data":"：其职责是“阅读”并理解整个输入序列（如一个德语句子）。它是一个RNN，逐词处理输入，并将每个词的信息整合进其隐藏状态。最终，它输出一系列状态表示 "},{"Type":"NodeTextMark","TextMarkType":"strong inline-math","TextMarkInlineMathContent":"z"},{"Type":"NodeText","Data":"，这个 "},{"Type":"NodeTextMark","TextMarkType":"strong inline-math","TextMarkInlineMathContent":"z"},{"Type":"NodeText","Data":" 可以是每个时间步的隐藏状态集合。"}]}]},{"ID":"20250808133833-8aixpip","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808133833-8aixpip","updated":"20250808140654"},"Children":[{"ID":"20250808133833-16fukha","Type":"NodeParagraph","Properties":{"id":"20250808133833-16fukha","updated":"20250808133833"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"解码器 (Decoder)"},{"Type":"NodeText","Data":"：其职责是根据编码器提供的信息来“写入”或生成输出序列（如一个英语句子）。它也是一个RNN，但其工作方式是自回归的（auto-regressive），即："}]},{"ID":"20250808133833-8n5q0f4","Type":"NodeList","ListData":{},"Properties":{"id":"20250808133833-8n5q0f4","updated":"20250808140654"},"Children":[{"ID":"20250808133833-judbh8u","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808133833-judbh8u","updated":"20250808140654"},"Children":[{"ID":"20250808133833-c8wt308","Type":"NodeParagraph","Properties":{"id":"20250808133833-c8wt308","updated":"20250808140644"},"Children":[{"Type":"NodeText","Data":"在生成第 "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"i+1"},{"Type":"NodeText","Data":"​ 个词时，它会考虑三个关键信息："}]},{"ID":"20250808133833-mlta5ui","Type":"NodeList","ListData":{"Typ":1},"Properties":{"id":"20250808133833-mlta5ui","updated":"20250808140654"},"Children":[{"ID":"20250808133833-pwxlto5","Type":"NodeListItem","ListData":{"Typ":1,"Delimiter":46,"Marker":"MS4=","Num":1},"Properties":{"id":"20250808133833-pwxlto5","updated":"20250808140647"},"Children":[{"ID":"20250808133833-jw5r92g","Type":"NodeParagraph","Properties":{"id":"20250808133833-jw5r92g","updated":"20250808140647"},"Children":[{"Type":"NodeText","Data":"它自己前一步的隐藏状态 "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"hᵢ"},{"Type":"NodeText","Data":"​（内部记忆）。"}]}]},{"ID":"20250808133833-eflrxoz","Type":"NodeListItem","ListData":{"Typ":1,"Delimiter":46,"Marker":"Mi4=","Num":2},"Properties":{"id":"20250808133833-eflrxoz","updated":"20250808140650"},"Children":[{"ID":"20250808133833-zehwr93","Type":"NodeParagraph","Properties":{"id":"20250808133833-zehwr93","updated":"20250808140650"},"Children":[{"Type":"NodeText","Data":"刚刚生成的上一个词 "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"yᵢ"},{"Type":"NodeText","Data":"​（已生成的部分输出）。"}]}]},{"ID":"20250808133833-kygvpnk","Type":"NodeListItem","ListData":{"Typ":1,"Delimiter":46,"Marker":"My4=","Num":3},"Properties":{"id":"20250808133833-kygvpnk","updated":"20250808140654"},"Children":[{"ID":"20250808133833-lw6ij4s","Type":"NodeParagraph","Properties":{"id":"20250808133833-lw6ij4s","updated":"20250808140654"},"Children":[{"Type":"NodeText","Data":"从编码器那里得到的上下文信息 "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"cᵢ"},{"Type":"NodeText","Data":"​。"}]}]}]}]},{"ID":"20250808133833-ayu5fm5","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808133833-ayu5fm5","updated":"20250808133833"},"Children":[{"ID":"20250808133833-febg1bu","Type":"NodeParagraph","Properties":{"id":"20250808133833-febg1bu","updated":"20250808133833"},"Children":[{"Type":"NodeText","Data":"这个过程一步步进行，直到生成一个特殊的“结束”标记为止。"}]}]}]}]}]}]}]}]},{"ID":"20250808133842-ozpak75","Type":"NodeBlockquote","Properties":{"fold":"1","id":"20250808133842-ozpak75","updated":"20250808140811"},"Children":[{"Type":"NodeBlockquoteMarker","Data":"\u003e"},{"ID":"20250808133852-gi4kinn","Type":"NodeList","ListData":{"Typ":1},"Properties":{"id":"20250808133852-gi4kinn","updated":"20250808140811"},"Children":[{"ID":"20250808133852-10px5e4","Type":"NodeListItem","ListData":{"Typ":1,"Delimiter":46,"Marker":"Mi4=","Num":2},"Properties":{"id":"20250808133852-10px5e4","updated":"20250808140811"},"Children":[{"ID":"20250808133852-rhnva1e","Type":"NodeParagraph","Properties":{"id":"20250808133852-rhnva1e","updated":"20250808133852"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"注意力机制 (Attention Mechanism) 的关键作用"},{"Type":"NodeText","Data":"："}]},{"ID":"20250808133852-r3ms896","Type":"NodeList","ListData":{},"Properties":{"id":"20250808133852-r3ms896","updated":"20250808140811"},"Children":[{"ID":"20250808133852-jr2domc","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808133852-jr2domc","updated":"20250808140751"},"Children":[{"ID":"20250808133852-oqyeydu","Type":"NodeParagraph","Properties":{"id":"20250808133852-oqyeydu","updated":"20250808140751"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"无注意力机制的模型"},{"Type":"NodeText","Data":"：这是早期的Seq2Seq模型。它们将整个输入序列的所有信息强行压缩进编码器的"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"最后一个"},{"Type":"NodeText","Data":"隐藏状态 "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"zₘ"},{"Type":"NodeText","Data":"​。解码器在所有时间步都只能利用这个固定的、包含了全部输入信息的向量。这种方法的"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"严重缺陷"},{"Type":"NodeText","Data":"是信息瓶颈：对于长句子，一个固定大小的向量很难记住所有细节，导致信息丢失。"}]}]},{"ID":"20250808133852-sue864c","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808133852-sue864c","updated":"20250808140811"},"Children":[{"ID":"20250808133852-yg8nlw4","Type":"NodeParagraph","Properties":{"id":"20250808133852-yg8nlw4","updated":"20250808133852"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"有注意力机制的模型"},{"Type":"NodeText","Data":"：这是一个革命性的改进。它不再依赖单一的固定向量，而是让解码器在生成每个词时，都能“回头看”并“关注”输入序列的所有部分。"}]},{"ID":"20250808133852-6dcj2jc","Type":"NodeList","ListData":{},"Properties":{"id":"20250808133852-6dcj2jc","updated":"20250808140811"},"Children":[{"ID":"20250808133852-vi9gfcc","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808133852-vi9gfcc","updated":"20250808140804"},"Children":[{"ID":"20250808133852-pekhzyc","Type":"NodeParagraph","Properties":{"id":"20250808133852-pekhzyc","updated":"20250808140804"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"动态的上下文向量"},{"Type":"NodeText","Data":" "},{"Type":"NodeTextMark","TextMarkType":"strong inline-math","TextMarkInlineMathContent":"cᵢ"},{"Type":"NodeText","Data":"：这里的 "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"cᵢ"},{"Type":"NodeText","Data":"​ 不再是固定的 "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"zₘ"},{"Type":"NodeText","Data":"​，而是一个"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"动态计算"},{"Type":"NodeText","Data":"的加权平均值。"}]}]},{"ID":"20250808133852-tefjco4","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808133852-tefjco4","updated":"20250808140811"},"Children":[{"ID":"20250808133852-n89ufxe","Type":"NodeParagraph","Properties":{"id":"20250808133852-n89ufxe","updated":"20250808140811"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"注意力分数 (Attention Scores)"},{"Type":"NodeText","Data":"：权重是通过一个“打分”过程计算出来的。解码器当前的内部状态 "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"hᵢ"},{"Type":"NodeText","Data":"​ 会和编码器的每一个输出状态 "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"zⱼ"},{"Type":"NodeText","Data":"​ 进行比较（比如计算相似度），从而判断输入序列的哪个部分与当前要生成的输出词最相关。相关性越高，权重（分数）就越大。"}]}]},{"ID":"20250808133852-ccrqoap","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808133852-ccrqoap","updated":"20250808133852"},"Children":[{"ID":"20250808133852-vcm8uj8","Type":"NodeParagraph","Properties":{"id":"20250808133852-vcm8uj8","updated":"20250808133852"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"效果"},{"Type":"NodeText","Data":"：这使得解码器可以动态地聚焦于输入序列的不同部分，极大地提升了翻译质量，特别是对于长句子。"}]}]}]}]}]}]}]}]},{"ID":"20250808133904-6tbz9zf","Type":"NodeBlockquote","Properties":{"fold":"1","id":"20250808133904-6tbz9zf","updated":"20250808133907"},"Children":[{"Type":"NodeBlockquoteMarker","Data":"\u003e"},{"ID":"20250808133907-9elsx8h","Type":"NodeList","ListData":{"Typ":1},"Properties":{"id":"20250808133907-9elsx8h","updated":"20250808133907"},"Children":[{"ID":"20250808133907-yvtp349","Type":"NodeListItem","ListData":{"Typ":1,"Delimiter":46,"Marker":"My4=","Num":3},"Properties":{"id":"20250808133907-yvtp349","updated":"20250808133907"},"Children":[{"ID":"20250808133907-w0uwmha","Type":"NodeParagraph","Properties":{"id":"20250808133907-w0uwmha","updated":"20250808133907"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"流行的组件和改进"},{"Type":"NodeText","Data":"："}]},{"ID":"20250808133907-3zmifb9","Type":"NodeList","ListData":{},"Properties":{"id":"20250808133907-3zmifb9","updated":"20250808133907"},"Children":[{"ID":"20250808133907-7gpgv3d","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808133907-7gpgv3d","updated":"20250808133907"},"Children":[{"ID":"20250808133907-5143dkg","Type":"NodeParagraph","Properties":{"id":"20250808133907-5143dkg","updated":"20250808133907"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"RNN变体 (LSTM \u0026amp; GRU)"},{"Type":"NodeText","Data":"：标准的RNN有梯度消失问题，难以学习长期依赖。因此，实际应用中几乎总是使用"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"LSTM"},{"Type":"NodeText","Data":"或"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"GRU"},{"Type":"NodeText","Data":"。它们内部有复杂的“门控机制”（输入门、遗忘门、输出门等），可以有选择地记忆、遗忘和输出信息，从而更好地捕捉长距离依赖关系。"}]}]},{"ID":"20250808133907-uead6y1","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808133907-uead6y1","updated":"20250808133907"},"Children":[{"ID":"20250808133907-kt1b51k","Type":"NodeParagraph","Properties":{"id":"20250808133907-kt1b51k","updated":"20250808133907"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"双向编码器 (Bi-directional Encoder)"},{"Type":"NodeText","Data":"：编码器在处理一个词时，如果能同时看到它前面和后面的词（即完整的上下文），显然能更好地理解其含义。双向编码器通过使用一个前向RNN和一个后向RNN来实现这一点，并将两者的状态拼接起来，得到对每个词更丰富的表示。"}]}]},{"ID":"20250808133907-gtcplg2","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808133907-gtcplg2","updated":"20250808133907"},"Children":[{"ID":"20250808133907-qfiu5yz","Type":"NodeParagraph","Properties":{"id":"20250808133907-qfiu5yz","updated":"20250808133907"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"残差/快捷连接 (Residual/Shortcut Connections)"},{"Type":"NodeText","Data":"：当模型非常深（层数很多）时，为了解决梯度消失问题和方便训练，会使用残差连接。这种连接允许信息“跳过”某些层直接传递到更深层，使得构建和训练非常深的网络成为可能。"}]}]}]}]}]}]},{"ID":"20250808134220-amdkq78","Type":"NodeHeading","HeadingLevel":1,"Properties":{"id":"20250808134220-amdkq78","updated":"20250809090141"},"Children":[{"Type":"NodeText","Data":"一种卷积架构"}]},{"ID":"20250808134234-40x0lrw","Type":"NodeParagraph","Properties":{"id":"20250808134234-40x0lrw","updated":"20250808141204"},"Children":[{"Type":"NodeText","Data":"接下来，我们介绍一种用于序列到序列建模的全卷积架构。我们不再依赖RNN来计算中间的编码器状态 "},{"Type":"NodeTextMark","TextMarkType":"strong inline-math","TextMarkInlineMathContent":"z"},{"Type":"NodeText","Data":" 和解码器状态 "},{"Type":"NodeTextMark","TextMarkType":"strong inline-math","TextMarkInlineMathContent":"h"},{"Type":"NodeText","Data":"，而是使用卷积神经网络（CNN）。"}]},{"ID":"20250808134249-29lwl6o","Type":"NodeHeading","HeadingLevel":3,"Properties":{"id":"20250808134249-29lwl6o","updated":"20250809090141"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"3.1. 位置嵌入 (Position Embeddings)"}]},{"ID":"20250808134249-vypy2ev","Type":"NodeParagraph","Properties":{"id":"20250808134249-vypy2ev","updated":"20250808141302"},"Children":[{"Type":"NodeText","Data":"首先，我们将输入元素 "},{"Type":"NodeTextMark","TextMarkType":"strong inline-math","TextMarkInlineMathContent":"x"},{"Type":"NodeText","Data":"​"},{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":" = (x₁, ..., xₘ)"},{"Type":"NodeText","Data":" 嵌入到一个分布式空间中，得到 "},{"Type":"NodeTextMark","TextMarkType":"strong inline-math","TextMarkInlineMathContent":"w"},{"Type":"NodeText","Data":"​"},{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":" = (w₁, ..., wₘ)"},{"Type":"NodeText","Data":"，其中每个 "},{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":"wⱼ ∈ ℝᶠ"},{"Type":"NodeText","Data":" 是嵌入矩阵 "},{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":"D ∈ ℝ"},{"Type":"NodeText","Data":"​"},{"Type":"NodeTextMark","TextMarkType":"sup inline-math","TextMarkInlineMathContent":"V×f"},{"Type":"NodeText","Data":" 中的一个列向量。我们还通过嵌入输入元素的绝对位置 "},{"Type":"NodeTextMark","TextMarkType":"strong inline-math","TextMarkInlineMathContent":"p"},{"Type":"NodeText","Data":"​"},{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":" = (p₁, ..., pₘ)"},{"Type":"NodeText","Data":" 来为我们的模型赋予顺序感，其中 "},{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":"pⱼ ∈ ℝᶠ"},{"Type":"NodeText","Data":"。将这两者结合起来（相加）以获得输入元素的表示 "},{"Type":"NodeTextMark","TextMarkType":"strong inline-math","TextMarkInlineMathContent":"e"},{"Type":"NodeText","Data":"​"},{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":" = (w₁ + p₁, ..., wₘ + pₘ)"},{"Type":"NodeText","Data":"。对于已经由解码器网络生成的输出元素，我们也采用类似的处理方式，以产生被反馈回解码器网络的输出元素表示 "},{"Type":"NodeTextMark","TextMarkType":"strong inline-math","TextMarkInlineMathContent":"g"},{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":" = (g₁, ..., gₙ)"},{"Type":"NodeText","Data":"。位置嵌入在我们的架构中很有用，因为它们让我们的模型感知到当前正在处理的是输入或输出序列的哪一部分（§5.4）。"}]},{"ID":"20250808134306-vxawkgy","Type":"NodeBlockquote","Properties":{"id":"20250808134306-vxawkgy","updated":"20250808134316"},"Children":[{"Type":"NodeBlockquoteMarker","Data":"\u003e"},{"ID":"20250808134315-rh5vth2","Type":"NodeParagraph","Properties":{"id":"20250808134315-rh5vth2","updated":"20250808134316"},"Children":[{"Type":"NodeText","Data":"这部分内容开始正式介绍本文提出的核心模型——全卷积序列到序列模型（ConvS2S），并首先讲解了其处理输入数据的第一步：位置嵌入。"}]}]},{"ID":"20250808134323-meq407t","Type":"NodeBlockquote","Properties":{"fold":"1","id":"20250808134323-meq407t","updated":"20250808141458"},"Children":[{"Type":"NodeBlockquoteMarker","Data":"\u003e"},{"ID":"20250808134324-gz8zpj4","Type":"NodeList","ListData":{"Typ":1},"Properties":{"id":"20250808134324-gz8zpj4","updated":"20250808141458"},"Children":[{"ID":"20250808134324-1emacnq","Type":"NodeListItem","ListData":{"Typ":1,"Delimiter":46,"Marker":"MS4=","Num":1},"Properties":{"id":"20250808134324-1emacnq","updated":"20250808141458"},"Children":[{"ID":"20250808134324-0mxygir","Type":"NodeParagraph","Properties":{"id":"20250808134324-0mxygir","updated":"20250808134324"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"架构的根本转变"},{"Type":"NodeText","Data":"："}]},{"ID":"20250808134324-yoinltz","Type":"NodeList","ListData":{},"Properties":{"id":"20250808134324-yoinltz","updated":"20250808141458"},"Children":[{"ID":"20250808134324-0da7qbf","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808134324-0da7qbf","updated":"20250808134324"},"Children":[{"ID":"20250808134324-jrx05hl","Type":"NodeParagraph","Properties":{"id":"20250808134324-jrx05hl","updated":"20250808134324"},"Children":[{"Type":"NodeText","Data":"文章开宗明义地指出，这个新架构是"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"全卷积的 (fully convolutional)"},{"Type":"NodeText","Data":"。"}]}]},{"ID":"20250808134324-bx1wtn1","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808134324-bx1wtn1","updated":"20250808141458"},"Children":[{"ID":"20250808134324-qhii0pk","Type":"NodeParagraph","Properties":{"id":"20250808134324-qhii0pk","updated":"20250808141458"},"Children":[{"Type":"NodeText","Data":"这意味着模型中用于计算核心表示（即编码器状态 "},{"Type":"NodeTextMark","TextMarkType":"strong inline-math","TextMarkInlineMathContent":"z"},{"Type":"NodeText","Data":" 和解码器状态 "},{"Type":"NodeTextMark","TextMarkType":"strong inline-math","TextMarkInlineMathContent":"h"},{"Type":"NodeText","Data":"）的组件，将不再是循环神经网络（RNN），而是"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"卷积神经网络（CNN）"},{"Type":"NodeText","Data":"。这是一个根本性的变化，旨在利用CNN的并行计算优势来克服RNN的顺序处理瓶颈。"}]}]}]}]}]}]},{"ID":"20250808134333-u5ujzrz","Type":"NodeBlockquote","Properties":{"fold":"1","id":"20250808134333-u5ujzrz","updated":"20250808160032"},"Children":[{"Type":"NodeBlockquoteMarker","Data":"\u003e"},{"ID":"20250808134335-me2d8ut","Type":"NodeList","ListData":{"Typ":1},"Properties":{"id":"20250808134335-me2d8ut","updated":"20250808160032"},"Children":[{"ID":"20250808134335-r164f1j","Type":"NodeListItem","ListData":{"Typ":1,"Delimiter":46,"Marker":"Mi4=","Num":2},"Properties":{"id":"20250808134335-r164f1j","updated":"20250808160032"},"Children":[{"ID":"20250808134335-d30qjz3","Type":"NodeParagraph","Properties":{"id":"20250808134335-d30qjz3","updated":"20250808134335"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"位置嵌入的必要性"},{"Type":"NodeText","Data":"："}]},{"ID":"20250808134335-dzdgbsv","Type":"NodeList","ListData":{},"Properties":{"id":"20250808134335-dzdgbsv","updated":"20250808160032"},"Children":[{"ID":"20250808134335-9fkbgd8","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808134335-9fkbgd8","updated":"20250808134335"},"Children":[{"ID":"20250808134335-85fx0sp","Type":"NodeParagraph","Properties":{"id":"20250808134335-85fx0sp","updated":"20250808134335"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"CNN的“位置不敏感性”"},{"Type":"NodeText","Data":"：标准的CNN在应用于图像时，其卷积核在图片的不同位置滑动，执行的是相同的操作，它本身并不关心一个像素块的绝对位置。同样，当应用于文本序列时，一个卷积核扫过句子的不同部分，它本身无法知道某个词是句子的第1个词还是第10个词。"}]}]},{"ID":"20250808134335-vgq4rul","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808134335-vgq4rul","updated":"20250808160032"},"Children":[{"ID":"20250808134335-o1wsq7l","Type":"NodeParagraph","Properties":{"id":"20250808134335-o1wsq7l","updated":"20250808160032"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"RNN的内置顺序性"},{"Type":"NodeText","Data":"：与CNN相反，RNN天生就具备处理顺序的能力。它的计算是按时间步依次进行的（从 "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"t-1"},{"Type":"NodeText","Data":"​ 到 "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"t"},{"Type":"NodeText","Data":"​），所以“顺序”信息已经隐含在了其计算流程中。"}]}]},{"ID":"20250808134335-cz45dib","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808134335-cz45dib","updated":"20250808134335"},"Children":[{"ID":"20250808134335-zygn3en","Type":"NodeParagraph","Properties":{"id":"20250808134335-zygn3en","updated":"20250808134335"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"解决方案"},{"Type":"NodeText","Data":"：为了让CNN能够理解和利用单词在句子中的顺序信息（这在语言中至关重要），必须"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"显式地"},{"Type":"NodeText","Data":"将位置信息提供给模型。这就是"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"位置嵌入（Position Embeddings）"},{"Type":"NodeText","Data":"的作用。"}]}]}]}]}]}]},{"ID":"20250808134350-hwy1j7v","Type":"NodeBlockquote","Properties":{"fold":"1","id":"20250808134350-hwy1j7v","updated":"20250808141744"},"Children":[{"Type":"NodeBlockquoteMarker","Data":"\u003e"},{"ID":"20250808134353-8z9jz2i","Type":"NodeList","ListData":{"Typ":1},"Properties":{"id":"20250808134353-8z9jz2i","updated":"20250808141744"},"Children":[{"ID":"20250808134353-xbs2k2h","Type":"NodeListItem","ListData":{"Typ":1,"Delimiter":46,"Marker":"My4=","Num":3},"Properties":{"id":"20250808134353-xbs2k2h","updated":"20250808141744"},"Children":[{"ID":"20250808134353-zl5ac77","Type":"NodeParagraph","Properties":{"id":"20250808134353-zl5ac77","updated":"20250808134353"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"输入表示的构建过程"},{"Type":"NodeText","Data":"："}]},{"ID":"20250808134353-46ud8hy","Type":"NodeList","ListData":{},"Properties":{"id":"20250808134353-46ud8hy","updated":"20250808141744"},"Children":[{"ID":"20250808134353-3wtcj39","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808134353-3wtcj39","updated":"20250808141711"},"Children":[{"ID":"20250808134353-w1eqei4","Type":"NodeParagraph","Properties":{"id":"20250808134353-w1eqei4","updated":"20250808134353"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"第一步：词嵌入 (Word Embeddings)"}]},{"ID":"20250808134353-om14r9k","Type":"NodeList","ListData":{},"Properties":{"id":"20250808134353-om14r9k","updated":"20250808141711"},"Children":[{"ID":"20250808134353-wo8oeiv","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808134353-wo8oeiv","updated":"20250808141711"},"Children":[{"ID":"20250808134353-ygkzhj5","Type":"NodeParagraph","Properties":{"id":"20250808134353-ygkzhj5","updated":"20250808141711"},"Children":[{"Type":"NodeText","Data":"和所有现代NLP模型一样，首先将离散的单词（如 "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"xᵢ"},{"Type":"NodeText","Data":"​）通过一个嵌入矩阵 "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"D"},{"Type":"NodeText","Data":"​ 转换成密集的向量表示 "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"wᵢ"},{"Type":"NodeText","Data":"​。这个向量 "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"wᵢ"},{"Type":"NodeText","Data":"​ 捕捉了单词的"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"语义信息"},{"Type":"NodeText","Data":"。"}]}]}]}]},{"ID":"20250808134353-46zxsfs","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808134353-46zxsfs","updated":"20250808141728"},"Children":[{"ID":"20250808134353-mv56273","Type":"NodeParagraph","Properties":{"id":"20250808134353-mv56273","updated":"20250808134353"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"第二步：位置嵌入 (Position Embeddings)"}]},{"ID":"20250808134353-gkmu4ut","Type":"NodeList","ListData":{},"Properties":{"id":"20250808134353-gkmu4ut","updated":"20250808141728"},"Children":[{"ID":"20250808134353-x37w96y","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808134353-x37w96y","updated":"20250808141728"},"Children":[{"ID":"20250808134353-rhe58vv","Type":"NodeParagraph","Properties":{"id":"20250808134353-rhe58vv","updated":"20250808141728"},"Children":[{"Type":"NodeText","Data":"为序列中的每一个绝对位置（第1位, 第2位, ..., 第"},{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":"m"},{"Type":"NodeText","Data":"位）都学习一个对应的向量表示 "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"pᵢ"},{"Type":"NodeText","Data":"​。这个向量 "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"pᵢ"},{"Type":"NodeText","Data":"​ 只编码"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"位置信息"},{"Type":"NodeText","Data":"，与该位置上是什么单词无关。"}]}]}]}]},{"ID":"20250808134353-ig86y0d","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808134353-ig86y0d","updated":"20250808141744"},"Children":[{"ID":"20250808134353-e1bejg1","Type":"NodeParagraph","Properties":{"id":"20250808134353-e1bejg1","updated":"20250808134353"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"第三步：结合"}]},{"ID":"20250808134353-icr3msv","Type":"NodeList","ListData":{},"Properties":{"id":"20250808134353-icr3msv","updated":"20250808141744"},"Children":[{"ID":"20250808134353-h5d0la9","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808134353-h5d0la9","updated":"20250808141742"},"Children":[{"ID":"20250808134353-1yiapd8","Type":"NodeParagraph","Properties":{"id":"20250808134353-1yiapd8","updated":"20250808141742"},"Children":[{"Type":"NodeText","Data":"将单词的"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"语义嵌入"},{"Type":"NodeText","Data":" "},{"Type":"NodeTextMark","TextMarkType":"strong code inline-math","TextMarkInlineMathContent":"wᵢ"},{"Type":"NodeText","Data":"​ 和其对应的"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"位置嵌入"},{"Type":"NodeText","Data":" "},{"Type":"NodeTextMark","TextMarkType":"strong code inline-math","TextMarkInlineMathContent":"pᵢ"},{"Type":"NodeText","Data":"​ 逐元素相加，得到最终的输入表示 "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"eᵢ = wᵢ + pᵢ"},{"Type":"NodeText","Data":"​。"}]}]},{"ID":"20250808134353-slt8xqv","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808134353-slt8xqv","updated":"20250808141744"},"Children":[{"ID":"20250808134353-8a3yxis","Type":"NodeParagraph","Properties":{"id":"20250808134353-8a3yxis","updated":"20250808141744"},"Children":[{"Type":"NodeText","Data":"这样一来，每个输入到CNN的向量 "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"eᵢ"},{"Type":"NodeText","Data":"​ 就同时包含了“这个位置上是什么词”和“这个词在句子的哪个位置”两种信息。"}]}]}]}]}]}]}]}]},{"ID":"20250808134356-5qr7zkm","Type":"NodeBlockquote","Properties":{"fold":"1","id":"20250808134356-5qr7zkm","updated":"20250808160019"},"Children":[{"Type":"NodeBlockquoteMarker","Data":"\u003e"},{"ID":"20250808134408-52ojum9","Type":"NodeList","ListData":{"Typ":1},"Properties":{"id":"20250808134408-52ojum9","updated":"20250808160019"},"Children":[{"ID":"20250808134408-m7lq3n3","Type":"NodeListItem","ListData":{"Typ":1,"Delimiter":46,"Marker":"NC4=","Num":4},"Properties":{"id":"20250808134408-m7lq3n3","updated":"20250808160019"},"Children":[{"ID":"20250808134408-sxizgy5","Type":"NodeParagraph","Properties":{"id":"20250808134408-sxizgy5","updated":"20250808134408"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"解码器的应用"},{"Type":"NodeText","Data":"："}]},{"ID":"20250808134408-4knu6lp","Type":"NodeList","ListData":{},"Properties":{"id":"20250808134408-4knu6lp","updated":"20250808160019"},"Children":[{"ID":"20250808134408-0km4lk6","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808134408-0km4lk6","updated":"20250808160019"},"Children":[{"ID":"20250808134408-1ljj2mx","Type":"NodeParagraph","Properties":{"id":"20250808134408-1ljj2mx","updated":"20250808160019"},"Children":[{"Type":"NodeText","Data":"这个过程不仅用于编码器的输入，也同样适用于解码器。在生成输出序列时，解码器需要将已经生成的词作为下一步的输入，此时，这些词同样需要经过“词嵌入 + 位置嵌入”的处理，得到 "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"gᵢ"},{"Type":"NodeText","Data":"​，然后再被送入解码器的卷积层。"}]}]}]}]}]}]},{"ID":"20250808134458-zmaoper","Type":"NodeBlockquote","Properties":{"id":"20250808134458-zmaoper","updated":"20250808134500"},"Children":[{"Type":"NodeBlockquoteMarker","Data":"\u003e"},{"ID":"20250808134459-devyxrt","Type":"NodeParagraph","Properties":{"id":"20250808134459-devyxrt","updated":"20250808134500"},"Children":[{"Type":"NodeText","Data":"位置嵌入是所有非循环（non-recurrent）的序列处理架构（包括本文的ConvS2S和后来的Transformer）的基石。它解决了CNN和自注意力网络等模型本身缺乏顺序感知能力的核心问题，通过直接将位置信息注入到输入表示中，使得模型能够学习和利用词序这一关键特征。"}]}]},{"ID":"20250808134634-h2430fq","Type":"NodeHeading","HeadingLevel":3,"Properties":{"id":"20250808134634-h2430fq","updated":"20250809090141"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"3.2. 卷积块结构"}]},{"ID":"20250808134634-w5fpsay","Type":"NodeParagraph","Properties":{"id":"20250808134634-w5fpsay","updated":"20250808141930"},"Children":[{"Type":"NodeText","Data":"编码器和解码器网络都共享一个简单的块结构，该结构基于固定数量的输入元素来计算中间状态。我们将第 "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"l"},{"Type":"NodeText","Data":"​ 个块的输出表示为解码器网络的 "},{"Type":"NodeTextMark","TextMarkType":"strong inline-math","TextMarkInlineMathContent":"h"},{"Type":"NodeText","Data":"​"},{"Type":"NodeTextMark","TextMarkType":"sup inline-math","TextMarkInlineMathContent":"l"},{"Type":"NodeText","Data":"​"},{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":" = (h₁"},{"Type":"NodeText","Data":"​"},{"Type":"NodeTextMark","TextMarkType":"sup inline-math","TextMarkInlineMathContent":"l"},{"Type":"NodeText","Data":"​"},{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":", ..., hₙ"},{"Type":"NodeText","Data":"​"},{"Type":"NodeTextMark","TextMarkType":"sup inline-math","TextMarkInlineMathContent":"l"},{"Type":"NodeText","Data":"​"},{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":")"},{"Type":"NodeText","Data":" 和编码器网络的 "},{"Type":"NodeTextMark","TextMarkType":"strong inline-math","TextMarkInlineMathContent":"z"},{"Type":"NodeText","Data":"​"},{"Type":"NodeTextMark","TextMarkType":"sup inline-math","TextMarkInlineMathContent":"l"},{"Type":"NodeText","Data":"​"},{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":" = (z₁"},{"Type":"NodeText","Data":"​"},{"Type":"NodeTextMark","TextMarkType":"sup inline-math","TextMarkInlineMathContent":"l"},{"Type":"NodeText","Data":"​"},{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":", ..., zₘ"},{"Type":"NodeText","Data":"​"},{"Type":"NodeTextMark","TextMarkType":"sup inline-math","TextMarkInlineMathContent":"l"},{"Type":"NodeText","Data":"​"},{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":")"},{"Type":"NodeText","Data":"；我们交替使用“块”（blocks）和“层”（layers）这两个术语。每个块包含一个一维卷积，其后跟一个非线性激活函数。对于一个具有单个块且卷积核宽度为 "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"k"},{"Type":"NodeText","Data":"​ 的解码器网络，其每个结果状态 "},{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":"hᵢ¹"},{"Type":"NodeText","Data":" 都包含了关于 "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"k"},{"Type":"NodeText","Data":"​ 个输入元素的信息。将多个块堆叠在一起会增加一个状态中所能表示的输入元素的数量。例如，将6个卷积核宽度 "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"k=5"},{"Type":"NodeText","Data":"​ 的块堆叠起来，会产生一个包含25个元素的输入域，即每个输出都依赖于25个输入。非线性激活函数允许网络利用完整的输入域，或者在需要时聚焦于更少的元素。"}]},{"ID":"20250808134634-pksf0ch","Type":"NodeParagraph","Properties":{"id":"20250808134634-pksf0ch","updated":"20250808142036"},"Children":[{"Type":"NodeText","Data":"每个卷积核的参数为 "},{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":"W ∈ ℝ"},{"Type":"NodeText","Data":"​"},{"Type":"NodeTextMark","TextMarkType":"sup inline-math","TextMarkInlineMathContent":"2d×kd"},{"Type":"NodeText","Data":"，"},{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":"b"},{"Type":"NodeText","Data":"​"},{"Type":"NodeTextMark","TextMarkType":"sub inline-math","TextMarkInlineMathContent":"w"},{"Type":"NodeText","Data":"​"},{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":" ∈ ℝ"},{"Type":"NodeText","Data":"​"},{"Type":"NodeTextMark","TextMarkType":"sup inline-math","TextMarkInlineMathContent":"2d"},{"Type":"NodeText","Data":"，它接收一个维度为 "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"d"},{"Type":"NodeText","Data":"​ 的 "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"k"},{"Type":"NodeText","Data":"​ 个输入元素的拼接作为输入 "},{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":"X ∈ ℝ"},{"Type":"NodeText","Data":"​"},{"Type":"NodeTextMark","TextMarkType":"sup inline-math","TextMarkInlineMathContent":"k×d"},{"Type":"NodeText","Data":"，并将它们映射到一个维度为输入两倍的单个输出元素 "},{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":"Y ∈ ℝ"},{"Type":"NodeText","Data":"​"},{"Type":"NodeTextMark","TextMarkType":"sup inline-math","TextMarkInlineMathContent":"2d"},{"Type":"NodeText","Data":"；后续的层在前一层 "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"k"},{"Type":"NodeText","Data":"​ 个输出元素上进行操作。我们选择门控线性单元（GLU; Dauphin等人, 2016）作为非线性激活函数，它对卷积的输出 "},{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":"Y = [A\\;\\,B] ∈ ℝ"},{"Type":"NodeText","Data":"​"},{"Type":"NodeTextMark","TextMarkType":"sup inline-math","TextMarkInlineMathContent":"2d"},{"Type":"NodeText","Data":" 实现了一个简单的门控机制："}]},{"ID":"20250808134634-pyyea4r","Type":"NodeMathBlock","Properties":{"id":"20250808134634-pyyea4r","updated":"20250808142128"},"Children":[{"Type":"NodeMathBlockOpenMarker"},{"Type":"NodeMathBlockContent","Data":"v([A\\;\\,B]) = A ⊗ σ(B)"},{"Type":"NodeMathBlockCloseMarker"}]},{"ID":"20250808134634-v89c0c3","Type":"NodeParagraph","Properties":{"id":"20250808134634-v89c0c3","updated":"20250808142230"},"Children":[{"Type":"NodeText","Data":"其中，"},{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":"A, B ∈ ℝ"},{"Type":"NodeText","Data":"​"},{"Type":"NodeTextMark","TextMarkType":"sup inline-math","TextMarkInlineMathContent":"d"},{"Type":"NodeText","Data":" 是非线性单元的输入，"},{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":"⊗"},{"Type":"NodeText","Data":" 是逐点乘法（point-wise multiplication），输出 "},{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":"v([A\\;\\,B]) ∈ ℝ"},{"Type":"NodeText","Data":"​"},{"Type":"NodeTextMark","TextMarkType":"sup inline-math","TextMarkInlineMathContent":"d"},{"Type":"NodeText","Data":" 的大小是 "},{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":"Y"},{"Type":"NodeText","Data":" 的一半。门控 "},{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":"σ(B)"},{"Type":"NodeText","Data":" 控制着当前上下文中哪些输入 "},{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":"A"},{"Type":"NodeText","Data":" 是相关的。Oord等人（2016b）曾引入过一种类似的非线性激活函数，它对 "},{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":"A"},{"Type":"NodeText","Data":" 应用了 "},{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":"tanh"},{"Type":"NodeText","Data":" 变换，但 Dauphin等人（2016）的研究表明，在语言建模的背景下，GLU 的表现更好。"}]},{"ID":"20250808134703-xvsvv43","Type":"NodeBlockquote","Properties":{"id":"20250808134703-xvsvv43","updated":"20250808134708"},"Children":[{"Type":"NodeBlockquoteMarker","Data":"\u003e"},{"ID":"20250808134706-4kghub2","Type":"NodeParagraph","Properties":{"id":"20250808134706-4kghub2","updated":"20250808134708"},"Children":[{"Type":"NodeText","Data":"这部分详细介绍了构成整个模型（编码器和解码器）的核心构建单元——"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"卷积块 (Convolutional Block)"},{"Type":"NodeText","Data":" 的内部结构和工作原理。"}]}]},{"ID":"20250808134711-9tmrx3p","Type":"NodeBlockquote","Properties":{"fold":"1","id":"20250808134711-9tmrx3p","updated":"20250808134724"},"Children":[{"Type":"NodeBlockquoteMarker","Data":"\u003e"},{"ID":"20250808134724-8gzzcvz","Type":"NodeList","ListData":{"Typ":1},"Properties":{"id":"20250808134724-8gzzcvz","updated":"20250808134724"},"Children":[{"ID":"20250808134724-tm2omwq","Type":"NodeListItem","ListData":{"Typ":1,"Delimiter":46,"Marker":"MS4=","Num":1},"Properties":{"id":"20250808134724-tm2omwq","updated":"20250808134724"},"Children":[{"ID":"20250808134724-0luxfqn","Type":"NodeParagraph","Properties":{"id":"20250808134724-0luxfqn","updated":"20250808134724"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"统一的块结构 (Simple Block Structure)"},{"Type":"NodeText","Data":"："}]},{"ID":"20250808134724-msxurjl","Type":"NodeList","ListData":{},"Properties":{"id":"20250808134724-msxurjl","updated":"20250808134724"},"Children":[{"ID":"20250808134724-ghf32wc","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808134724-ghf32wc","updated":"20250808134724"},"Children":[{"ID":"20250808134724-4xgl4fz","Type":"NodeParagraph","Properties":{"id":"20250808134724-4xgl4fz","updated":"20250808134724"},"Children":[{"Type":"NodeText","Data":"编码器和解码器都不是单一的巨大卷积层，而是由多个"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"相同结构"},{"Type":"NodeText","Data":"的“块”堆叠而成。这种模块化设计是现代深度学习架构（如ResNet, Transformer）的标志，便于构建深层网络。"}]}]}]}]}]}]},{"ID":"20250808134726-jefo1cf","Type":"NodeBlockquote","Properties":{"fold":"1","id":"20250808134726-jefo1cf","updated":"20250808144022"},"Children":[{"Type":"NodeBlockquoteMarker","Data":"\u003e"},{"ID":"20250808134736-lvneti3","Type":"NodeList","ListData":{"Typ":1},"Properties":{"id":"20250808134736-lvneti3","updated":"20250808144022"},"Children":[{"ID":"20250808134736-sv0tts2","Type":"NodeListItem","ListData":{"Typ":1,"Delimiter":46,"Marker":"Mi4=","Num":2},"Properties":{"id":"20250808134736-sv0tts2","updated":"20250808144022"},"Children":[{"ID":"20250808134736-0znneg8","Type":"NodeParagraph","Properties":{"id":"20250808134736-0znneg8","updated":"20250808134736"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"通过堆叠扩大感受野 (Increasing Receptive Field)"},{"Type":"NodeText","Data":"："}]},{"ID":"20250808134736-59l4wmn","Type":"NodeList","ListData":{},"Properties":{"id":"20250808134736-59l4wmn","updated":"20250808144022"},"Children":[{"ID":"20250808134736-jsmvfiv","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808134736-jsmvfiv","updated":"20250808134736"},"Children":[{"ID":"20250808134736-hsw45xf","Type":"NodeParagraph","Properties":{"id":"20250808134736-hsw45xf","updated":"20250808134736"},"Children":[{"Type":"NodeText","Data":"这是理解CNN如何处理序列的关键。单个卷积层只能看到一个固定大小的局部上下文（由卷积核宽度 "},{"Type":"NodeTextMark","TextMarkType":"code","TextMarkTextContent":"k"},{"Type":"NodeText","Data":"​ 决定）。"}]}]},{"ID":"20250808134736-g3m8qqn","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808134736-g3m8qqn","updated":"20250808134736"},"Children":[{"ID":"20250808134736-gsiwi9x","Type":"NodeParagraph","Properties":{"id":"20250808134736-gsiwi9x","updated":"20250808134736"},"Children":[{"Type":"NodeText","Data":"通过"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"堆叠 (stacking)"},{"Type":"NodeText","Data":" 卷积块，更高层的神经元可以看到更广阔的输入范围。文中给出了一个具体的例子：6个 "},{"Type":"NodeTextMark","TextMarkType":"code","TextMarkTextContent":"k=5"},{"Type":"NodeText","Data":"​ 的卷积块堆叠，其最终的感受野是25。"}]}]},{"ID":"20250808134736-2qre3if","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808134736-2qre3if","updated":"20250808144022"},"Children":[{"ID":"20250808134736-rli3v22","Type":"NodeParagraph","Properties":{"id":"20250808134736-rli3v22","updated":"20250808144022"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"计算公式"},{"Type":"NodeText","Data":"：感受野大小 = "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"(k-1) * L + 1"},{"Type":"NodeText","Data":"​，其中 "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"L"},{"Type":"NodeText","Data":"​ 是层数，"},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"k"},{"Type":"NodeText","Data":"​ 是卷积核大小。代入数据："},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"(5-1) * 6 + 1 = 4 * 6 + 1 = 25"},{"Type":"NodeText","Data":"​。"}]}]},{"ID":"20250808134736-r8cjii0","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808134736-r8cjii0","updated":"20250808134736"},"Children":[{"ID":"20250808134736-1o416c8","Type":"NodeParagraph","Properties":{"id":"20250808134736-1o416c8","updated":"20250808134736"},"Children":[{"Type":"NodeText","Data":"这意味着，顶层输出的每个向量都融合了原始输入序列中长达25个元素的信息。这就是CNN在没有循环机制的情况下，捕捉"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"长距离依赖"},{"Type":"NodeText","Data":"的方法。"}]}]}]}]}]}]},{"ID":"20250808134746-yk0zsu6","Type":"NodeBlockquote","Properties":{"fold":"1","id":"20250808134746-yk0zsu6","updated":"20250808144059"},"Children":[{"Type":"NodeBlockquoteMarker","Data":"\u003e"},{"ID":"20250808134748-wvb7j81","Type":"NodeList","ListData":{"Typ":1},"Properties":{"id":"20250808134748-wvb7j81","updated":"20250808144059"},"Children":[{"ID":"20250808134748-swwityd","Type":"NodeListItem","ListData":{"Typ":1,"Delimiter":46,"Marker":"My4=","Num":3},"Properties":{"id":"20250808134748-swwityd","updated":"20250808144059"},"Children":[{"ID":"20250808134748-35a9b0b","Type":"NodeParagraph","Properties":{"id":"20250808134748-35a9b0b","updated":"20250808134748"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"卷积操作细节"},{"Type":"NodeText","Data":"："}]},{"ID":"20250808134748-1o8pfmp","Type":"NodeList","ListData":{},"Properties":{"id":"20250808134748-1o8pfmp","updated":"20250808144059"},"Children":[{"ID":"20250808134748-cmozsj0","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808134748-cmozsj0","updated":"20250808144041"},"Children":[{"ID":"20250808134748-4tweryd","Type":"NodeParagraph","Properties":{"id":"20250808134748-4tweryd","updated":"20250808144041"},"Children":[{"Type":"NodeText","Data":"输入 "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"X"},{"Type":"NodeText","Data":"​ 是 "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"k"},{"Type":"NodeText","Data":"​ 个 "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"d"},{"Type":"NodeText","Data":"​ 维的向量。"}]}]},{"ID":"20250808134748-iju30q7","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808134748-iju30q7","updated":"20250808144047"},"Children":[{"ID":"20250808134748-prv6bal","Type":"NodeParagraph","Properties":{"id":"20250808134748-prv6bal","updated":"20250808144047"},"Children":[{"Type":"NodeText","Data":"通过卷积核 "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"W"},{"Type":"NodeText","Data":"​ 进行卷积，输出一个 "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"2d"},{"Type":"NodeText","Data":"​ 维的向量 "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"Y"},{"Type":"NodeText","Data":"​。"}]}]},{"ID":"20250808134748-g39t3dg","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808134748-g39t3dg","updated":"20250808144059"},"Children":[{"ID":"20250808134748-eglsun2","Type":"NodeParagraph","Properties":{"id":"20250808134748-eglsun2","updated":"20250808144059"},"Children":[{"Type":"NodeText","Data":"一个关键点是：卷积操作将输出的维度"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"加倍"},{"Type":"NodeText","Data":"了（从 "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"d"},{"Type":"NodeText","Data":"​ 到 "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"2d"},{"Type":"NodeText","Data":"​）。这个设计是为了给接下来的门控线性单元（GLU）提供输入。"}]}]}]}]}]}]},{"ID":"20250808134854-8xm4t49","Type":"NodeBlockquote","Properties":{"fold":"1","id":"20250808134854-8xm4t49","updated":"20250808144439"},"Children":[{"Type":"NodeBlockquoteMarker","Data":"\u003e"},{"ID":"20250808134933-4d2d8l4","Type":"NodeList","ListData":{"Typ":1},"Properties":{"id":"20250808134933-4d2d8l4","updated":"20250808134937"},"Children":[{"ID":"20250808134937-iv7u1fw","Type":"NodeListItem","ListData":{"Typ":1,"Delimiter":46,"Marker":"NC4=","Num":4},"Properties":{"id":"20250808134937-iv7u1fw","updated":"20250808134937"},"Children":[{"ID":"20250808134937-qxyzmna","Type":"NodeParagraph","Properties":{"id":"20250808134937-qxyzmna","updated":"20250808134937"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"门控线性单元 (Gated Linear Unit, GLU)"},{"Type":"NodeText","Data":"："}]}]}]},{"ID":"20250808134933-zxqn7zn","Type":"NodeList","ListData":{},"Properties":{"id":"20250808134933-zxqn7zn","updated":"20250808144439"},"Children":[{"ID":"20250808134933-1l8qa13","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808134933-1l8qa13","updated":"20250808144326"},"Children":[{"ID":"20250808134933-ymgwkc9","Type":"NodeParagraph","Properties":{"id":"20250808134933-ymgwkc9","updated":"20250808144326"},"Children":[{"Type":"NodeText","Data":"这是该卷积块的点睛之笔，也是其性能强大的关键原因之一。它"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"充当了非线性激活函数的角色"},{"Type":"NodeText","Data":"。"}]}]},{"ID":"20250808134933-3sfhftn","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808134933-3sfhftn","updated":"20250808144240"},"Children":[{"ID":"20250808134933-5cejykq","Type":"NodeParagraph","Properties":{"id":"20250808134933-5cejykq","updated":"20250808134933"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"工作机制"},{"Type":"NodeText","Data":"："}]},{"ID":"20250808134933-b6brkrk","Type":"NodeList","ListData":{"Typ":1},"Properties":{"id":"20250808134933-b6brkrk","updated":"20250808144240"},"Children":[{"ID":"20250808134933-ox4hbyh","Type":"NodeListItem","ListData":{"Typ":1,"Delimiter":46,"Marker":"MS4=","Num":1},"Properties":{"id":"20250808134933-ox4hbyh","updated":"20250808144227"},"Children":[{"ID":"20250808134933-2j171my","Type":"NodeParagraph","Properties":{"id":"20250808134933-2j171my","updated":"20250808144227"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"分割 (Split)"},{"Type":"NodeText","Data":"：将卷积层输出的 "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"2d"},{"Type":"NodeText","Data":"​ 维向量 "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"Y"},{"Type":"NodeText","Data":"​ 平均分割成两部分："},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"A"},{"Type":"NodeText","Data":"​ 和 "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"B"},{"Type":"NodeText","Data":"​，每一部分都是 "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"d"},{"Type":"NodeText","Data":"​ 维。"}]}]},{"ID":"20250808134933-0vwxgxe","Type":"NodeListItem","ListData":{"Typ":1,"Delimiter":46,"Marker":"Mi4=","Num":2},"Properties":{"id":"20250808134933-0vwxgxe","updated":"20250808144232"},"Children":[{"ID":"20250808134933-qnwimx5","Type":"NodeParagraph","Properties":{"id":"20250808134933-qnwimx5","updated":"20250808144232"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"门控 (Gate)"},{"Type":"NodeText","Data":"：将 "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"B"},{"Type":"NodeText","Data":"​ 部分输入到一个 Sigmoid 函数 "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"σ(B)"},{"Type":"NodeText","Data":"​ 中。Sigmoid 函数的输出在0到1之间，可以被理解为一个“门”或者“阀门”。当输出接近1时，表示“允许通过”；接近0时，表示“阻止通过”。"}]}]},{"ID":"20250808134933-tkt7hrd","Type":"NodeListItem","ListData":{"Typ":1,"Delimiter":46,"Marker":"My4=","Num":3},"Properties":{"id":"20250808134933-tkt7hrd","updated":"20250808144240"},"Children":[{"ID":"20250808134933-38kgmj4","Type":"NodeParagraph","Properties":{"id":"20250808134933-38kgmj4","updated":"20250808144240"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"控制 (Control)"},{"Type":"NodeText","Data":"：将 "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"A"},{"Type":"NodeText","Data":"​ 部分（可以看作是“信息内容”）与 "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"σ(B)"},{"Type":"NodeText","Data":"​（“信息阀门”）进行逐元素相乘 "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"A ⊗ σ(B)"},{"Type":"NodeText","Data":"​。"}]}]}]}]},{"ID":"20250808134933-tk7j1qa","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808134933-tk7j1qa","updated":"20250808144439"},"Children":[{"ID":"20250808134933-9abj1dl","Type":"NodeParagraph","Properties":{"id":"20250808134933-9abj1dl","updated":"20250808144439"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"功能"},{"Type":"NodeText","Data":"：这个门控机制允许网络"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"动态地、有选择性地"},{"Type":"NodeText","Data":"决定 "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"A"},{"Type":"NodeText","Data":"​ 中哪些信息是重要的，应该被保留并传递到下一层。"},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"B"},{"Type":"NodeText","Data":"​ 通过学习来决定如何控制信息流。这"},{"Type":"NodeTextMark","TextMarkType":"strong mark","TextMarkTextContent":"赋予了CNN一种类似RNN中门控（如LSTM的遗忘门、输入门）的能力"},{"Type":"NodeText","Data":"，但实现方式是完全前馈和可并行的。"}]}]},{"ID":"20250808134933-lahgkry","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808134933-lahgkry","updated":"20250808144254"},"Children":[{"ID":"20250808134933-uj4v6vs","Type":"NodeParagraph","Properties":{"id":"20250808134933-uj4v6vs","updated":"20250808144254"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"最终输出"},{"Type":"NodeText","Data":"：经过GLU后，向量的维度从 "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"2d"},{"Type":"NodeText","Data":"​ 降回了 "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"d"},{"Type":"NodeText","Data":"​，与块的输入维度保持一致，便于堆叠。"}]}]}]}]},{"ID":"20250808134839-s9zobgd","Type":"NodeBlockquote","Properties":{"id":"20250808134839-s9zobgd","updated":"20250808134842"},"Children":[{"Type":"NodeBlockquoteMarker","Data":"\u003e"},{"ID":"20250808134841-adnf5pd","Type":"NodeParagraph","Properties":{"id":"20250808134841-adnf5pd","updated":"20250808134842"},"Children":[{"Type":"NodeText","Data":"一个卷积块的流程可以概括为："},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"输入 (d维) → 1D卷积 (维度变为2d) → GLU门控 (维度降回d) → 输出 (d维)"},{"Type":"NodeText","Data":"。通过堆叠这些块，模型在不牺牲并行计算能力的前提下，既能通过卷积的堆叠来捕捉长距离依赖，又能通过GLU门控来灵活地控制信息流动，从而实现了强大的序列建模能力。"}]}]},{"ID":"20250808144453-ip9mxvm","Type":"NodeBlockquote","Properties":{"id":"20250808144453-ip9mxvm","updated":"20250808144751"},"Children":[{"Type":"NodeBlockquoteMarker","Data":"\u003e"},{"ID":"20250808144457-nmixg8r","Type":"NodeParagraph","Properties":{"id":"20250808144457-nmixg8r","updated":"20250808144751"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"个人感悟："},{"Type":"NodeText","Data":"从位置信息嵌入，到信息和信息保留程度的分割，不同含义的抽象单位都在细化、数值化、具象化！"}]}]},{"ID":"20250808145048-5ds4z4c","Type":"NodeParagraph","Properties":{"id":"20250808145048-5ds4z4c","updated":"20250808145048"},"Children":[{"Type":"NodeText","Data":"为了能够构建深层卷积网络，我们从每个卷积的输入端到该块的输出端添加了残差连接（He 等人，2015a）。"}]},{"ID":"20250808152553-ys6rdr8","Type":"NodeMathBlock","Properties":{"id":"20250808152553-ys6rdr8","updated":"20250808152559"},"Children":[{"Type":"NodeMathBlockOpenMarker"},{"Type":"NodeMathBlockContent","Data":"h_i^l=v(W^l[h_{i-k/2}^{l-1},\\ldots,h_{i+k/2}^{l-1}]+b_w^l)+h_i^{l-1}"},{"Type":"NodeMathBlockCloseMarker"}]},{"ID":"20250808145048-vtgbozt","Type":"NodeParagraph","Properties":{"id":"20250808145048-vtgbozt","updated":"20250808152918"},"Children":[{"Type":"NodeText","Data":"对于编码器网络，我们通过在每一层对输入进行填充（padding）来确保卷积层的输出长度与输入长度一致。然而，对于解码器网络，我们必须注意确保没有未来的信息可供解码器使用（Oord 等人，2016a）。具体来说，我们在输入的左侧填充 "},{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":"k-1"},{"Type":"NodeText","Data":" 个零向量，然后从卷积输出的末尾移除 "},{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":"k-1"},{"Type":"NodeText","Data":" 个元素。"}]},{"ID":"20250808145048-8b6d4o4","Type":"NodeParagraph","Properties":{"id":"20250808145048-8b6d4o4","updated":"20250808152733"},"Children":[{"Type":"NodeText","Data":"我们还添加了线性映射，以便在嵌入大小 "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"f"},{"Type":"NodeText","Data":"​ 和大小为 "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"2d"},{"Type":"NodeText","Data":"​ 的卷积输出之间进行投影。当将嵌入 "},{"Type":"NodeTextMark","TextMarkType":"strong inline-math","TextMarkInlineMathContent":"w"},{"Type":"NodeText","Data":" 输入编码器网络时、当将编码器输出 "},{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":"zⱼ"},{"Type":"NodeText","Data":"​"},{"Type":"NodeTextMark","TextMarkType":"sup inline-math","TextMarkInlineMathContent":"u"},{"Type":"NodeText","Data":" 和解码器最后一层 "},{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":"hᴸ"},{"Type":"NodeText","Data":"（在softmax之前）输入时，以及在计算注意力分数（1）之前将所有解码器层 "},{"Type":"NodeTextMark","TextMarkType":"strong inline-math","TextMarkInlineMathContent":"h"},{"Type":"NodeText","Data":"​"},{"Type":"NodeTextMark","TextMarkType":"sup inline-math","TextMarkInlineMathContent":"l"},{"Type":"NodeText","Data":" 输入时，我们都应用了这样的变换。"}]},{"ID":"20250808145048-1j8ezh3","Type":"NodeParagraph","Properties":{"id":"20250808145048-1j8ezh3","updated":"20250808152746"},"Children":[{"Type":"NodeText","Data":"最后，我们通过一个带有权重 "},{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":"W"},{"Type":"NodeText","Data":"ₒ 和偏置 "},{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":"b"},{"Type":"NodeText","Data":"ₒ 的线性层来变换顶层解码器的输出 "},{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":"hᵢᴸ"},{"Type":"NodeText","Data":"，从而计算出在 "},{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":"T"},{"Type":"NodeText","Data":" 个可能的下一个目标元素上的分布："}]},{"ID":"20250808152641-798k6an","Type":"NodeMathBlock","Properties":{"id":"20250808152641-798k6an","updated":"20250808152711"},"Children":[{"Type":"NodeMathBlockOpenMarker"},{"Type":"NodeMathBlockContent","Data":"p(y_{i+1}|y_1,\\ldots,y_i,\\mathbf{x})=\\mathrm{softmax}(W_oh_i^L+b_o)\\in\\mathbb{R}^T"},{"Type":"NodeMathBlockCloseMarker"}]},{"ID":"20250808145121-z80548i","Type":"NodeBlockquote","Properties":{"id":"20250808145121-z80548i","updated":"20250808145124"},"Children":[{"Type":"NodeBlockquoteMarker","Data":"\u003e"},{"ID":"20250808145122-04g9xhx","Type":"NodeParagraph","Properties":{"id":"20250808145122-04g9xhx","updated":"20250808145124"},"Children":[{"Type":"NodeText","Data":"这部分内容介绍了使ConvS2S架构能够有效训练和工作的几个关键技术细节：残差连接、为解码器设计的特殊填充（padding）策略、维度匹配的线性映射以及最终的输出层。"}]}]},{"ID":"20250808145126-gzbreym","Type":"NodeBlockquote","Properties":{"fold":"1","id":"20250808145126-gzbreym","updated":"20250808153259"},"Children":[{"Type":"NodeBlockquoteMarker","Data":"\u003e"},{"ID":"20250808145138-ci80vm8","Type":"NodeList","ListData":{"Typ":1},"Properties":{"id":"20250808145138-ci80vm8","updated":"20250808153259"},"Children":[{"ID":"20250808145138-1v8v22v","Type":"NodeListItem","ListData":{"Typ":1,"Delimiter":46,"Marker":"MS4=","Num":1},"Properties":{"id":"20250808145138-1v8v22v","updated":"20250808153259"},"Children":[{"ID":"20250808145138-8d1q7m9","Type":"NodeParagraph","Properties":{"id":"20250808145138-8d1q7m9","updated":"20250808145138"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"残差连接 (Residual Connections)"}]},{"ID":"20250808145138-2gqhw8d","Type":"NodeList","ListData":{},"Properties":{"id":"20250808145138-2gqhw8d","updated":"20250808153259"},"Children":[{"ID":"20250808145138-po79yva","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808145138-po79yva","updated":"20250808145138"},"Children":[{"ID":"20250808145138-4g0l38n","Type":"NodeParagraph","Properties":{"id":"20250808145138-4g0l38n","updated":"20250808145138"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"目的"},{"Type":"NodeText","Data":"：这是构建"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"深层"},{"Type":"NodeText","Data":"网络的关键技术。当网络层数很多时，梯度在反向传播过程中容易消失或爆炸，导致网络难以训练。"}]}]},{"ID":"20250808145138-qg2buen","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808145138-qg2buen","updated":"20250808153259"},"Children":[{"ID":"20250808145138-wvr3etb","Type":"NodeParagraph","Properties":{"id":"20250808145138-wvr3etb","updated":"20250808153259"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"工作原理"},{"Type":"NodeText","Data":"：如公式 "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"hᵢˡ = ... + hᵢˡ⁻¹"},{"Type":"NodeText","Data":"​ 所示，它将一个块的"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"输入"},{"Type":"NodeText","Data":" "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"hᵢˡ⁻¹"},{"Type":"NodeText","Data":"​ 直接加到这个块的"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"输出"},{"Type":"NodeText","Data":" "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"v(...)"},{"Type":"NodeText","Data":"​ 上。这相当于创建了一条“快捷通道”（shortcut），允许梯度和信息可以绕过卷积和非线性变换直接向前传播。"}]}]},{"ID":"20250808145138-zrwxdnh","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808145138-zrwxdnh","updated":"20250808145138"},"Children":[{"ID":"20250808145138-58iqfih","Type":"NodeParagraph","Properties":{"id":"20250808145138-58iqfih","updated":"20250808145138"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"效果"},{"Type":"NodeText","Data":"：极大地缓解了梯度消失问题，使得构建并成功训练非常深（几十甚至上百层）的卷积网络成为可能，从而能够捕捉更复杂、更长距离的依赖关系。"}]}]}]}]}]}]},{"ID":"20250808145142-crxcjyg","Type":"NodeBlockquote","Properties":{"fold":"1","id":"20250808145142-crxcjyg","updated":"20250808153402"},"Children":[{"Type":"NodeBlockquoteMarker","Data":"\u003e"},{"ID":"20250808145153-2sh6esu","Type":"NodeList","ListData":{"Typ":1},"Properties":{"id":"20250808145153-2sh6esu","updated":"20250808153402"},"Children":[{"ID":"20250808145153-wy3qxyt","Type":"NodeListItem","ListData":{"Typ":1,"Delimiter":46,"Marker":"Mi4=","Num":2},"Properties":{"id":"20250808145153-wy3qxyt","updated":"20250808153402"},"Children":[{"ID":"20250808145153-vtkwubg","Type":"NodeParagraph","Properties":{"id":"20250808145153-vtkwubg","updated":"20250808145153"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"解码器的因果卷积 (Causal Convolution)"}]},{"ID":"20250808145153-ntuwjwf","Type":"NodeList","ListData":{},"Properties":{"id":"20250808145153-ntuwjwf","updated":"20250808153402"},"Children":[{"ID":"20250808145153-tqo6dgc","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808145153-tqo6dgc","updated":"20250808153343"},"Children":[{"ID":"20250808145153-01hkxht","Type":"NodeParagraph","Properties":{"id":"20250808145153-01hkxht","updated":"20250808153343"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"问题"},{"Type":"NodeText","Data":"：在解码（如生成译文）时，模型在预测下一个词 "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"yᵢ₊₁"},{"Type":"NodeText","Data":"​ 时，"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"绝对不能看到"},{"Type":"NodeText","Data":" "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"yᵢ₊₁"},{"Type":"NodeText","Data":"​ 及其之后的任何“未来”信息。这是一个基本的“因果关系”（causality）约束。"}]}]},{"ID":"20250808145153-c5dgmw4","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808145153-c5dgmw4","updated":"20250808145153"},"Children":[{"ID":"20250808145153-et57vlh","Type":"NodeParagraph","Properties":{"id":"20250808145153-et57vlh","updated":"20250808145153"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"编码器的处理"},{"Type":"NodeText","Data":"：编码器则没有这个限制，它可以一次性看到整个输入序列。因此，它使用标准的填充方式，确保每层卷积的输入输出序列长度不变。"}]}]},{"ID":"20250808145153-pqx3czc","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808145153-pqx3czc","updated":"20250808153356"},"Children":[{"ID":"20250808145153-7befi46","Type":"NodeParagraph","Properties":{"id":"20250808145153-7befi46","updated":"20250808145153"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"解码器的解决方案"},{"Type":"NodeText","Data":"：为了在解码器中维持因果关系，同时又要使用并行的卷积操作，本文采用了一种巧妙的"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"填充和裁剪"},{"Type":"NodeText","Data":"策略："}]},{"ID":"20250808145153-bz91nwf","Type":"NodeList","ListData":{},"Properties":{"id":"20250808145153-bz91nwf","updated":"20250808153356"},"Children":[{"ID":"20250808145153-if5ciuy","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808145153-if5ciuy","updated":"20250808153353"},"Children":[{"ID":"20250808145153-es6lopf","Type":"NodeParagraph","Properties":{"id":"20250808145153-es6lopf","updated":"20250808153353"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"填充 (Padding)"},{"Type":"NodeText","Data":"：在输入序列的"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"左侧"},{"Type":"NodeText","Data":"（即过去的方向）填充 "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"k-1"},{"Type":"NodeText","Data":"​ 个零。"}]}]},{"ID":"20250808145153-r42880q","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808145153-r42880q","updated":"20250808145153"},"Children":[{"ID":"20250808145153-fqi2y91","Type":"NodeParagraph","Properties":{"id":"20250808145153-fqi2y91","updated":"20250808145153"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"卷积 (Convolution)"},{"Type":"NodeText","Data":"：正常执行卷积操作。"}]}]},{"ID":"20250808145153-1qku2qn","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808145153-1qku2qn","updated":"20250808153356"},"Children":[{"ID":"20250808145153-uncz0x5","Type":"NodeParagraph","Properties":{"id":"20250808145153-uncz0x5","updated":"20250808153356"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"裁剪 (Cropping)"},{"Type":"NodeText","Data":"：将卷积输出结果的"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"右侧"},{"Type":"NodeText","Data":"（即未来的方向）的 "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"k-1"},{"Type":"NodeText","Data":"​ 个元素"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"移除"},{"Type":"NodeText","Data":"。"}]}]}]}]},{"ID":"20250808145153-gz86jjs","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808145153-gz86jjs","updated":"20250808153402"},"Children":[{"ID":"20250808145153-5kwvdlx","Type":"NodeParagraph","Properties":{"id":"20250808145153-5kwvdlx","updated":"20250808153402"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"效果"},{"Type":"NodeText","Data":"：通过这样的“左填右切”操作，确保了在计算任何位置 "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"i"},{"Type":"NodeText","Data":"​ 的输出时，卷积核所能“看到”的输入信息最多只到位置 "},{"Type":"NodeTextMark","TextMarkType":"code","TextMarkTextContent":"i"},{"Type":"NodeText","Data":"​，而绝不会接触到位置 "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"i+1"},{"Type":"NodeText","Data":"​ 或之后的信息。这就在保持并行计算优势的同时，完美地模拟了RNN的顺序生成过程，因此被称为"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"因果卷积"},{"Type":"NodeText","Data":"。"}]}]}]}]}]}]},{"ID":"20250808145158-xw7nyyu","Type":"NodeBlockquote","Properties":{"fold":"1","id":"20250808145158-xw7nyyu","updated":"20250808153601"},"Children":[{"Type":"NodeBlockquoteMarker","Data":"\u003e"},{"ID":"20250808145207-mumehui","Type":"NodeList","ListData":{"Typ":1},"Properties":{"id":"20250808145207-mumehui","updated":"20250808153601"},"Children":[{"ID":"20250808145207-nxb97pl","Type":"NodeListItem","ListData":{"Typ":1,"Delimiter":46,"Marker":"My4=","Num":3},"Properties":{"id":"20250808145207-nxb97pl","updated":"20250808153601"},"Children":[{"ID":"20250808145207-4x2lfiv","Type":"NodeParagraph","Properties":{"id":"20250808145207-4x2lfiv","updated":"20250808145207"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"线性映射 (Linear Mappings)"}]},{"ID":"20250808145207-26tgbtv","Type":"NodeList","ListData":{},"Properties":{"id":"20250808145207-26tgbtv","updated":"20250808153601"},"Children":[{"ID":"20250808145207-53vn4fz","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808145207-53vn4fz","updated":"20250808153551"},"Children":[{"ID":"20250808145207-jxovgrj","Type":"NodeParagraph","Properties":{"id":"20250808145207-jxovgrj","updated":"20250808153551"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"目的"},{"Type":"NodeText","Data":"：在模型的不同部分，向量的维度可能不同。例如，词嵌入的维度是 "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"f"},{"Type":"NodeText","Data":"​，而卷积块内部的维度是 "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"d"},{"Type":"NodeText","Data":"​ 或 "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"2d"},{"Type":"NodeText","Data":"​。线性映射（本质上就是一个全连接层，或者说矩阵乘法）的作用就像一个“适配器”，用来匹配不同模块之间的维度。"}]}]},{"ID":"20250808145207-m147cl0","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808145207-m147cl0","updated":"20250808153601"},"Children":[{"ID":"20250808145207-tdk1zi8","Type":"NodeParagraph","Properties":{"id":"20250808145207-tdk1zi8","updated":"20250808145207"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"应用场景"},{"Type":"NodeText","Data":"："}]},{"ID":"20250808145207-ozx8pnx","Type":"NodeList","ListData":{},"Properties":{"id":"20250808145207-ozx8pnx","updated":"20250808153601"},"Children":[{"ID":"20250808145207-jveqie0","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808145207-jveqie0","updated":"20250808153555"},"Children":[{"ID":"20250808145207-th33lct","Type":"NodeParagraph","Properties":{"id":"20250808145207-th33lct","updated":"20250808153555"},"Children":[{"Type":"NodeText","Data":"将词嵌入 "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"w"},{"Type":"NodeText","Data":"​ 投影到编码器网络所需的维度。"}]}]},{"ID":"20250808145207-vn7dj9b","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808145207-vn7dj9b","updated":"20250808153559"},"Children":[{"ID":"20250808145207-j1ao5fb","Type":"NodeParagraph","Properties":{"id":"20250808145207-j1ao5fb","updated":"20250808153559"},"Children":[{"Type":"NodeText","Data":"在计算注意力时，将解码器层 "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"h"},{"Type":"NodeText","Data":"​ 和编码器输出 "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"z"},{"Type":"NodeText","Data":"​ 投影到合适的维度进行交互。"}]}]},{"ID":"20250808145207-np2w3xy","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808145207-np2w3xy","updated":"20250808153601"},"Children":[{"ID":"20250808145207-fvmwlic","Type":"NodeParagraph","Properties":{"id":"20250808145207-fvmwlic","updated":"20250808153601"},"Children":[{"Type":"NodeText","Data":"在最后一步，将解码器顶层的输出 "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"hᴸ"},{"Type":"NodeText","Data":"​ 投影到词汇表大小。"}]}]}]}]}]}]}]}]},{"ID":"20250808145225-32ck5vp","Type":"NodeBlockquote","Properties":{"fold":"1","id":"20250808145225-32ck5vp","updated":"20250808153855"},"Children":[{"Type":"NodeBlockquoteMarker","Data":"\u003e"},{"ID":"20250808145228-mhajwya","Type":"NodeList","ListData":{"Typ":1},"Properties":{"id":"20250808145228-mhajwya","updated":"20250808153855"},"Children":[{"ID":"20250808145228-h10urga","Type":"NodeListItem","ListData":{"Typ":1,"Delimiter":46,"Marker":"NC4=","Num":4},"Properties":{"id":"20250808145228-h10urga","updated":"20250808153855"},"Children":[{"ID":"20250808145228-39qn6rz","Type":"NodeParagraph","Properties":{"id":"20250808145228-39qn6rz","updated":"20250808145228"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"最终输出层 (Final Output Layer)"}]},{"ID":"20250808145228-8n3dv0u","Type":"NodeList","ListData":{},"Properties":{"id":"20250808145228-8n3dv0u","updated":"20250808153855"},"Children":[{"ID":"20250808145228-uca3fgp","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808145228-uca3fgp","updated":"20250808153838"},"Children":[{"ID":"20250808145228-kzy3uqv","Type":"NodeParagraph","Properties":{"id":"20250808145228-kzy3uqv","updated":"20250808153838"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"过程"},{"Type":"NodeText","Data":"：将经过多层卷积、残差连接和注意力计算后得到的、解码器最顶层的输出向量 "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"hᵢᴸ"},{"Type":"NodeText","Data":"​（它包含了生成下一个词所需的所有信息）进行最后处理。"}]}]},{"ID":"20250808145228-5nsjiyb","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808145228-5nsjiyb","updated":"20250808153855"},"Children":[{"ID":"20250808145228-eqfrv2z","Type":"NodeParagraph","Properties":{"id":"20250808145228-eqfrv2z","updated":"20250808145228"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"步骤"},{"Type":"NodeText","Data":"："}]},{"ID":"20250808145228-3h7moh6","Type":"NodeList","ListData":{"Typ":1},"Properties":{"id":"20250808145228-3h7moh6","updated":"20250808153855"},"Children":[{"ID":"20250808145228-ep4bro1","Type":"NodeListItem","ListData":{"Typ":1,"Delimiter":46,"Marker":"MS4=","Num":1},"Properties":{"id":"20250808145228-ep4bro1","updated":"20250808153852"},"Children":[{"ID":"20250808145228-vfa6aje","Type":"NodeParagraph","Properties":{"id":"20250808145228-vfa6aje","updated":"20250808153852"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"线性变换"},{"Type":"NodeText","Data":"：通过一个线性层（权重为 "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"W"},{"Type":"NodeText","Data":"​。）将其投影到维度为 "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"T"},{"Type":"NodeText","Data":"​ 的向量上，其中 "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"T"},{"Type":"NodeText","Data":"​ 是整个词汇表的大小。这个向量的每个元素可以看作是对应词汇表里每个词的得分（logit）。"}]}]},{"ID":"20250808145228-fznqov8","Type":"NodeListItem","ListData":{"Typ":1,"Delimiter":46,"Marker":"Mi4=","Num":2},"Properties":{"id":"20250808145228-fznqov8","updated":"20250808153855"},"Children":[{"ID":"20250808145228-ib987rj","Type":"NodeParagraph","Properties":{"id":"20250808145228-ib987rj","updated":"20250808153855"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"Softmax"},{"Type":"NodeText","Data":"：将这些得分通过 "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"softmax"},{"Type":"NodeText","Data":"​ 函数转换成一个概率分布。这个分布中的每个值都在0到1之间，且总和为1，代表了下一个词是词汇表中某个具体词的概率。"}]}]}]}]},{"ID":"20250808145228-gp1o5jw","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808145228-gp1o5jw","updated":"20250808145228"},"Children":[{"ID":"20250808145228-y8ltqlb","Type":"NodeParagraph","Properties":{"id":"20250808145228-y8ltqlb","updated":"20250808145228"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"结果"},{"Type":"NodeText","Data":"：模型最终输出的是一个概率分布，概率最高的那个词通常被选为最终的预测结果。"}]}]}]}]}]}]},{"ID":"20250808150741-90enug6","Type":"NodeParagraph","Properties":{"id":"20250808150741-90enug6","updated":"20250808150741"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"3.3. 多步注意力 (Multi-step Attention)"}]},{"ID":"20250808150822-e2xakrr","Type":"NodeParagraph","Properties":{"id":"20250808150822-e2xakrr","updated":"20250808150854"},"Children":[{"Type":"NodeText","Data":"​"},{"Type":"NodeImage","Data":"span","Children":[{"Type":"NodeBang"},{"Type":"NodeOpenBracket"},{"Type":"NodeLinkText","Data":"image"},{"Type":"NodeCloseBracket"},{"Type":"NodeOpenParen"},{"Type":"NodeLinkDest","Data":"assets/image-20250808150822-cxbto4j.png"},{"Type":"NodeLinkSpace"},{"Type":"NodeLinkTitle","Data":"图1：训练过程中批处理的图示。英文源句在顶部进行编码，我们同时计算四个德语目标词的全部注意力值（中间）。我们的注意力机制只是解码器上下文表示（左下）和编码器表示（左上）之间的点积。我们将通过注意力计算出的条件输入（中右）添加到解码器状态中，然后解码器状态会预测目标词（右下）。Sigmoid和乘法框图示了门控线性单元。"},{"Type":"NodeCloseParen"}]},{"Type":"NodeText","Data":"​"}]},{"ID":"20250808151344-c3tu9d8","Type":"NodeBlockquote","Properties":{"fold":"1","id":"20250808151344-c3tu9d8","updated":"20250808151346"},"Children":[{"Type":"NodeBlockquoteMarker","Data":"\u003e"},{"ID":"20250808151345-mkv0cd7","Type":"NodeParagraph","Properties":{"id":"20250808151345-mkv0cd7","updated":"20250808151346"},"Children":[{"Type":"NodeText","Data":"这张图完美地展示了并行计算的优势。在训练时，编码器（顶部）一次性处理完源句。解码器（底部）也一次性处理完当前已知的目标词序列（\"Sie stimmen zu\"）。然后，注意力模块（中间）可以"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"同时并行地"},{"Type":"NodeText","Data":"计算出所有目标词位置（\"Sie\", \"stimmen\", \"zu\"）相对于所有源词的注意力权重和上下文向量。最后，这些上下文向量被加回到对应的解码器位置，再并行地预测出下一个词。整个过程没有RNN的顺序依赖，效率极高。"}]}]},{"ID":"20250808150741-him83vm","Type":"NodeParagraph","Properties":{"id":"20250808150741-him83vm","updated":"20250808154355"},"Children":[{"Type":"NodeText","Data":"我们为每个解码器层引入了一个独立的注意力机制。为了计算注意力，我们将当前的解码器状态 "},{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":"hᵢˡ"},{"Type":"NodeText","Data":" 与前一个目标元素的嵌入 "},{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":"gᵢ"},{"Type":"NodeText","Data":" 相结合："}]},{"ID":"20250808150741-6brt5xq","Type":"NodeMathBlock","Properties":{"id":"20250808150741-6brt5xq","updated":"20250808154907"},"Children":[{"Type":"NodeMathBlockOpenMarker"},{"Type":"NodeMathBlockContent","Data":"d_i^l=W_d^lh_i^l+b_d^l+g_i"},{"Type":"NodeMathBlockCloseMarker"}]},{"ID":"20250808150741-343xtgz","Type":"NodeParagraph","Properties":{"id":"20250808150741-343xtgz","updated":"20250808155326"},"Children":[{"Type":"NodeText","Data":"对于解码器的第 "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"l"},{"Type":"NodeText","Data":"​ 层，状态 "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"i"},{"Type":"NodeText","Data":"​ 和源元素 "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"j"},{"Type":"NodeText","Data":"​ 之间的注意力 "},{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":"a_{ij}^l"},{"Type":"NodeText","Data":" 是通过计算解码器状态摘要 "},{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":"dᵢˡ"},{"Type":"NodeText","Data":" 与最后一个编码器块 "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"u"},{"Type":"NodeText","Data":"​ 的每个输出 "},{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":"z"},{"Type":"NodeText","Data":"​"},{"Type":"NodeTextMark","TextMarkType":"sub inline-math","TextMarkInlineMathContent":"j"},{"Type":"NodeText","Data":"​"},{"Type":"NodeTextMark","TextMarkType":"sup inline-math","TextMarkInlineMathContent":"u"},{"Type":"NodeText","Data":" 之间的点积（dot-product）来得到的："}]},{"ID":"20250808154623-ykfa9jb","Type":"NodeMathBlock","Properties":{"id":"20250808154623-ykfa9jb","updated":"20250808154905"},"Children":[{"Type":"NodeMathBlockOpenMarker"},{"Type":"NodeMathBlockContent","Data":"a_{ij}^l=\\frac{\\exp\\left(d_i^l\\cdot z_j^u\\right)}{\\sum_{t=1}^m\\exp\\left(d_i^l\\cdot z_t^u\\right)}"},{"Type":"NodeMathBlockCloseMarker"}]},{"ID":"20250808150741-z5qtzmq","Type":"NodeParagraph","Properties":{"id":"20250808150741-z5qtzmq","updated":"20250808151018"},"Children":[{"Type":"NodeText","Data":"提供给当前解码器层的条件输入 "},{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":"cᵢˡ"},{"Type":"NodeText","Data":" 是编码器输出以及输入元素嵌入 "},{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":"eⱼ"},{"Type":"NodeText","Data":" 的加权和（图1，中右侧）："}]},{"ID":"20250808154857-w5ccaks","Type":"NodeMathBlock","Properties":{"id":"20250808154857-w5ccaks","updated":"20250808154904"},"Children":[{"Type":"NodeMathBlockOpenMarker"},{"Type":"NodeMathBlockContent","Data":"c_i^l=\\sum_{j=1}^ma_{ij}^l(z_j^u+e_j)"},{"Type":"NodeMathBlockCloseMarker"}]},{"ID":"20250808150741-l92s7yb","Type":"NodeParagraph","Properties":{"id":"20250808150741-l92s7yb","updated":"20250808151128"},"Children":[{"Type":"NodeText","Data":"这与循环式方法略有不同，后者通常只对 "},{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":"z"},{"Type":"NodeText","Data":"​"},{"Type":"NodeTextMark","TextMarkType":"sub inline-math","TextMarkInlineMathContent":"j"},{"Type":"NodeText","Data":"​"},{"Type":"NodeTextMark","TextMarkType":"sup inline-math","TextMarkInlineMathContent":"u"},{"Type":"NodeText","Data":" 计算注意力和加权和。我们发现添加 "},{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":"eⱼ"},{"Type":"NodeText","Data":" 是有益的，它类似于键值记忆网络（key-value memory networks），其中键（keys）是 "},{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":"z"},{"Type":"NodeText","Data":"​"},{"Type":"NodeTextMark","TextMarkType":"sub inline-math","TextMarkInlineMathContent":"j"},{"Type":"NodeText","Data":"​"},{"Type":"NodeTextMark","TextMarkType":"sup inline-math","TextMarkInlineMathContent":"u"},{"Type":"NodeText","Data":"，而值（values）是 ("},{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":"z"},{"Type":"NodeText","Data":"​"},{"Type":"NodeTextMark","TextMarkType":"sub inline-math","TextMarkInlineMathContent":"j"},{"Type":"NodeText","Data":"​"},{"Type":"NodeTextMark","TextMarkType":"sup inline-math","TextMarkInlineMathContent":"u"},{"Type":"NodeText","Data":"​"},{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":" + eⱼ"},{"Type":"NodeText","Data":")（Miller 等人，2016）。编码器输出 "},{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":"z"},{"Type":"NodeText","Data":"​"},{"Type":"NodeTextMark","TextMarkType":"sub inline-math","TextMarkInlineMathContent":"j"},{"Type":"NodeText","Data":"​"},{"Type":"NodeTextMark","TextMarkType":"sup inline-math","TextMarkInlineMathContent":"u"},{"Type":"NodeText","Data":" 代表了可能很大的输入上下文，而 "},{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":"eⱼ"},{"Type":"NodeText","Data":" 提供了关于特定输入元素的点信息，这在进行预测时很有用。一旦 "},{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":"cᵢˡ"},{"Type":"NodeText","Data":" 被计算出来，它就会被直接加到相应的解码器层输出 "},{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":"hᵢˡ"},{"Type":"NodeText","Data":" 上。"}]},{"ID":"20250808151154-cbrnjyk","Type":"NodeBlockquote","Properties":{"id":"20250808151154-cbrnjyk","updated":"20250808151207"},"Children":[{"Type":"NodeBlockquoteMarker","Data":"\u003e"},{"ID":"20250808151206-cm65q4i","Type":"NodeParagraph","Properties":{"id":"20250808151206-cm65q4i","updated":"20250808151207"},"Children":[{"Type":"NodeText","Data":"这部分是本文模型（ConvS2S）最具创新性的部分之一，详细介绍了其独特的多步注意力机制。"}]}]},{"ID":"20250808151215-6xil9mh","Type":"NodeBlockquote","Properties":{"fold":"1","id":"20250808151215-6xil9mh","updated":"20250808151217"},"Children":[{"Type":"NodeBlockquoteMarker","Data":"\u003e"},{"ID":"20250808151217-3rqa8wq","Type":"NodeList","ListData":{"Typ":1},"Properties":{"id":"20250808151217-3rqa8wq","updated":"20250808151217"},"Children":[{"ID":"20250808151217-qcac9b5","Type":"NodeListItem","ListData":{"Typ":1,"Delimiter":46,"Marker":"MS4=","Num":1},"Properties":{"id":"20250808151217-qcac9b5","updated":"20250808151217"},"Children":[{"ID":"20250808151217-mqhbsvy","Type":"NodeParagraph","Properties":{"id":"20250808151217-mqhbsvy","updated":"20250808151217"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"多步注意力 (Multi-step Attention) 的核心思想"}]},{"ID":"20250808151217-qxmoxru","Type":"NodeList","ListData":{},"Properties":{"id":"20250808151217-qxmoxru","updated":"20250808151217"},"Children":[{"ID":"20250808151217-98fk605","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808151217-98fk605","updated":"20250808151217"},"Children":[{"ID":"20250808151217-6cpl25s","Type":"NodeParagraph","Properties":{"id":"20250808151217-6cpl25s","updated":"20250808151217"},"Children":[{"Type":"NodeText","Data":"传统的基于RNN的注意力机制通常只在解码器的每一时间步，用顶层的解码器状态去查询一次编码器的输出。"}]}]},{"ID":"20250808151217-o0z8htv","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808151217-o0z8htv","updated":"20250808151217"},"Children":[{"ID":"20250808151217-mifiib4","Type":"NodeParagraph","Properties":{"id":"20250808151217-mifiib4","updated":"20250808151217"},"Children":[{"Type":"NodeText","Data":"本文提出的架构则在"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"解码器的每一层（每一个卷积块）"},{"Type":"NodeText","Data":"都计算一次独立的注意力。这意味着模型在不同抽象层次上都能与输入序列进行对齐和信息交互。底层解码器关注的是更底层的特征对齐，而高层解码器则关注更抽象的语义对齐。这使得信息融合更加精细和深入。"}]}]}]}]}]}]},{"ID":"20250808151222-ri4diht","Type":"NodeBlockquote","Properties":{"fold":"1","id":"20250808151222-ri4diht","updated":"20250808160648"},"Children":[{"Type":"NodeBlockquoteMarker","Data":"\u003e"},{"ID":"20250808151246-u1k8ybp","Type":"NodeList","ListData":{"Typ":1},"Properties":{"id":"20250808151246-u1k8ybp","updated":"20250808160648"},"Children":[{"ID":"20250808151246-qsrw1ts","Type":"NodeListItem","ListData":{"Typ":1,"Delimiter":46,"Marker":"Mi4=","Num":2},"Properties":{"id":"20250808151246-qsrw1ts","updated":"20250808160648"},"Children":[{"ID":"20250808151246-r1pnxyn","Type":"NodeParagraph","Properties":{"id":"20250808151246-r1pnxyn","updated":"20250808151246"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"注意力计算的三个步骤"}]},{"ID":"20250808151246-g26pdkq","Type":"NodeList","ListData":{},"Properties":{"id":"20250808151246-g26pdkq","updated":"20250808160648"},"Children":[{"ID":"20250808151246-9teq2nu","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808151246-9teq2nu","updated":"20250808160430"},"Children":[{"ID":"20250808151246-edaqhs4","Type":"NodeParagraph","Properties":{"id":"20250808151246-edaqhs4","updated":"20250808151246"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"第一步：生成查询向量 (Query)"}]},{"ID":"20250808151246-3r87hfc","Type":"NodeList","ListData":{},"Properties":{"id":"20250808151246-3r87hfc","updated":"20250808160430"},"Children":[{"ID":"20250808151246-fmv2vmo","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808151246-fmv2vmo","updated":"20250808160430"},"Children":[{"ID":"20250808151246-l31tzk7","Type":"NodeParagraph","Properties":{"id":"20250808151246-l31tzk7","updated":"20250808160430"},"Children":[{"Type":"NodeText","Data":"公式(1) "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"dᵢˡ = Wdˡhᵢˡ + bdˡ + gᵢ"},{"Type":"NodeText","Data":"​ 显示，用于查询的向量 "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"dᵢˡ"},{"Type":"NodeText","Data":"​ 并不仅仅是当前解码器卷积块的输出 "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"hᵢˡ"},{"Type":"NodeText","Data":"​，还加上了"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"前一个目标词的嵌入"},{"Type":"NodeText","Data":" "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"gᵢ"},{"Type":"NodeText","Data":"​。"}]}]},{"ID":"20250808151246-riww5v9","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808151246-riww5v9","updated":"20250808160246"},"Children":[{"ID":"20250808151246-mzos6tw","Type":"NodeParagraph","Properties":{"id":"20250808151246-mzos6tw","updated":"20250808160246"},"Children":[{"Type":"NodeText","Data":"这使得查询向量同时包含了“我当前解码到什么状态了” ("},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"hᵢˡ"},{"Type":"NodeText","Data":"​) 和“我是基于哪个词来预测下一个词的” ("},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"gᵢ"},{"Type":"NodeText","Data":"​) 这两种信息，使得查询意图更加明确。"}]}]}]}]},{"ID":"20250808151246-4vyem8l","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808151246-4vyem8l","updated":"20250808160320"},"Children":[{"ID":"20250808151246-vhckwgd","Type":"NodeParagraph","Properties":{"id":"20250808151246-vhckwgd","updated":"20250808151246"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"第二步：计算注意力权重 (Attention Weights)"}]},{"ID":"20250808151246-plbec49","Type":"NodeList","ListData":{},"Properties":{"id":"20250808151246-plbec49","updated":"20250808160320"},"Children":[{"ID":"20250808151246-24n255w","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808151246-24n255w","updated":"20250808160320"},"Children":[{"ID":"20250808151246-ro1owws","Type":"NodeParagraph","Properties":{"id":"20250808151246-ro1owws","updated":"20250808160320"},"Children":[{"Type":"NodeText","Data":"​"},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"aᵢⱼˡ = softmax(dᵢˡ ⋅ zⱼᵘ)"},{"Type":"NodeText","Data":"​ 这是一个标准的"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"点积注意力"},{"Type":"NodeText","Data":"。它计算查询向量 "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"dᵢˡ"},{"Type":"NodeText","Data":"​ 和每个编码器顶层输出 "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"zⱼᵘ"},{"Type":"NodeText","Data":"​ 的相似度（点积），然后用"},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"softmax"},{"Type":"NodeText","Data":"​归一化成概率分布。这个分布 "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"aᵢⱼˡ"},{"Type":"NodeText","Data":"​ 就表示在解码器第 "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"l"},{"Type":"NodeText","Data":"​ 层的第 "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"i"},{"Type":"NodeText","Data":"​ 个位置，应该对输入序列的第 "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"j"},{"Type":"NodeText","Data":"​ 个词关注多少。"}]}]}]}]},{"ID":"20250808151246-8ahtxzj","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808151246-8ahtxzj","updated":"20250808160648"},"Children":[{"ID":"20250808151246-ztez2z3","Type":"NodeParagraph","Properties":{"id":"20250808151246-ztez2z3","updated":"20250808151246"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"第三步：计算上下文向量 (Context Vector)"}]},{"ID":"20250808151246-2tlq9qg","Type":"NodeList","ListData":{},"Properties":{"id":"20250808151246-2tlq9qg","updated":"20250808160648"},"Children":[{"ID":"20250808151246-sg3ftcc","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808151246-sg3ftcc","updated":"20250808160325"},"Children":[{"ID":"20250808151246-rbgw8q0","Type":"NodeParagraph","Properties":{"id":"20250808151246-rbgw8q0","updated":"20250808160325"},"Children":[{"Type":"NodeText","Data":"公式(2) "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"cᵢˡ = Σ aᵢⱼˡ(zⱼᵘ + eⱼ)"},{"Type":"NodeText","Data":"​ 是本文注意力的"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"最大创新点"},{"Type":"NodeText","Data":"。"}]}]},{"ID":"20250808151246-7t846eb","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808151246-7t846eb","updated":"20250808160330"},"Children":[{"ID":"20250808151246-2n9atsc","Type":"NodeParagraph","Properties":{"id":"20250808151246-2n9atsc","updated":"20250808160330"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"传统注意力"},{"Type":"NodeText","Data":"：计算的是 "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"Σ aᵢⱼˡ * zⱼᵘ"},{"Type":"NodeText","Data":"​，即对编码器的输出 "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"zⱼᵘ"},{"Type":"NodeText","Data":"​ 进行加权求和。"}]}]},{"ID":"20250808151246-ozw89lh","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808151246-ozw89lh","updated":"20250808160633"},"Children":[{"ID":"20250808151246-wwjz4va","Type":"NodeParagraph","Properties":{"id":"20250808151246-wwjz4va","updated":"20250808160633"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"本文的注意力"},{"Type":"NodeText","Data":"：计算的是 "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"Σ aᵢⱼˡ * (zⱼᵘ + eⱼ)"},{"Type":"NodeText","Data":"​，即对 "},{"Type":"NodeTextMark","TextMarkType":"strong code inline-math","TextMarkInlineMathContent":"zⱼᵘ + eⱼ"},{"Type":"NodeText","Data":" 进行加权求和。"}]}]},{"ID":"20250808151246-o0g7j1j","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808151246-o0g7j1j","updated":"20250808160642"},"Children":[{"ID":"20250808151246-zgo63yb","Type":"NodeParagraph","Properties":{"id":"20250808151246-zgo63yb","updated":"20250808160642"},"Children":[{"Type":"NodeText","Data":"​"},{"Type":"NodeTextMark","TextMarkType":"strong code inline-math","TextMarkInlineMathContent":"zⱼᵘ"},{"Type":"NodeText","Data":"​ 是编码器经过多层卷积后的输出，它包含了关于词 "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"j"},{"Type":"NodeText","Data":"​ 的"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"丰富的上下文信息"},{"Type":"NodeText","Data":"（比如“agree”在一个长句中的语义）。"}]}]},{"ID":"20250808151246-gkvs8w1","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808151246-gkvs8w1","updated":"20250808160646"},"Children":[{"ID":"20250808151246-z6run8z","Type":"NodeParagraph","Properties":{"id":"20250808151246-z6run8z","updated":"20250808160646"},"Children":[{"Type":"NodeText","Data":"​"},{"Type":"NodeTextMark","TextMarkType":"strong code inline-math","TextMarkInlineMathContent":"eⱼ"},{"Type":"NodeText","Data":"​ 是源端词 "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"j"},{"Type":"NodeText","Data":"​ 最初的"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"词嵌入+位置嵌入"},{"Type":"NodeText","Data":"，它代表了这个词本身"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"精确的、无歧义的“点信息”"},{"Type":"NodeText","Data":"（就是“agree”这个词和它的位置）。"}]}]},{"ID":"20250808151246-asmi432","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808151246-asmi432","updated":"20250808160648"},"Children":[{"ID":"20250808151246-7xn8ngp","Type":"NodeParagraph","Properties":{"id":"20250808151246-7xn8ngp","updated":"20250808160648"},"Children":[{"Type":"NodeText","Data":"将两者相加，使得解码器在检索信息时，既能得到经过深层处理的上下文感知信息 ("},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"zⱼᵘ"},{"Type":"NodeText","Data":"​"},{"Type":"NodeText","Data":")，又能得到原始的、未被稀释的词本身的信息 ("},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"eⱼ"},{"Type":"NodeText","Data":"​)。"}]}]}]}]}]}]}]}]},{"ID":"20250808151255-r9n5eky","Type":"NodeBlockquote","Properties":{"id":"20250808151255-r9n5eky","updated":"20250808160835"},"Children":[{"Type":"NodeBlockquoteMarker","Data":"\u003e"},{"ID":"20250808151304-vpe2639","Type":"NodeList","ListData":{"Typ":1},"Properties":{"id":"20250808151304-vpe2639","updated":"20250808160835"},"Children":[{"ID":"20250808151304-6r1ugsw","Type":"NodeListItem","ListData":{"Typ":1,"Delimiter":46,"Marker":"My4=","Num":3},"Properties":{"id":"20250808151304-6r1ugsw","updated":"20250808160835"},"Children":[{"ID":"20250808151304-ltbymcr","Type":"NodeParagraph","Properties":{"id":"20250808151304-ltbymcr","updated":"20250808151304"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"与键值记忆网络的类比 (Key-Value Memory Network Analogy)"}]},{"ID":"20250808151304-8epecob","Type":"NodeList","ListData":{},"Properties":{"id":"20250808151304-8epecob","updated":"20250808160835"},"Children":[{"ID":"20250808151304-gnyv8jd","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808151304-gnyv8jd","updated":"20250808160754"},"Children":[{"ID":"20250808151304-uwl4nq8","Type":"NodeParagraph","Properties":{"id":"20250808151304-uwl4nq8","updated":"20250808151304"},"Children":[{"Type":"NodeText","Data":"作者将这个机制类比为键值记忆网络，这是一个非常精妙的解释："}]},{"ID":"20250808151304-aum0dqj","Type":"NodeList","ListData":{},"Properties":{"id":"20250808151304-aum0dqj","updated":"20250808160754"},"Children":[{"ID":"20250808151304-mc7jzni","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808151304-mc7jzni","updated":"20250808160751"},"Children":[{"ID":"20250808151304-2k9gqsb","Type":"NodeParagraph","Properties":{"id":"20250808151304-2k9gqsb","updated":"20250808160751"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"键 (Keys)"},{"Type":"NodeText","Data":": "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"zⱼᵘ"},{"Type":"NodeText","Data":"​。解码器用它来进行“寻址”，即通过计算与查询向量的相似度来找到应该关注哪个源词。"}]}]},{"ID":"20250808151304-84f6aks","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808151304-84f6aks","updated":"20250808160754"},"Children":[{"ID":"20250808151304-9vedabt","Type":"NodeParagraph","Properties":{"id":"20250808151304-9vedabt","updated":"20250808160754"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"值 (Values)"},{"Type":"NodeText","Data":": "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"zⱼᵘ + eⱼ"},{"Type":"NodeText","Data":"​。一旦找到了要关注的词（通过键），模型实际取回的是这个更丰富的值。"}]}]}]}]},{"ID":"20250808151304-9irifl9","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808151304-9irifl9","updated":"20250808160835"},"Children":[{"ID":"20250808151304-5rxshzq","Type":"NodeParagraph","Properties":{"id":"20250808151304-5rxshzq","updated":"20250808160835"},"Children":[{"Type":"NodeText","Data":"这种设计使得"},{"Type":"NodeTextMark","TextMarkType":"strong mark","TextMarkTextContent":"“用于匹配的表示”和“用于传递的表示”可以不同"},{"Type":"NodeText","Data":"，增加了模型的灵活性和表达能力。"}]}]}]}]}]}]},{"ID":"20250808151308-7egjxuk","Type":"NodeBlockquote","Properties":{"fold":"1","id":"20250808151308-7egjxuk","updated":"20250808151318"},"Children":[{"Type":"NodeBlockquoteMarker","Data":"\u003e"},{"ID":"20250808151318-75cl637","Type":"NodeList","ListData":{"Typ":1},"Properties":{"id":"20250808151318-75cl637","updated":"20250808151318"},"Children":[{"ID":"20250808151318-qran58f","Type":"NodeListItem","ListData":{"Typ":1,"Delimiter":46,"Marker":"NC4=","Num":4},"Properties":{"id":"20250808151318-qran58f","updated":"20250808151318"},"Children":[{"ID":"20250808151318-45jrpws","Type":"NodeParagraph","Properties":{"id":"20250808151318-45jrpws","updated":"20250808151318"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"整合注意力信息"}]},{"ID":"20250808151318-21vwtsr","Type":"NodeList","ListData":{},"Properties":{"id":"20250808151318-21vwtsr","updated":"20250808151318"},"Children":[{"ID":"20250808151318-wu1caeg","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808151318-wu1caeg","updated":"20250808151318"},"Children":[{"ID":"20250808151318-h6f6fa0","Type":"NodeParagraph","Properties":{"id":"20250808151318-h6f6fa0","updated":"20250808151318"},"Children":[{"Type":"NodeText","Data":"计算出的上下文向量 "},{"Type":"NodeTextMark","TextMarkType":"code","TextMarkTextContent":"cᵢˡ"},{"Type":"NodeText","Data":"​ 并不会直接进入下一层，而是被"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"直接加到"},{"Type":"NodeText","Data":"当前解码器层的输出 "},{"Type":"NodeTextMark","TextMarkType":"code","TextMarkTextContent":"hᵢˡ"},{"Type":"NodeText","Data":"​ 上。"}]}]},{"ID":"20250808151318-43l11fk","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808151318-43l11fk","updated":"20250808151318"},"Children":[{"ID":"20250808151318-pub64ue","Type":"NodeParagraph","Properties":{"id":"20250808151318-pub64ue","updated":"20250808151318"},"Children":[{"Type":"NodeText","Data":"所以，解码器第 "},{"Type":"NodeTextMark","TextMarkType":"code","TextMarkTextContent":"l"},{"Type":"NodeText","Data":"​ 层最终传递给第 "},{"Type":"NodeTextMark","TextMarkType":"code","TextMarkTextContent":"l+1"},{"Type":"NodeText","Data":"​ 层的输出是 "},{"Type":"NodeTextMark","TextMarkType":"code","TextMarkTextContent":"hᵢˡ + cᵢˡ"},{"Type":"NodeText","Data":"​。这是一种类似残差连接的方式，将从源端获取的对齐信息直接注入到解码流程中。"}]}]}]}]}]}]},{"ID":"20250808160920-b4mmtdb","Type":"NodeBlockquote","Properties":{"id":"20250808160920-b4mmtdb","updated":"20250808161136"},"Children":[{"Type":"NodeBlockquoteMarker","Data":"\u003e"},{"ID":"20250808160923-hh1q11y","Type":"NodeParagraph","Properties":{"id":"20250808160923-hh1q11y","updated":"20250808161136"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"个人感悟："},{"Type":"NodeText","Data":"有的信息需要分割，取其部分以更好地进行含义处理；有的信息需要合并，以统合成更高维度的语义信息"}]}]},{"ID":"20250808151613-pk0tj3x","Type":"NodeParagraph","Properties":{"id":"20250808151613-pk0tj3x","updated":"20250808161639"},"Children":[{"Type":"NodeText","Data":"这可以被看作是与单步注意力（Bahdanau 等人, 2014; Luong 等人, 2015; Zhou 等人, 2016; Wu 等人, 2016）相比、具有多次“跳跃”（hops）的注意力（Sukhbaatar 等人, 2015）。具体而言，第一层的注意力决定了一个有用的源端上下文，这个上下文随后被送入第二层，而第二层在计算其自身的注意力时会考虑这些信息，以此类推。解码器还能立即访问前 "},{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":"k-1"},{"Type":"NodeText","Data":" 个时间步的注意力历史，因为条件输入 "},{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":"cᵢ₋ₖˡ⁻¹, ..., cᵢˡ⁻¹"},{"Type":"NodeText","Data":" 是 "},{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":"hᵢ₋ₖˡ⁻¹, ..., hᵢˡ⁻¹"},{"Type":"NodeText","Data":" 的一部分，而后者是计算 "},{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":"hᵢˡ"},{"Type":"NodeText","Data":" 的输入。与循环网络相比，这使得模型更容易考虑到哪些先前的输入已经被关注过；在循环网络中，这些信息存在于循环状态中，需要经过数个非线性变换才能存活下来。总的来说，我们的注意力机制会考虑我们之前关注过哪些词（Yang 等人, 2016），并在每个时间步执行多次注意力“跳跃”。在附录§C中，我们绘制了一个深度解码器的注意力分数图，并展示了在不同层级，源句的不同部分会被关注到。"}]},{"ID":"20250808151613-mstctan","Type":"NodeParagraph","Properties":{"id":"20250808151613-mstctan","updated":"20250808151613"},"Children":[{"Type":"NodeText","Data":"与RNNs相比，我们的卷积架构还允许对序列中所有元素的注意力计算进行批处理（图1，中间部分）。我们对每个解码器层的计算进行单独的批处理。"}]},{"ID":"20250808151620-2hewsot","Type":"NodeBlockquote","Properties":{"id":"20250808151620-2hewsot","updated":"20250808151623"},"Children":[{"Type":"NodeBlockquoteMarker","Data":"\u003e"},{"ID":"20250808151622-jc1n48o","Type":"NodeParagraph","Properties":{"id":"20250808151622-jc1n48o","updated":"20250808151623"},"Children":[{"Type":"NodeText","Data":"这段文字深入阐述了本文提出的“多步注意力”机制的两个核心优势：一是其概念上的强大性（通过多级“跳跃”和显式的注意力历史），二是其计算上的高效性（完全可并行）。"}]}]},{"ID":"20250808151624-gpz4kqn","Type":"NodeBlockquote","Properties":{"fold":"1","id":"20250808151624-gpz4kqn","updated":"20250808151637"},"Children":[{"Type":"NodeBlockquoteMarker","Data":"\u003e"},{"ID":"20250808151637-j8q07i4","Type":"NodeList","ListData":{"Typ":1},"Properties":{"id":"20250808151637-j8q07i4","updated":"20250808151637"},"Children":[{"ID":"20250808151637-xxg5f9z","Type":"NodeListItem","ListData":{"Typ":1,"Delimiter":46,"Marker":"MS4=","Num":1},"Properties":{"id":"20250808151637-xxg5f9z","updated":"20250808151637"},"Children":[{"ID":"20250808151637-tm9wi6k","Type":"NodeParagraph","Properties":{"id":"20250808151637-tm9wi6k","updated":"20250808151637"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"概念优势：多级“跳跃”与分层注意力 (Multiple Hops \u0026amp; Hierarchical Attention)"}]},{"ID":"20250808151637-2rq28du","Type":"NodeList","ListData":{},"Properties":{"id":"20250808151637-2rq28du","updated":"20250808151637"},"Children":[{"ID":"20250808151637-izdi2wl","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808151637-izdi2wl","updated":"20250808151637"},"Children":[{"ID":"20250808151637-6wqzqsm","Type":"NodeParagraph","Properties":{"id":"20250808151637-6wqzqsm","updated":"20250808151637"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"多级跳跃 (Multiple Hops)"},{"Type":"NodeText","Data":"：将每一层解码器都执行一次注意力计算的过程，比作多次“跳跃”。这不仅仅是重复，而是一个"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"逐步求精"},{"Type":"NodeText","Data":"的过程。"}]},{"ID":"20250808151637-1xhwu2s","Type":"NodeList","ListData":{},"Properties":{"id":"20250808151637-1xhwu2s","updated":"20250808151637"},"Children":[{"ID":"20250808151637-hfjhw4k","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808151637-hfjhw4k","updated":"20250808151637"},"Children":[{"ID":"20250808151637-xrtst2u","Type":"NodeParagraph","Properties":{"id":"20250808151637-xrtst2u","updated":"20250808151637"},"Children":[{"Type":"NodeText","Data":"第一层解码器（第一次跳跃）根据初步的解码状态，从源句中提取一个初步的上下文。"}]}]},{"ID":"20250808151637-955gc36","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808151637-955gc36","updated":"20250808151637"},"Children":[{"ID":"20250808151637-wmh0pxo","Type":"NodeParagraph","Properties":{"id":"20250808151637-wmh0pxo","updated":"20250808151637"},"Children":[{"Type":"NodeText","Data":"第二层解码器在进行自己的注意力计算（第二次跳跃）时，其输入已经包含了第一层提取的上下文信息。因此，它可以"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"基于第一层的结果"},{"Type":"NodeText","Data":"进行更精细、更准确的对齐。"}]}]},{"ID":"20250808151637-jc7qrw1","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808151637-jc7qrw1","updated":"20250808151637"},"Children":[{"ID":"20250808151637-wm0db1w","Type":"NodeParagraph","Properties":{"id":"20250808151637-wm0db1w","updated":"20250808151637"},"Children":[{"Type":"NodeText","Data":"以此类推，每一层都在前一层的基础上优化对源句的理解和对齐。"}]}]}]}]},{"ID":"20250808151637-z4ry6o1","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808151637-z4ry6o1","updated":"20250808151637"},"Children":[{"ID":"20250808151637-rfsv86r","Type":"NodeParagraph","Properties":{"id":"20250808151637-rfsv86r","updated":"20250808151637"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"分层注意力 (Hierarchical Attention)"},{"Type":"NodeText","Data":"：这种多级跳跃机制自然地导致了分层注意力的出现。论文附录的结果表明，"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"不同的解码器层学会了关注源句的不同方面"},{"Type":"NodeText","Data":"。例如，底层可能更关注词法或句法上的对齐（比如对齐动词和名词），而高层则可能关注更抽象的语义或语篇关系。"}]}]}]}]}]}]},{"ID":"20250808151651-ruv0392","Type":"NodeBlockquote","Properties":{"fold":"1","id":"20250808151651-ruv0392","updated":"20250808162118"},"Children":[{"Type":"NodeBlockquoteMarker","Data":"\u003e"},{"ID":"20250808151656-9ke8zn5","Type":"NodeList","ListData":{"Typ":1},"Properties":{"id":"20250808151656-9ke8zn5","updated":"20250808162118"},"Children":[{"ID":"20250808151656-7e5c394","Type":"NodeListItem","ListData":{"Typ":1,"Delimiter":46,"Marker":"Mi4=","Num":2},"Properties":{"id":"20250808151656-7e5c394","updated":"20250808162118"},"Children":[{"ID":"20250808151656-y7t8me6","Type":"NodeParagraph","Properties":{"id":"20250808151656-y7t8me6","updated":"20250808151656"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"概念优势：显式的注意力历史 (Explicit Attention History)"}]},{"ID":"20250808151656-3ra6qjj","Type":"NodeList","ListData":{},"Properties":{"id":"20250808151656-3ra6qjj","updated":"20250808162118"},"Children":[{"ID":"20250808151656-4vj1xa7","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808151656-4vj1xa7","updated":"20250808162108"},"Children":[{"ID":"20250808151656-mqtcw69","Type":"NodeParagraph","Properties":{"id":"20250808151656-mqtcw69","updated":"20250808162108"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"ConvS2S 的实现"},{"Type":"NodeText","Data":"：在一个卷积块 "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"l"},{"Type":"NodeText","Data":"​ 中，计算 "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"hᵢˡ"},{"Type":"NodeText","Data":"​ 时，它的输入是前一层 "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"hˡ⁻¹"},{"Type":"NodeText","Data":"​ 的一个窗口。而 "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"hˡ⁻¹"},{"Type":"NodeText","Data":"​ 本身是由 "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"(卷积输出) + cˡ⁻¹"},{"Type":"NodeText","Data":"​ 构成的，其中 "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"cˡ⁻¹"},{"Type":"NodeText","Data":"​ 是前一层的注意力上下文向量。这意味着，"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"前一层的注意力结果"},{"Type":"NodeText","Data":" "},{"Type":"NodeTextMark","TextMarkType":"strong code inline-math","TextMarkInlineMathContent":"cˡ⁻¹"},{"Type":"NodeText","Data":"​ "},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"被直接、完整地作为输入"},{"Type":"NodeText","Data":"，参与到当前层 "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"l"},{"Type":"NodeText","Data":"​ 的计算中。"}]}]},{"ID":"20250808151656-zp08skq","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808151656-zp08skq","updated":"20250808162118"},"Children":[{"ID":"20250808151656-dabd7ew","Type":"NodeParagraph","Properties":{"id":"20250808151656-dabd7ew","updated":"20250808162118"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"与 RNN 的对比"},{"Type":"NodeText","Data":"：在 RNN 中，关于过去注意了哪里的历史信息，是被"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"压缩并编码"},{"Type":"NodeText","Data":"进单一的循环隐藏状态 "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"h"},{"Type":"NodeText","Data":"​ 中的。这些信息必须在 RNN 单元的各种非线性门控（如 "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"tanh"},{"Type":"NodeText","Data":"​）中“存活”下来，很容易被稀释或遗忘。"}]}]},{"ID":"20250808151656-bm0f1gj","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808151656-bm0f1gj","updated":"20250808151656"},"Children":[{"ID":"20250808151656-zzodsjx","Type":"NodeParagraph","Properties":{"id":"20250808151656-zzodsjx","updated":"20250808151656"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"优势"},{"Type":"NodeText","Data":"：ConvS2S 的设计使得注意力历史变得"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"非常显式和直接"},{"Type":"NodeText","Data":"。这让模型更容易学习复杂的注意力模式，例如“我已经翻译过源句的这个部分了，现在应该关注其他部分”，从而避免重复翻译或漏掉信息。"}]}]}]}]}]}]},{"ID":"20250808151707-i9wpelb","Type":"NodeBlockquote","Properties":{"id":"20250808151707-i9wpelb","updated":"20250808162241"},"Children":[{"Type":"NodeBlockquoteMarker","Data":"\u003e"},{"ID":"20250808151709-j45z1es","Type":"NodeList","ListData":{"Typ":1},"Properties":{"id":"20250808151709-j45z1es","updated":"20250808162241"},"Children":[{"ID":"20250808151709-hb2tu33","Type":"NodeListItem","ListData":{"Typ":1,"Delimiter":46,"Marker":"My4=","Num":3},"Properties":{"id":"20250808151709-hb2tu33","updated":"20250808162241"},"Children":[{"ID":"20250808151709-4d9k6l6","Type":"NodeParagraph","Properties":{"id":"20250808151709-4d9k6l6","updated":"20250808151709"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"计算优势：注意力的并行化 (Parallel Attention Computation)"}]},{"ID":"20250808151709-s8b0ho7","Type":"NodeList","ListData":{},"Properties":{"id":"20250808151709-s8b0ho7","updated":"20250808162241"},"Children":[{"ID":"20250808151709-08uydim","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808151709-08uydim","updated":"20250808151709"},"Children":[{"ID":"20250808151709-29sheo7","Type":"NodeParagraph","Properties":{"id":"20250808151709-29sheo7","updated":"20250808151709"},"Children":[{"Type":"NodeText","Data":"这是卷积架构带来的最直接的好处。"}]}]},{"ID":"20250808151709-p2u2o6v","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808151709-p2u2o6v","updated":"20250808162241"},"Children":[{"ID":"20250808151709-utiz9q5","Type":"NodeParagraph","Properties":{"id":"20250808151709-utiz9q5","updated":"20250808162241"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"RNN 的瓶颈"},{"Type":"NodeText","Data":"：RNN 解码器是自回归的，必须先生成 "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"y₁"},{"Type":"NodeText","Data":"​，再计算 "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"y₂"},{"Type":"NodeText","Data":"​ 的注意力，再生成 "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"y₂"},{"Type":"NodeText","Data":"​，以此类推。注意力的计算是"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"按时间步串行"},{"Type":"NodeText","Data":"的，无法并行。"}]}]},{"ID":"20250808151709-awi4hki","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808151709-awi4hki","updated":"20250808151709"},"Children":[{"ID":"20250808151709-v0eh1ty","Type":"NodeParagraph","Properties":{"id":"20250808151709-v0eh1ty","updated":"20250808151709"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"ConvS2S 的优势"},{"Type":"NodeText","Data":"：由于解码器的所有位置都是一次性通过卷积网络计算的，因此"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"所有位置的注意力也可以并行计算"},{"Type":"NodeText","Data":"。如 "},{"Type":"NodeTextMark","TextMarkType":"code","TextMarkTextContent":"图1"},{"Type":"NodeText","Data":"​ 所示，模型可以一次性为 \"Sie\", \"stimmen\", \"zu\" 这三个目标词，同时计算出它们各自对于源句的注意力分布。"}]}]},{"ID":"20250808151709-ybb416b","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808151709-ybb416b","updated":"20250808151709"},"Children":[{"ID":"20250808151709-wv3n1j5","Type":"NodeParagraph","Properties":{"id":"20250808151709-wv3n1j5","updated":"20250808151709"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"批处理 (Batching)"},{"Type":"NodeText","Data":"：文中的“对每个解码器层的计算进行单独的批处理”指的就是这种并行性。计算是逐层进行的：整个序列在第一层被并行处理完，然后结果被传递给第二层，再次被并行处理，直到最后一层。"}]}]}]}]}]}]},{"ID":"20250808151846-79adu2r","Type":"NodeHeading","HeadingLevel":3,"Properties":{"id":"20250808151846-79adu2r","updated":"20250809090141"},"Children":[{"Type":"NodeText","Data":"3.4 归一化策略"}]},{"ID":"20250808151858-orlujl3","Type":"NodeParagraph","Properties":{"id":"20250808151858-orlujl3","updated":"20250808162808"},"Children":[{"Type":"NodeText","Data":"我们通过精细的权重初始化（§3.5）以及对网络部分输出进行缩放来稳定学习过程，以确保整个网络的方差不会发生剧烈变化。具体来说，我们对残差块的输出以及注意力的输出进行缩放，以保持激活值的方差。我们将一个残差块的输入与输出之和乘以 "},{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":"\\sqrt{0.5}"},{"Type":"NodeText","Data":"，从而使该和的方差减半。这个做法假设了被相加的两者具有相同的方差，这虽然不总是成立，但在实践中是有效的。"}]},{"ID":"20250808151858-igiq5ub","Type":"NodeParagraph","Properties":{"id":"20250808151858-igiq5ub","updated":"20250808162456"},"Children":[{"Type":"NodeText","Data":"由注意力机制生成的条件输入 "},{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":"cᵢˡ"},{"Type":"NodeText","Data":" 是 "},{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":"m"},{"Type":"NodeText","Data":" 个向量的加权和（公式2），我们通过缩放来抵消方差的变化；我们（的等效做法是）将加权和乘以 "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"m×\\sqrt{\\frac{1}{m}}"},{"Type":"NodeText","Data":"​ (即 "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"\\sqrt{m}"},{"Type":"NodeText","Data":"​)，以将其（方差）提升回原始大小，这里假设了注意力分数是均匀分布的。这在通常情况下并不成立，但我们发现在实践中效果很好。"}]},{"ID":"20250808151858-xz4pdnd","Type":"NodeParagraph","Properties":{"id":"20250808151858-xz4pdnd","updated":"20250808151858"},"Children":[{"Type":"NodeText","Data":"对于具有多重注意力的卷积解码器，我们用所使用的注意力机制的数量来缩放反向传播到编码器层的梯度；我们排除了对源端词嵌入的梯度缩放。我们发现这样做可以稳定学习，因为若不如此，编码器会接收到过大的梯度。"}]},{"ID":"20250808151911-3520efg","Type":"NodeBlockquote","Properties":{"id":"20250808151911-3520efg","updated":"20250808151913"},"Children":[{"Type":"NodeBlockquoteMarker","Data":"\u003e"},{"ID":"20250808151912-nhc71o8","Type":"NodeParagraph","Properties":{"id":"20250808151912-nhc71o8","updated":"20250808151913"},"Children":[{"Type":"NodeText","Data":"这一节描述了为了成功训练一个非常深的"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"全卷积"},{"Type":"NodeText","Data":"序列到序列模型所采用的关键工程技巧——"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"归一化策略"},{"Type":"NodeText","Data":"。这些策略的核心目标是控制网络中信号（激活值）和梯度（误差）的“强度”（即方差），防止它们在深层传播中爆炸或消失，从而稳定整个训练过程。"}]}]},{"ID":"20250808151915-dt16vxb","Type":"NodeBlockquote","Properties":{"fold":"1","id":"20250808151915-dt16vxb","updated":"20250808162830"},"Children":[{"Type":"NodeBlockquoteMarker","Data":"\u003e"},{"ID":"20250808151929-lyhzfog","Type":"NodeList","ListData":{"Typ":1},"Properties":{"id":"20250808151929-lyhzfog","updated":"20250808162830"},"Children":[{"ID":"20250808151929-uksktnz","Type":"NodeListItem","ListData":{"Typ":1,"Delimiter":46,"Marker":"MS4=","Num":1},"Properties":{"id":"20250808151929-uksktnz","updated":"20250808162830"},"Children":[{"ID":"20250808151929-7u27ae4","Type":"NodeParagraph","Properties":{"id":"20250808151929-7u27ae4","updated":"20250808151929"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"残差块输出的缩放 (Scaling Residual Blocks)"}]},{"ID":"20250808151929-21xjtyk","Type":"NodeList","ListData":{},"Properties":{"id":"20250808151929-21xjtyk","updated":"20250808162830"},"Children":[{"ID":"20250808151929-pma6wbk","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808151929-pma6wbk","updated":"20250808162735"},"Children":[{"ID":"20250808151929-hbxwuef","Type":"NodeParagraph","Properties":{"id":"20250808151929-hbxwuef","updated":"20250808162735"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"问题"},{"Type":"NodeText","Data":"：在一个标准的残差连接中，输出是 "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"F(x) + x"},{"Type":"NodeText","Data":"​。如果输入 "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"x"},{"Type":"NodeText","Data":"​ 和块的输出 "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"F(x)"},{"Type":"NodeText","Data":"​ 的方差大致相等（比如都为 "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"σ²"},{"Type":"NodeText","Data":"​），那么两者相加后的方差大约会变成 "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"2σ²"},{"Type":"NodeText","Data":"​。这意味着每经过一个残差块，信号的方差就会翻倍。在一个很深的网络里，这会导致激活值爆炸。"}]}]},{"ID":"20250808151929-jhlsrxj","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808151929-jhlsrxj","updated":"20250808162813"},"Children":[{"ID":"20250808151929-hbt1c0j","Type":"NodeParagraph","Properties":{"id":"20250808151929-hbt1c0j","updated":"20250808162813"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"解决方案"},{"Type":"NodeText","Data":"：将残差块的最终输出 "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"F(x) + x"},{"Type":"NodeText","Data":"​ 乘以一个缩放因子 "},{"Type":"NodeTextMark","TextMarkType":"strong code inline-math","TextMarkInlineMathContent":"\\sqrt{0.5}"},{"Type":"NodeText","Data":"​。"}]}]},{"ID":"20250808151929-kndrv9l","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808151929-kndrv9l","updated":"20250808162830"},"Children":[{"ID":"20250808151929-ftdsde5","Type":"NodeParagraph","Properties":{"id":"20250808151929-ftdsde5","updated":"20250808162830"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"原理"},{"Type":"NodeText","Data":"：根据方差的性质 "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"Var(aX) = a²Var(X)"},{"Type":"NodeText","Data":"​，乘以 "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"\\sqrt{0.5}"},{"Type":"NodeText","Data":"​ 后，新的方差就变成了 "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"(\\sqrt{0.5})² * (2σ²) = 0.5 * 2σ² = σ²"},{"Type":"NodeText","Data":"​。这样，信号的方差在通过残差块后得以保持稳定，使得网络可以堆叠得非常深。"}]}]}]}]}]}]},{"ID":"20250808151941-b3nds83","Type":"NodeBlockquote","Properties":{"fold":"1","id":"20250808151941-b3nds83","updated":"20250808163122"},"Children":[{"Type":"NodeBlockquoteMarker","Data":"\u003e"},{"ID":"20250808151953-uh1l5bc","Type":"NodeList","ListData":{"Typ":1},"Properties":{"id":"20250808151953-uh1l5bc","updated":"20250808163122"},"Children":[{"ID":"20250808151953-lx87k8y","Type":"NodeListItem","ListData":{"Typ":1,"Delimiter":46,"Marker":"Mi4=","Num":2},"Properties":{"id":"20250808151953-lx87k8y","updated":"20250808163122"},"Children":[{"ID":"20250808151953-vkgquka","Type":"NodeParagraph","Properties":{"id":"20250808151953-vkgquka","updated":"20250808151953"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"注意力输出的缩放 (Scaling Attention Outputs)"}]},{"ID":"20250808151953-3lpl0mt","Type":"NodeList","ListData":{},"Properties":{"id":"20250808151953-3lpl0mt","updated":"20250808163122"},"Children":[{"ID":"20250808151953-7pyfwuk","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808151953-7pyfwuk","updated":"20250808162915"},"Children":[{"ID":"20250808151953-qwhj2m6","Type":"NodeParagraph","Properties":{"id":"20250808151953-qwhj2m6","updated":"20250808162915"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"问题"},{"Type":"NodeText","Data":"：注意力机制的输出 "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"c"},{"Type":"NodeText","Data":"​ 是 "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"m"},{"Type":"NodeText","Data":"​ 个编码器输出向量的加权和 "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"c = Σ aⱼ * vⱼ"},{"Type":"NodeText","Data":"​"},{"Type":"NodeText","Data":"。如果注意力权重是均匀的（即每个权重 "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"aⱼ"},{"Type":"NodeText","Data":"​ 都是 "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"1/m"},{"Type":"NodeText","Data":"​"},{"Type":"NodeText","Data":"），那么加权和的方差大约会是原始向量方差的 "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"1/m"},{"Type":"NodeText","Data":"。这意味着经过注意力层后，信号的方差会显著减小，这在深层网络中同样是有害的。"}]}]},{"ID":"20250808151953-jse0um7","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808151953-jse0um7","updated":"20250808163049"},"Children":[{"ID":"20250808151953-v1a3no4","Type":"NodeParagraph","Properties":{"id":"20250808151953-v1a3no4","updated":"20250808163049"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"解决方案"},{"Type":"NodeText","Data":"：将注意力计算出的上下文向量 "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"c"},{"Type":"NodeText","Data":"​ 乘以一个缩放因子 "},{"Type":"NodeTextMark","TextMarkType":"strong code inline-math","TextMarkInlineMathContent":"\\sqrt{m}"},{"Type":"NodeText","Data":"​（文中 "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"m*\\sqrt{1/m} = \\sqrt{m}"},{"Type":"NodeText","Data":"​），其中 "},{"Type":"NodeTextMark","TextMarkType":"code","TextMarkTextContent":"m"},{"Type":"NodeText","Data":"​ 是源序列的长度。"}]}]},{"ID":"20250808151953-wir2t7d","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808151953-wir2t7d","updated":"20250808163122"},"Children":[{"ID":"20250808151953-k8hfs8m","Type":"NodeParagraph","Properties":{"id":"20250808151953-k8hfs8m","updated":"20250808163122"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"原理"},{"Type":"NodeText","Data":"：和上面类似，乘以 "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"\\sqrt{m}"},{"Type":"NodeText","Data":"​ 后，新的方差就变成了 "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"(\\sqrt{m})² * (σ²/m) = m * (σ²/m) = σ²"},{"Type":"NodeText","Data":"​"},{"Type":"NodeText","Data":"。这样也保持了方差的稳定。"}]}]}]}]}]}]},{"ID":"20250808151958-0ye6hln","Type":"NodeBlockquote","Properties":{"fold":"1","id":"20250808151958-0ye6hln","updated":"20250808163159"},"Children":[{"Type":"NodeBlockquoteMarker","Data":"\u003e"},{"ID":"20250808152009-ne0upp5","Type":"NodeList","ListData":{"Typ":1},"Properties":{"id":"20250808152009-ne0upp5","updated":"20250808163159"},"Children":[{"ID":"20250808152009-snl93ya","Type":"NodeListItem","ListData":{"Typ":1,"Delimiter":46,"Marker":"My4=","Num":3},"Properties":{"id":"20250808152009-snl93ya","updated":"20250808163159"},"Children":[{"ID":"20250808152009-nuv6zdm","Type":"NodeParagraph","Properties":{"id":"20250808152009-nuv6zdm","updated":"20250808152009"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"编码器梯度的缩放 (Scaling Encoder Gradients)"}]},{"ID":"20250808152009-v29iksr","Type":"NodeList","ListData":{},"Properties":{"id":"20250808152009-v29iksr","updated":"20250808163159"},"Children":[{"ID":"20250808152009-owihyx8","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808152009-owihyx8","updated":"20250808163143"},"Children":[{"ID":"20250808152009-g0xrmb4","Type":"NodeParagraph","Properties":{"id":"20250808152009-g0xrmb4","updated":"20250808163143"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"问题"},{"Type":"NodeText","Data":"：本文的解码器有 "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"L"},{"Type":"NodeText","Data":"​ 层，每一层都有一个独立的注意力机制。在反向传播时，这 "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"L"},{"Type":"NodeText","Data":"​ 个注意力机制"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"都会"},{"Type":"NodeText","Data":"将梯度传回到编码器。这意味着编码器会从 "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"L"},{"Type":"NodeText","Data":"​ 个不同的“源头”接收梯度。如果解码器很深（"},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"L"},{"Type":"NodeText","Data":"​ 很大），累积起来的梯度总量就会非常巨大，导致编码器部分的训练非常不稳定。"}]}]},{"ID":"20250808152009-3sltrxs","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808152009-3sltrxs","updated":"20250808163148"},"Children":[{"ID":"20250808152009-2lxrqi8","Type":"NodeParagraph","Properties":{"id":"20250808152009-2lxrqi8","updated":"20250808163148"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"解决方案"},{"Type":"NodeText","Data":"：将反向传播到编码器的梯度"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"除以注意力机制的数量"},{"Type":"NodeText","Data":" "},{"Type":"NodeTextMark","TextMarkType":"strong code inline-math","TextMarkInlineMathContent":"L"},{"Type":"NodeText","Data":"​。"}]}]},{"ID":"20250808152009-bef6flg","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808152009-bef6flg","updated":"20250808152009"},"Children":[{"ID":"20250808152009-iuc8el5","Type":"NodeParagraph","Properties":{"id":"20250808152009-iuc8el5","updated":"20250808152009"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"效果"},{"Type":"NodeText","Data":"：这相当于对来自不同解码器层的梯度做了一个平均，有效地控制了流入编码器的总梯度的大小，从而稳定了编码器的训练过程。这是一个针对本文“多步注意力”架构非常关键的、定制化的归一化技巧。"}]}]},{"ID":"20250808152009-79flaw3","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808152009-79flaw3","updated":"20250808163159"},"Children":[{"ID":"20250808152009-xgx0pq8","Type":"NodeParagraph","Properties":{"id":"20250808152009-xgx0pq8","updated":"20250808163159"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"细节"},{"Type":"NodeText","Data":"：作者特别提到，这个缩放"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"不适用"},{"Type":"NodeText","Data":"于源端词嵌入（"},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"eⱼ"},{"Type":"NodeText","Data":"​）的梯度。缩放只针对编码器卷积层输出（"},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"zⱼ"},{"Type":"NodeText","Data":"​）的梯度，因为后者才是深层网络的主体，最容易受到梯度爆炸的影响。"}]}]}]}]}]}]},{"ID":"20250808152156-36zqc2u","Type":"NodeHeading","HeadingLevel":3,"Properties":{"id":"20250808152156-36zqc2u","updated":"20250809090141"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"3.5. 初始化"}]},{"ID":"20250808152200-yx81a6k","Type":"NodeParagraph","Properties":{"id":"20250808152200-yx81a6k","updated":"20250808152206"},"Children":[{"Type":"NodeText","Data":"在添加不同层的输出时（例如残差连接），对激活值进行归一化需要精细的权重初始化。我们初始化的动机与归一化相同：在前向和反向传播过程中保持激活值的方差稳定。"}]},{"ID":"20250808152200-35ixs25","Type":"NodeParagraph","Properties":{"id":"20250808152200-35ixs25","updated":"20250808163545"},"Children":[{"Type":"NodeText","Data":"所有的嵌入都从一个均值为0、标准差为0.1的正态分布中初始化。对于其输出不直接送入一个门控线性单元的层，我们从 "},{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":"\\mathcal{N}(0,\\sqrt{1/n_l})"},{"Type":"NodeText","Data":" 中初始化权重，其中 nₗ 是每个神经元的输入连接数。这确保了一个正态分布输入的方差得以保持。"}]},{"ID":"20250808152200-krc4c1n","Type":"NodeParagraph","Properties":{"id":"20250808152200-krc4c1n","updated":"20250808163630"},"Children":[{"Type":"NodeText","Data":"对于其后跟随一个GLU激活的层，我们通过调整（He等人，2015b；Glorot \u0026 Bengio，2010）中的推导，提出了一个权重初始化方案（附录A）。如果GLU的输入是均值为0且具有足够小方差的分布，那么我们可以用输入方差的1/4来近似其输出方差（附录A.1）。因此，我们初始化权重，使得送入GLU激活的输入的方差是该层输入的4倍。这是通过从 "},{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":"\\mathcal{N}(0,\\sqrt{4/n_l})"},{"Type":"NodeText","Data":" 中抽取初始值来实现的。在构建网络时，偏置被统一设置为零。"}]},{"ID":"20250808152200-71jvgxb","Type":"NodeParagraph","Properties":{"id":"20250808152200-71jvgxb","updated":"20250808163701"},"Children":[{"Type":"NodeText","Data":"我们对某些层的输入应用dropout，使得输入以概率 p 被保留。这可以看作是与一个伯努利随机变量相乘，该变量以概率 "},{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":"p"},{"Type":"NodeText","Data":" 取值 "},{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":"1/p"},{"Type":"NodeText","Data":"，否则取值为0（Srivastava 等人, 2014）。应用dropout将导致方差被缩放 "},{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":"1/p"},{"Type":"NodeText","Data":"。我们的目标是通过用更大的权重来初始化相应层来恢复传入的方差。具体来说，对于其输出会经过一个GLU的层，我们使用 "},{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":"\\mathcal{N}(0,\\sqrt{4p/n_l})"},{"Type":"NodeText","Data":"，否则使用 "},{"Type":"NodeTextMark","TextMarkType":"strong inline-math","TextMarkInlineMathContent":"\\mathcal{N}(0,\\sqrt{p/n_l})"},{"Type":"NodeText","Data":"（附录A.3）。"}]},{"ID":"20250808152218-mjsnekb","Type":"NodeBlockquote","Properties":{"id":"20250808152218-mjsnekb","updated":"20250808152221"},"Children":[{"Type":"NodeBlockquoteMarker","Data":"\u003e"},{"ID":"20250808152220-mw3yb53","Type":"NodeParagraph","Properties":{"id":"20250808152220-mw3yb53","updated":"20250808152221"},"Children":[{"Type":"NodeText","Data":"这一节详细介绍了为稳定深度卷积网络的训练而设计的复杂权重初始化策略。其核心思想是"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"方差保持"},{"Type":"NodeText","Data":"（Variance Preservation），即确保信息（激活值）和梯度在前向和反向传播通过多层网络时，其方差（或说“能量”）既不爆炸也不消失。"}]}]},{"ID":"20250808152222-4c2onlr","Type":"NodeBlockquote","Properties":{"fold":"1","id":"20250808152222-4c2onlr","updated":"20250808152240"},"Children":[{"Type":"NodeBlockquoteMarker","Data":"\u003e"},{"ID":"20250808152240-qcnbt22","Type":"NodeList","ListData":{"Typ":1},"Properties":{"id":"20250808152240-qcnbt22","updated":"20250808152240"},"Children":[{"ID":"20250808152240-zieg0x5","Type":"NodeListItem","ListData":{"Typ":1,"Delimiter":46,"Marker":"MS4=","Num":1},"Properties":{"id":"20250808152240-zieg0x5","updated":"20250808152240"},"Children":[{"ID":"20250808152240-a21h267","Type":"NodeParagraph","Properties":{"id":"20250808152240-a21h267","updated":"20250808152240"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"基本原则：方差保持"}]},{"ID":"20250808152240-ksg2bm1","Type":"NodeList","ListData":{},"Properties":{"id":"20250808152240-ksg2bm1","updated":"20250808152240"},"Children":[{"ID":"20250808152240-vn6ey1f","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808152240-vn6ey1f","updated":"20250808152240"},"Children":[{"ID":"20250808152240-9lv3iid","Type":"NodeParagraph","Properties":{"id":"20250808152240-9lv3iid","updated":"20250808152240"},"Children":[{"Type":"NodeText","Data":"与上一节的归一化策略目标一致，权重初始化的目的也是为了稳定信号流。一个好的初始化是成功训练深度模型的起点。"}]}]}]}]}]}]},{"ID":"20250808161417-jh7jqni","Type":"NodeBlockquote","Properties":{"fold":"1","id":"20250808161417-jh7jqni","updated":"20250808163741"},"Children":[{"Type":"NodeBlockquoteMarker","Data":"\u003e"},{"ID":"20250808161505-wkennl2","Type":"NodeList","ListData":{"Typ":1},"Properties":{"id":"20250808161505-wkennl2","updated":"20250808161505"},"Children":[{"ID":"20250808161505-5k041tz","Type":"NodeListItem","ListData":{"Typ":1,"Delimiter":46,"Marker":"Mi4=","Num":2},"Properties":{"id":"20250808161505-5k041tz","updated":"20250808161505"},"Children":[{"ID":"20250808161505-iudrnw6","Type":"NodeParagraph","Properties":{"id":"20250808161505-iudrnw6","updated":"20250808161505"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"针对不同层类型的定制化初始化"}]}]}]},{"ID":"20250808161505-r1g00a0","Type":"NodeList","ListData":{},"Properties":{"id":"20250808161505-r1g00a0","updated":"20250808163741"},"Children":[{"ID":"20250808161505-atnppxd","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808161505-atnppxd","updated":"20250808163741"},"Children":[{"ID":"20250808161505-l5fh6wd","Type":"NodeParagraph","Properties":{"id":"20250808161505-l5fh6wd","updated":"20250808161505"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"通用层 (非GLU层)"},{"Type":"NodeText","Data":"："}]},{"ID":"20250808161505-28t2zra","Type":"NodeList","ListData":{},"Properties":{"id":"20250808161505-28t2zra","updated":"20250808163741"},"Children":[{"ID":"20250808161505-b3ups1f","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808161505-b3ups1f","updated":"20250808163741"},"Children":[{"ID":"20250808161505-b5gbipk","Type":"NodeParagraph","Properties":{"id":"20250808161505-b5gbipk","updated":"20250808163741"},"Children":[{"Type":"NodeText","Data":"采用的是经典的 "},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"He/Xavier 初始化"},{"Type":"NodeText","Data":"。权重从均值为0、方差为 "},{"Type":"NodeTextMark","TextMarkType":"code inline-math","TextMarkInlineMathContent":"1/nₗ"},{"Type":"NodeText","Data":"​ 的正态分布中抽取（即标准差为 "},{"Type":"NodeTextMark","TextMarkType":"code","TextMarkTextContent":"√(1/nₗ)"},{"Type":"NodeText","Data":"​）。"},{"Type":"NodeTextMark","TextMarkType":"code","TextMarkTextContent":"nₗ"},{"Type":"NodeText","Data":"​ 是输入的维度（fan-in）。这个方法被证明可以有效地保持激活值的方差在通过线性层后基本不变。"}]}]}]}]},{"ID":"20250808161505-9qoc615","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808161505-9qoc615","updated":"20250808161505"},"Children":[{"ID":"20250808161505-s7b41d3","Type":"NodeParagraph","Properties":{"id":"20250808161505-s7b41d3","updated":"20250808161505"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"GLU之前的层 (关键创新)"},{"Type":"NodeText","Data":"："}]},{"ID":"20250808161505-b7dkmil","Type":"NodeList","ListData":{},"Properties":{"id":"20250808161505-b7dkmil","updated":"20250808161505"},"Children":[{"ID":"20250808161505-cuif42o","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808161505-cuif42o","updated":"20250808161505"},"Children":[{"ID":"20250808161505-dl7ka4o","Type":"NodeParagraph","Properties":{"id":"20250808161505-dl7ka4o","updated":"20250808161505"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"问题"},{"Type":"NodeText","Data":"：作者通过理论分析（附录A.1）发现，门控线性单元（GLU）会使其输出的方差大约"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"减少为输入方差的1/4"},{"Type":"NodeText","Data":"。如果直接使用标准初始化，每经过一个“卷积+GLU”块，信号强度就会锐减，导致深层网络无法学习。"}]}]},{"ID":"20250808161505-b4stmcd","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808161505-b4stmcd","updated":"20250808161505"},"Children":[{"ID":"20250808161505-i76b5jp","Type":"NodeParagraph","Properties":{"id":"20250808161505-i76b5jp","updated":"20250808161505"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"解决方案"},{"Type":"NodeText","Data":"：必须在信号进入GLU之前，提前将其方差"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"放大4倍"},{"Type":"NodeText","Data":"以作补偿。"}]}]},{"ID":"20250808161505-hiti86e","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808161505-hiti86e","updated":"20250808161505"},"Children":[{"ID":"20250808161505-b4amgsm","Type":"NodeParagraph","Properties":{"id":"20250808161505-b4amgsm","updated":"20250808161505"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"实现"},{"Type":"NodeText","Data":"：通过将该层权重的初始化标准差乘以 "},{"Type":"NodeTextMark","TextMarkType":"code","TextMarkTextContent":"√4 = 2"},{"Type":"NodeText","Data":"​ 来实现。因此，初始化分布变成了 "},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"N(0, √(4/nₗ))"},{"Type":"NodeText","Data":"。这样，经过这个线性层后，输出方差变成了输入的4倍；再经过GLU，方差减少为1/4，最终正好恢复到与原始输入相同的水平，实现了方差的稳定传递。"}]}]}]}]},{"ID":"20250808161505-e8dfifh","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808161505-e8dfifh","updated":"20250808161505"},"Children":[{"ID":"20250808161505-hd98h19","Type":"NodeParagraph","Properties":{"id":"20250808161505-hd98h19","updated":"20250808161505"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"Dropout之后的层"},{"Type":"NodeText","Data":"："}]},{"ID":"20250808161505-6nx3zmn","Type":"NodeList","ListData":{},"Properties":{"id":"20250808161505-6nx3zmn","updated":"20250808161505"},"Children":[{"ID":"20250808161505-jmlzv8o","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808161505-jmlzv8o","updated":"20250808161505"},"Children":[{"ID":"20250808161505-eykokjs","Type":"NodeParagraph","Properties":{"id":"20250808161505-eykokjs","updated":"20250808161505"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"问题"},{"Type":"NodeText","Data":"：本文使用了“Inverted Dropout”，在训练时，保留下来的激活值会被乘以 "},{"Type":"NodeTextMark","TextMarkType":"code","TextMarkTextContent":"1/p"},{"Type":"NodeText","Data":"​（其中 "},{"Type":"NodeTextMark","TextMarkType":"code","TextMarkTextContent":"p"},{"Type":"NodeText","Data":"​ 是保留概率）。这会导致该层的输出方差被"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"放大 1/p 倍"},{"Type":"NodeText","Data":"。"}]}]},{"ID":"20250808161505-76smg99","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808161505-76smg99","updated":"20250808161505"},"Children":[{"ID":"20250808161505-40dejgd","Type":"NodeParagraph","Properties":{"id":"20250808161505-40dejgd","updated":"20250808161505"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"解决方案"},{"Type":"NodeText","Data":"：为了抵消这种方差的增加，需要将该层权重的初始方差"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"缩小 p 倍"},{"Type":"NodeText","Data":"。"}]}]},{"ID":"20250808161505-hisy0r5","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808161505-hisy0r5","updated":"20250808161505"},"Children":[{"ID":"20250808161505-h0c7thb","Type":"NodeParagraph","Properties":{"id":"20250808161505-h0c7thb","updated":"20250808161505"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"实现"},{"Type":"NodeText","Data":"：将权重的初始化标准差乘以 "},{"Type":"NodeTextMark","TextMarkType":"code","TextMarkTextContent":"√p"},{"Type":"NodeText","Data":"​。"}]}]},{"ID":"20250808161505-8q9nio8","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808161505-8q9nio8","updated":"20250808161505"},"Children":[{"ID":"20250808161505-iu02qde","Type":"NodeParagraph","Properties":{"id":"20250808161505-iu02qde","updated":"20250808161505"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"综合策略"},{"Type":"NodeText","Data":"：结合GLU和Dropout，最终的初始化策略是："}]},{"ID":"20250808161505-xgu8tmm","Type":"NodeList","ListData":{},"Properties":{"id":"20250808161505-xgu8tmm","updated":"20250808161505"},"Children":[{"ID":"20250808161505-qqjrv3d","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808161505-qqjrv3d","updated":"20250808161505"},"Children":[{"ID":"20250808161505-dws0gf7","Type":"NodeParagraph","Properties":{"id":"20250808161505-dws0gf7","updated":"20250808161505"},"Children":[{"Type":"NodeText","Data":"如果一个层"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"后面有Dropout和GLU"},{"Type":"NodeText","Data":"：初始化标准差为 "},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"√(4p/nₗ)"},{"Type":"NodeText","Data":"。"}]}]},{"ID":"20250808161505-f3ccbhl","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808161505-f3ccbhl","updated":"20250808161505"},"Children":[{"ID":"20250808161505-xjbs5bi","Type":"NodeParagraph","Properties":{"id":"20250808161505-xjbs5bi","updated":"20250808161505"},"Children":[{"Type":"NodeText","Data":"如果一个层"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"后面只有Dropout（没有GLU）"},{"Type":"NodeText","Data":"：初始化标准差为 "},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"√(p/nₗ)"},{"Type":"NodeText","Data":"。"}]}]}]}]},{"ID":"20250808161505-xabdjhr","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20250808161505-xabdjhr","updated":"20250808161505"},"Children":[{"ID":"20250808161505-81g0iof","Type":"NodeParagraph","Properties":{"id":"20250808161505-81g0iof","updated":"20250808161505"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"注意"},{"Type":"NodeText","Data":"：文中“通过用更大的权重来初始化”的说法可能有些误导。因为 "},{"Type":"NodeTextMark","TextMarkType":"code","TextMarkTextContent":"p \u0026lt; 1"},{"Type":"NodeText","Data":"​，所以 "},{"Type":"NodeTextMark","TextMarkType":"code","TextMarkTextContent":"√p \u0026lt; 1"},{"Type":"NodeText","Data":"​，实际上的初始化权重标准差是"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"变小了"},{"Type":"NodeText","Data":"，这是为了抵消Dropout带来的方差"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"增大"},{"Type":"NodeText","Data":"。这里的表述和公式所做的事情是正确的，即用更小的权重来平衡Dropout的影响。"}]}]}]}]}]}]},{"ID":"20250809085604-db8wb7n","Type":"NodeHeading","HeadingLevel":1,"Properties":{"id":"20250809085604-db8wb7n","updated":"20250809090141"},"Children":[{"Type":"NodeText","Data":"收获"}]},{"ID":"20250809085611-5n5di07","Type":"NodeParagraph","Properties":{"id":"20250809085611-5n5di07","updated":"20250809085905"},"Children":[{"Type":"NodeText","Data":"在编写代码时，要逐步深入要求，多个要求要切分出来，一切以先跑通代码为核心！比如说数据集也可以先手动创建本地数据集（但是最好是有提供通用数据接口的函数）"}]},{"ID":"20250809085952-82s0nnb","Type":"NodeParagraph","Properties":{"id":"20250809085952-82s0nnb","updated":"20250809090103"},"Children":[{"Type":"NodeText","Data":"代码更新时记得先在云平台上更新，确认无误之后再更新到本地仓库和github仓库（可以使用下面的代码进行文件重写）："}]},{"ID":"20250809090103-i4byg1v","Type":"NodeCodeBlock","IsFencedCodeBlock":true,"Properties":{"id":"20250809090103-i4byg1v","updated":"20250809090141"},"Children":[{"Type":"NodeCodeBlockFenceOpenMarker","Data":"```"},{"Type":"NodeCodeBlockFenceInfoMarker","CodeBlockInfo":"cHk="},{"Type":"NodeCodeBlockCode","Data":"%%writefile \u003c文件路径\u003e\n"},{"Type":"NodeCodeBlockFenceCloseMarker","Data":"```"}]}]}